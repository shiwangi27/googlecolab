{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhDcsToNa1LFw5VbviMeX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiwangi27/googlecolab/blob/main/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKVBSwdIh-Z5"
      },
      "source": [
        "### Basic Machine Learning optimizations\n",
        "\n",
        "For this learning, I used Andrej Karpathy's [cs231n](https://cs231n.github.io/) class lectures and [ml-cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest) docs.\n",
        "\n",
        "Following are the topics I have tried:\n",
        "\n",
        "1. Random Local Search \n",
        "2. Gradient Descent for Linear Regression\n",
        "3. Gradient Descent with L2 Regularization\n",
        "4. Multilayer Perceptron \n",
        "5. Logistic Regression Classifier\n",
        "6. 2-layer NN Classifier in Tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVI3xmCyU0E5"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6d0uT84t2eo"
      },
      "source": [
        "def plot_curve(metricx, metricy):\n",
        "  fig = plt.figure(figsize=(5, 5))\n",
        "  plt.plot(metricx, metricy, color=\"blue\")\n",
        "  plt.xlabel(\"mean loss\")\n",
        "  plt.ylabel(\"iteration\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ireH3ROct8tr"
      },
      "source": [
        "### Generate Random Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XxAE55GU5GR"
      },
      "source": [
        "num_features = 20\n",
        "num_train_examples = 900\n",
        "num_test_examples = 100 \n",
        "\n",
        "X_train = np.random.random((num_train_examples, num_features))\n",
        "y_train = np.random.random((num_train_examples, 1)) \n",
        "X_test = np.random.random((num_test_examples, num_features)) \n",
        "y_test = np.random.random((num_test_examples, 1))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXKMgy7jsYgQ"
      },
      "source": [
        "### Random Local Search optimization\n",
        "\n",
        "Every single step of the iteration we would take a step ever so small in the direction towards the goal (which is minima). This step is a small random number added to the weight and we check if the loss is lower than the minimum loss so far. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCXNfj_HWJ6w"
      },
      "source": [
        "num_iterations = 1000\n",
        "num_coefficients = 5 \n",
        "\n",
        "# Initialize the weights and biases\n",
        "W = np.random.random((num_features, num_coefficients))\n",
        "b = np.ones((num_coefficients))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ2Paf70DQkN",
        "outputId": "f616ebad-a1df-4c3f-e51a-210e4a9b2983"
      },
      "source": [
        "factor = 0.01\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "Wi = W*0.001\n",
        "bi = b.copy()\n",
        "\n",
        "losses = []\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  dW = factor * np.random.randn(num_features, num_coefficients)\n",
        "  Wi = Wi + dW\n",
        "\n",
        "  y_hat = X_train.dot(Wi) + bi\n",
        "  loss = y_hat - y_train\n",
        "\n",
        "  mean_loss = np.mean(loss)\n",
        "  losses.append(mean_loss)\n",
        "\n",
        "  if mean_loss < best_loss:\n",
        "    best_loss = mean_loss\n",
        "\n",
        "  print(\"iteration %s loss = %s:\" % (i, best_loss))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 loss = 0.5267222349298863:\n",
            "iteration 1 loss = 0.5267222349298863:\n",
            "iteration 2 loss = 0.5267222349298863:\n",
            "iteration 3 loss = 0.5267222349298863:\n",
            "iteration 4 loss = 0.5267222349298863:\n",
            "iteration 5 loss = 0.5267222349298863:\n",
            "iteration 6 loss = 0.5267222349298863:\n",
            "iteration 7 loss = 0.5267222349298863:\n",
            "iteration 8 loss = 0.5267222349298863:\n",
            "iteration 9 loss = 0.5267222349298863:\n",
            "iteration 10 loss = 0.5267222349298863:\n",
            "iteration 11 loss = 0.5267222349298863:\n",
            "iteration 12 loss = 0.5267222349298863:\n",
            "iteration 13 loss = 0.5267222349298863:\n",
            "iteration 14 loss = 0.5267222349298863:\n",
            "iteration 15 loss = 0.5267222349298863:\n",
            "iteration 16 loss = 0.5267222349298863:\n",
            "iteration 17 loss = 0.5267222349298863:\n",
            "iteration 18 loss = 0.5267222349298863:\n",
            "iteration 19 loss = 0.5267222349298863:\n",
            "iteration 20 loss = 0.5267222349298863:\n",
            "iteration 21 loss = 0.5267222349298863:\n",
            "iteration 22 loss = 0.5267222349298863:\n",
            "iteration 23 loss = 0.5267222349298863:\n",
            "iteration 24 loss = 0.5267222349298863:\n",
            "iteration 25 loss = 0.5267222349298863:\n",
            "iteration 26 loss = 0.5267222349298863:\n",
            "iteration 27 loss = 0.5267222349298863:\n",
            "iteration 28 loss = 0.5267222349298863:\n",
            "iteration 29 loss = 0.5267222349298863:\n",
            "iteration 30 loss = 0.5267222349298863:\n",
            "iteration 31 loss = 0.5267222349298863:\n",
            "iteration 32 loss = 0.5267222349298863:\n",
            "iteration 33 loss = 0.5267222349298863:\n",
            "iteration 34 loss = 0.5267222349298863:\n",
            "iteration 35 loss = 0.5267222349298863:\n",
            "iteration 36 loss = 0.5267222349298863:\n",
            "iteration 37 loss = 0.5267222349298863:\n",
            "iteration 38 loss = 0.5267222349298863:\n",
            "iteration 39 loss = 0.5267222349298863:\n",
            "iteration 40 loss = 0.5267222349298863:\n",
            "iteration 41 loss = 0.5267222349298863:\n",
            "iteration 42 loss = 0.5267222349298863:\n",
            "iteration 43 loss = 0.5267222349298863:\n",
            "iteration 44 loss = 0.5267222349298863:\n",
            "iteration 45 loss = 0.5267222349298863:\n",
            "iteration 46 loss = 0.5267222349298863:\n",
            "iteration 47 loss = 0.5267222349298863:\n",
            "iteration 48 loss = 0.5267222349298863:\n",
            "iteration 49 loss = 0.5267222349298863:\n",
            "iteration 50 loss = 0.5267222349298863:\n",
            "iteration 51 loss = 0.5267222349298863:\n",
            "iteration 52 loss = 0.5267222349298863:\n",
            "iteration 53 loss = 0.5267222349298863:\n",
            "iteration 54 loss = 0.5267222349298863:\n",
            "iteration 55 loss = 0.5267222349298863:\n",
            "iteration 56 loss = 0.5267222349298863:\n",
            "iteration 57 loss = 0.5267222349298863:\n",
            "iteration 58 loss = 0.5267222349298863:\n",
            "iteration 59 loss = 0.5267222349298863:\n",
            "iteration 60 loss = 0.5267222349298863:\n",
            "iteration 61 loss = 0.5267222349298863:\n",
            "iteration 62 loss = 0.5267222349298863:\n",
            "iteration 63 loss = 0.5267222349298863:\n",
            "iteration 64 loss = 0.5267222349298863:\n",
            "iteration 65 loss = 0.5267222349298863:\n",
            "iteration 66 loss = 0.5267222349298863:\n",
            "iteration 67 loss = 0.5267222349298863:\n",
            "iteration 68 loss = 0.5267222349298863:\n",
            "iteration 69 loss = 0.5267222349298863:\n",
            "iteration 70 loss = 0.5267222349298863:\n",
            "iteration 71 loss = 0.5267222349298863:\n",
            "iteration 72 loss = 0.5267222349298863:\n",
            "iteration 73 loss = 0.5267222349298863:\n",
            "iteration 74 loss = 0.5267222349298863:\n",
            "iteration 75 loss = 0.5267222349298863:\n",
            "iteration 76 loss = 0.5267222349298863:\n",
            "iteration 77 loss = 0.5267222349298863:\n",
            "iteration 78 loss = 0.5267222349298863:\n",
            "iteration 79 loss = 0.5267222349298863:\n",
            "iteration 80 loss = 0.5267222349298863:\n",
            "iteration 81 loss = 0.5267222349298863:\n",
            "iteration 82 loss = 0.5267222349298863:\n",
            "iteration 83 loss = 0.5267222349298863:\n",
            "iteration 84 loss = 0.5267222349298863:\n",
            "iteration 85 loss = 0.5267222349298863:\n",
            "iteration 86 loss = 0.5267222349298863:\n",
            "iteration 87 loss = 0.5267222349298863:\n",
            "iteration 88 loss = 0.5267222349298863:\n",
            "iteration 89 loss = 0.5267222349298863:\n",
            "iteration 90 loss = 0.5267222349298863:\n",
            "iteration 91 loss = 0.5267222349298863:\n",
            "iteration 92 loss = 0.5267222349298863:\n",
            "iteration 93 loss = 0.5267222349298863:\n",
            "iteration 94 loss = 0.5252810826187171:\n",
            "iteration 95 loss = 0.5252810826187171:\n",
            "iteration 96 loss = 0.5221398120549923:\n",
            "iteration 97 loss = 0.5221398120549923:\n",
            "iteration 98 loss = 0.5207749283361804:\n",
            "iteration 99 loss = 0.5207749283361804:\n",
            "iteration 100 loss = 0.5207749283361804:\n",
            "iteration 101 loss = 0.5207749283361804:\n",
            "iteration 102 loss = 0.5054920630717112:\n",
            "iteration 103 loss = 0.5020325612200313:\n",
            "iteration 104 loss = 0.49428557150645935:\n",
            "iteration 105 loss = 0.49428557150645935:\n",
            "iteration 106 loss = 0.49428557150645935:\n",
            "iteration 107 loss = 0.49428557150645935:\n",
            "iteration 108 loss = 0.49199179479253285:\n",
            "iteration 109 loss = 0.49199179479253285:\n",
            "iteration 110 loss = 0.4790165468683177:\n",
            "iteration 111 loss = 0.4790165468683177:\n",
            "iteration 112 loss = 0.4790165468683177:\n",
            "iteration 113 loss = 0.4790165468683177:\n",
            "iteration 114 loss = 0.4790165468683177:\n",
            "iteration 115 loss = 0.4790165468683177:\n",
            "iteration 116 loss = 0.4790165468683177:\n",
            "iteration 117 loss = 0.4790165468683177:\n",
            "iteration 118 loss = 0.4790165468683177:\n",
            "iteration 119 loss = 0.4790165468683177:\n",
            "iteration 120 loss = 0.4790165468683177:\n",
            "iteration 121 loss = 0.4790165468683177:\n",
            "iteration 122 loss = 0.4790165468683177:\n",
            "iteration 123 loss = 0.4790165468683177:\n",
            "iteration 124 loss = 0.4790165468683177:\n",
            "iteration 125 loss = 0.4790165468683177:\n",
            "iteration 126 loss = 0.4790165468683177:\n",
            "iteration 127 loss = 0.4790165468683177:\n",
            "iteration 128 loss = 0.4790165468683177:\n",
            "iteration 129 loss = 0.4790165468683177:\n",
            "iteration 130 loss = 0.4790165468683177:\n",
            "iteration 131 loss = 0.4790165468683177:\n",
            "iteration 132 loss = 0.4790165468683177:\n",
            "iteration 133 loss = 0.4790165468683177:\n",
            "iteration 134 loss = 0.4790165468683177:\n",
            "iteration 135 loss = 0.4790165468683177:\n",
            "iteration 136 loss = 0.4790165468683177:\n",
            "iteration 137 loss = 0.4790165468683177:\n",
            "iteration 138 loss = 0.4790165468683177:\n",
            "iteration 139 loss = 0.4790165468683177:\n",
            "iteration 140 loss = 0.4790165468683177:\n",
            "iteration 141 loss = 0.4790165468683177:\n",
            "iteration 142 loss = 0.4714743800254906:\n",
            "iteration 143 loss = 0.4647939126884446:\n",
            "iteration 144 loss = 0.447406516889817:\n",
            "iteration 145 loss = 0.4410490168278325:\n",
            "iteration 146 loss = 0.4410490168278325:\n",
            "iteration 147 loss = 0.434992586543422:\n",
            "iteration 148 loss = 0.4199631495808131:\n",
            "iteration 149 loss = 0.4175162273188581:\n",
            "iteration 150 loss = 0.4175162273188581:\n",
            "iteration 151 loss = 0.4175162273188581:\n",
            "iteration 152 loss = 0.4132199929066832:\n",
            "iteration 153 loss = 0.4132199929066832:\n",
            "iteration 154 loss = 0.4132199929066832:\n",
            "iteration 155 loss = 0.4132199929066832:\n",
            "iteration 156 loss = 0.4132199929066832:\n",
            "iteration 157 loss = 0.4132199929066832:\n",
            "iteration 158 loss = 0.4132199929066832:\n",
            "iteration 159 loss = 0.4132199929066832:\n",
            "iteration 160 loss = 0.4132199929066832:\n",
            "iteration 161 loss = 0.4132199929066832:\n",
            "iteration 162 loss = 0.4132199929066832:\n",
            "iteration 163 loss = 0.4132199929066832:\n",
            "iteration 164 loss = 0.4132199929066832:\n",
            "iteration 165 loss = 0.4132199929066832:\n",
            "iteration 166 loss = 0.4132199929066832:\n",
            "iteration 167 loss = 0.4132199929066832:\n",
            "iteration 168 loss = 0.4132199929066832:\n",
            "iteration 169 loss = 0.4132199929066832:\n",
            "iteration 170 loss = 0.4132199929066832:\n",
            "iteration 171 loss = 0.4132199929066832:\n",
            "iteration 172 loss = 0.4132199929066832:\n",
            "iteration 173 loss = 0.4132199929066832:\n",
            "iteration 174 loss = 0.4132199929066832:\n",
            "iteration 175 loss = 0.4132199929066832:\n",
            "iteration 176 loss = 0.4132199929066832:\n",
            "iteration 177 loss = 0.4132199929066832:\n",
            "iteration 178 loss = 0.4132199929066832:\n",
            "iteration 179 loss = 0.4132199929066832:\n",
            "iteration 180 loss = 0.4132199929066832:\n",
            "iteration 181 loss = 0.4132199929066832:\n",
            "iteration 182 loss = 0.4132199929066832:\n",
            "iteration 183 loss = 0.4132199929066832:\n",
            "iteration 184 loss = 0.4132199929066832:\n",
            "iteration 185 loss = 0.4132199929066832:\n",
            "iteration 186 loss = 0.4132199929066832:\n",
            "iteration 187 loss = 0.4132199929066832:\n",
            "iteration 188 loss = 0.4132199929066832:\n",
            "iteration 189 loss = 0.4132199929066832:\n",
            "iteration 190 loss = 0.4132199929066832:\n",
            "iteration 191 loss = 0.4132199929066832:\n",
            "iteration 192 loss = 0.4132199929066832:\n",
            "iteration 193 loss = 0.4132199929066832:\n",
            "iteration 194 loss = 0.4132199929066832:\n",
            "iteration 195 loss = 0.4132199929066832:\n",
            "iteration 196 loss = 0.4132199929066832:\n",
            "iteration 197 loss = 0.4132199929066832:\n",
            "iteration 198 loss = 0.4132199929066832:\n",
            "iteration 199 loss = 0.4132199929066832:\n",
            "iteration 200 loss = 0.4132199929066832:\n",
            "iteration 201 loss = 0.4132199929066832:\n",
            "iteration 202 loss = 0.4132199929066832:\n",
            "iteration 203 loss = 0.4132199929066832:\n",
            "iteration 204 loss = 0.4132199929066832:\n",
            "iteration 205 loss = 0.4132199929066832:\n",
            "iteration 206 loss = 0.4132199929066832:\n",
            "iteration 207 loss = 0.4132199929066832:\n",
            "iteration 208 loss = 0.4132199929066832:\n",
            "iteration 209 loss = 0.4132199929066832:\n",
            "iteration 210 loss = 0.4132199929066832:\n",
            "iteration 211 loss = 0.4132199929066832:\n",
            "iteration 212 loss = 0.4132199929066832:\n",
            "iteration 213 loss = 0.4132199929066832:\n",
            "iteration 214 loss = 0.4132199929066832:\n",
            "iteration 215 loss = 0.4132199929066832:\n",
            "iteration 216 loss = 0.4132199929066832:\n",
            "iteration 217 loss = 0.4132199929066832:\n",
            "iteration 218 loss = 0.4132199929066832:\n",
            "iteration 219 loss = 0.4132199929066832:\n",
            "iteration 220 loss = 0.4132199929066832:\n",
            "iteration 221 loss = 0.4132199929066832:\n",
            "iteration 222 loss = 0.4132199929066832:\n",
            "iteration 223 loss = 0.4132199929066832:\n",
            "iteration 224 loss = 0.4132199929066832:\n",
            "iteration 225 loss = 0.4132199929066832:\n",
            "iteration 226 loss = 0.4132199929066832:\n",
            "iteration 227 loss = 0.4132199929066832:\n",
            "iteration 228 loss = 0.4132199929066832:\n",
            "iteration 229 loss = 0.4132199929066832:\n",
            "iteration 230 loss = 0.4132199929066832:\n",
            "iteration 231 loss = 0.4132199929066832:\n",
            "iteration 232 loss = 0.4132199929066832:\n",
            "iteration 233 loss = 0.4132199929066832:\n",
            "iteration 234 loss = 0.4132199929066832:\n",
            "iteration 235 loss = 0.4132199929066832:\n",
            "iteration 236 loss = 0.4132199929066832:\n",
            "iteration 237 loss = 0.4132199929066832:\n",
            "iteration 238 loss = 0.4132199929066832:\n",
            "iteration 239 loss = 0.4132199929066832:\n",
            "iteration 240 loss = 0.4132199929066832:\n",
            "iteration 241 loss = 0.4132199929066832:\n",
            "iteration 242 loss = 0.4132199929066832:\n",
            "iteration 243 loss = 0.4132199929066832:\n",
            "iteration 244 loss = 0.4132199929066832:\n",
            "iteration 245 loss = 0.4132199929066832:\n",
            "iteration 246 loss = 0.4132199929066832:\n",
            "iteration 247 loss = 0.4132199929066832:\n",
            "iteration 248 loss = 0.4132199929066832:\n",
            "iteration 249 loss = 0.4132199929066832:\n",
            "iteration 250 loss = 0.4132199929066832:\n",
            "iteration 251 loss = 0.4132199929066832:\n",
            "iteration 252 loss = 0.4132199929066832:\n",
            "iteration 253 loss = 0.4132199929066832:\n",
            "iteration 254 loss = 0.4132199929066832:\n",
            "iteration 255 loss = 0.4132199929066832:\n",
            "iteration 256 loss = 0.4132199929066832:\n",
            "iteration 257 loss = 0.4132199929066832:\n",
            "iteration 258 loss = 0.4132199929066832:\n",
            "iteration 259 loss = 0.4132199929066832:\n",
            "iteration 260 loss = 0.4132199929066832:\n",
            "iteration 261 loss = 0.4132199929066832:\n",
            "iteration 262 loss = 0.4132199929066832:\n",
            "iteration 263 loss = 0.4132199929066832:\n",
            "iteration 264 loss = 0.4132199929066832:\n",
            "iteration 265 loss = 0.4132199929066832:\n",
            "iteration 266 loss = 0.4132199929066832:\n",
            "iteration 267 loss = 0.4132199929066832:\n",
            "iteration 268 loss = 0.4132199929066832:\n",
            "iteration 269 loss = 0.4132199929066832:\n",
            "iteration 270 loss = 0.4132199929066832:\n",
            "iteration 271 loss = 0.4132199929066832:\n",
            "iteration 272 loss = 0.4132199929066832:\n",
            "iteration 273 loss = 0.4132199929066832:\n",
            "iteration 274 loss = 0.4132199929066832:\n",
            "iteration 275 loss = 0.40796798870249334:\n",
            "iteration 276 loss = 0.3966682598013361:\n",
            "iteration 277 loss = 0.3914934198214509:\n",
            "iteration 278 loss = 0.3914934198214509:\n",
            "iteration 279 loss = 0.3914934198214509:\n",
            "iteration 280 loss = 0.3914934198214509:\n",
            "iteration 281 loss = 0.3914934198214509:\n",
            "iteration 282 loss = 0.3914934198214509:\n",
            "iteration 283 loss = 0.3914934198214509:\n",
            "iteration 284 loss = 0.3914934198214509:\n",
            "iteration 285 loss = 0.3914934198214509:\n",
            "iteration 286 loss = 0.3914934198214509:\n",
            "iteration 287 loss = 0.3914934198214509:\n",
            "iteration 288 loss = 0.3914934198214509:\n",
            "iteration 289 loss = 0.39001702239304875:\n",
            "iteration 290 loss = 0.39001702239304875:\n",
            "iteration 291 loss = 0.39001702239304875:\n",
            "iteration 292 loss = 0.39001702239304875:\n",
            "iteration 293 loss = 0.39001702239304875:\n",
            "iteration 294 loss = 0.39001702239304875:\n",
            "iteration 295 loss = 0.39001702239304875:\n",
            "iteration 296 loss = 0.39001702239304875:\n",
            "iteration 297 loss = 0.39001702239304875:\n",
            "iteration 298 loss = 0.39001702239304875:\n",
            "iteration 299 loss = 0.39001702239304875:\n",
            "iteration 300 loss = 0.39001702239304875:\n",
            "iteration 301 loss = 0.3835074627993894:\n",
            "iteration 302 loss = 0.3835074627993894:\n",
            "iteration 303 loss = 0.3835074627993894:\n",
            "iteration 304 loss = 0.3835074627993894:\n",
            "iteration 305 loss = 0.3835074627993894:\n",
            "iteration 306 loss = 0.3835074627993894:\n",
            "iteration 307 loss = 0.3835074627993894:\n",
            "iteration 308 loss = 0.3835074627993894:\n",
            "iteration 309 loss = 0.3802450481717366:\n",
            "iteration 310 loss = 0.3802450481717366:\n",
            "iteration 311 loss = 0.3802450481717366:\n",
            "iteration 312 loss = 0.3802450481717366:\n",
            "iteration 313 loss = 0.37043496643442914:\n",
            "iteration 314 loss = 0.36884804903942214:\n",
            "iteration 315 loss = 0.36884804903942214:\n",
            "iteration 316 loss = 0.36884804903942214:\n",
            "iteration 317 loss = 0.36884804903942214:\n",
            "iteration 318 loss = 0.3676517539997364:\n",
            "iteration 319 loss = 0.3639007128343063:\n",
            "iteration 320 loss = 0.3639007128343063:\n",
            "iteration 321 loss = 0.3526329150948559:\n",
            "iteration 322 loss = 0.33920176519482154:\n",
            "iteration 323 loss = 0.33920176519482154:\n",
            "iteration 324 loss = 0.3294210589350695:\n",
            "iteration 325 loss = 0.32576018093981907:\n",
            "iteration 326 loss = 0.32576018093981907:\n",
            "iteration 327 loss = 0.32576018093981907:\n",
            "iteration 328 loss = 0.32576018093981907:\n",
            "iteration 329 loss = 0.32576018093981907:\n",
            "iteration 330 loss = 0.32576018093981907:\n",
            "iteration 331 loss = 0.32576018093981907:\n",
            "iteration 332 loss = 0.32576018093981907:\n",
            "iteration 333 loss = 0.32576018093981907:\n",
            "iteration 334 loss = 0.32576018093981907:\n",
            "iteration 335 loss = 0.32576018093981907:\n",
            "iteration 336 loss = 0.32576018093981907:\n",
            "iteration 337 loss = 0.32576018093981907:\n",
            "iteration 338 loss = 0.32576018093981907:\n",
            "iteration 339 loss = 0.32576018093981907:\n",
            "iteration 340 loss = 0.32576018093981907:\n",
            "iteration 341 loss = 0.32576018093981907:\n",
            "iteration 342 loss = 0.32576018093981907:\n",
            "iteration 343 loss = 0.32576018093981907:\n",
            "iteration 344 loss = 0.32576018093981907:\n",
            "iteration 345 loss = 0.32576018093981907:\n",
            "iteration 346 loss = 0.32576018093981907:\n",
            "iteration 347 loss = 0.32576018093981907:\n",
            "iteration 348 loss = 0.32576018093981907:\n",
            "iteration 349 loss = 0.32576018093981907:\n",
            "iteration 350 loss = 0.32576018093981907:\n",
            "iteration 351 loss = 0.32576018093981907:\n",
            "iteration 352 loss = 0.32576018093981907:\n",
            "iteration 353 loss = 0.32576018093981907:\n",
            "iteration 354 loss = 0.32576018093981907:\n",
            "iteration 355 loss = 0.32576018093981907:\n",
            "iteration 356 loss = 0.32576018093981907:\n",
            "iteration 357 loss = 0.32576018093981907:\n",
            "iteration 358 loss = 0.32576018093981907:\n",
            "iteration 359 loss = 0.32576018093981907:\n",
            "iteration 360 loss = 0.32576018093981907:\n",
            "iteration 361 loss = 0.32576018093981907:\n",
            "iteration 362 loss = 0.32576018093981907:\n",
            "iteration 363 loss = 0.32576018093981907:\n",
            "iteration 364 loss = 0.32576018093981907:\n",
            "iteration 365 loss = 0.32576018093981907:\n",
            "iteration 366 loss = 0.32576018093981907:\n",
            "iteration 367 loss = 0.32576018093981907:\n",
            "iteration 368 loss = 0.32576018093981907:\n",
            "iteration 369 loss = 0.32576018093981907:\n",
            "iteration 370 loss = 0.32576018093981907:\n",
            "iteration 371 loss = 0.32576018093981907:\n",
            "iteration 372 loss = 0.32576018093981907:\n",
            "iteration 373 loss = 0.32576018093981907:\n",
            "iteration 374 loss = 0.32576018093981907:\n",
            "iteration 375 loss = 0.32576018093981907:\n",
            "iteration 376 loss = 0.32576018093981907:\n",
            "iteration 377 loss = 0.32576018093981907:\n",
            "iteration 378 loss = 0.32576018093981907:\n",
            "iteration 379 loss = 0.32576018093981907:\n",
            "iteration 380 loss = 0.32576018093981907:\n",
            "iteration 381 loss = 0.32576018093981907:\n",
            "iteration 382 loss = 0.32576018093981907:\n",
            "iteration 383 loss = 0.32576018093981907:\n",
            "iteration 384 loss = 0.32576018093981907:\n",
            "iteration 385 loss = 0.32576018093981907:\n",
            "iteration 386 loss = 0.32576018093981907:\n",
            "iteration 387 loss = 0.32576018093981907:\n",
            "iteration 388 loss = 0.32576018093981907:\n",
            "iteration 389 loss = 0.32576018093981907:\n",
            "iteration 390 loss = 0.32576018093981907:\n",
            "iteration 391 loss = 0.32576018093981907:\n",
            "iteration 392 loss = 0.32576018093981907:\n",
            "iteration 393 loss = 0.32576018093981907:\n",
            "iteration 394 loss = 0.32576018093981907:\n",
            "iteration 395 loss = 0.32576018093981907:\n",
            "iteration 396 loss = 0.32576018093981907:\n",
            "iteration 397 loss = 0.32576018093981907:\n",
            "iteration 398 loss = 0.32576018093981907:\n",
            "iteration 399 loss = 0.32576018093981907:\n",
            "iteration 400 loss = 0.32576018093981907:\n",
            "iteration 401 loss = 0.32576018093981907:\n",
            "iteration 402 loss = 0.32576018093981907:\n",
            "iteration 403 loss = 0.32576018093981907:\n",
            "iteration 404 loss = 0.32576018093981907:\n",
            "iteration 405 loss = 0.32576018093981907:\n",
            "iteration 406 loss = 0.32576018093981907:\n",
            "iteration 407 loss = 0.32576018093981907:\n",
            "iteration 408 loss = 0.32576018093981907:\n",
            "iteration 409 loss = 0.32576018093981907:\n",
            "iteration 410 loss = 0.32576018093981907:\n",
            "iteration 411 loss = 0.32576018093981907:\n",
            "iteration 412 loss = 0.32576018093981907:\n",
            "iteration 413 loss = 0.32576018093981907:\n",
            "iteration 414 loss = 0.32576018093981907:\n",
            "iteration 415 loss = 0.32576018093981907:\n",
            "iteration 416 loss = 0.32576018093981907:\n",
            "iteration 417 loss = 0.32576018093981907:\n",
            "iteration 418 loss = 0.32576018093981907:\n",
            "iteration 419 loss = 0.32576018093981907:\n",
            "iteration 420 loss = 0.32576018093981907:\n",
            "iteration 421 loss = 0.32576018093981907:\n",
            "iteration 422 loss = 0.32576018093981907:\n",
            "iteration 423 loss = 0.32576018093981907:\n",
            "iteration 424 loss = 0.32576018093981907:\n",
            "iteration 425 loss = 0.32576018093981907:\n",
            "iteration 426 loss = 0.32576018093981907:\n",
            "iteration 427 loss = 0.32576018093981907:\n",
            "iteration 428 loss = 0.32576018093981907:\n",
            "iteration 429 loss = 0.32576018093981907:\n",
            "iteration 430 loss = 0.32576018093981907:\n",
            "iteration 431 loss = 0.32576018093981907:\n",
            "iteration 432 loss = 0.32576018093981907:\n",
            "iteration 433 loss = 0.32576018093981907:\n",
            "iteration 434 loss = 0.32576018093981907:\n",
            "iteration 435 loss = 0.32576018093981907:\n",
            "iteration 436 loss = 0.32576018093981907:\n",
            "iteration 437 loss = 0.32576018093981907:\n",
            "iteration 438 loss = 0.32576018093981907:\n",
            "iteration 439 loss = 0.32576018093981907:\n",
            "iteration 440 loss = 0.32576018093981907:\n",
            "iteration 441 loss = 0.32576018093981907:\n",
            "iteration 442 loss = 0.32576018093981907:\n",
            "iteration 443 loss = 0.32576018093981907:\n",
            "iteration 444 loss = 0.32576018093981907:\n",
            "iteration 445 loss = 0.32576018093981907:\n",
            "iteration 446 loss = 0.32576018093981907:\n",
            "iteration 447 loss = 0.32576018093981907:\n",
            "iteration 448 loss = 0.32576018093981907:\n",
            "iteration 449 loss = 0.32576018093981907:\n",
            "iteration 450 loss = 0.32576018093981907:\n",
            "iteration 451 loss = 0.32576018093981907:\n",
            "iteration 452 loss = 0.32576018093981907:\n",
            "iteration 453 loss = 0.32576018093981907:\n",
            "iteration 454 loss = 0.32576018093981907:\n",
            "iteration 455 loss = 0.32576018093981907:\n",
            "iteration 456 loss = 0.32576018093981907:\n",
            "iteration 457 loss = 0.32576018093981907:\n",
            "iteration 458 loss = 0.32576018093981907:\n",
            "iteration 459 loss = 0.32576018093981907:\n",
            "iteration 460 loss = 0.32576018093981907:\n",
            "iteration 461 loss = 0.32576018093981907:\n",
            "iteration 462 loss = 0.32576018093981907:\n",
            "iteration 463 loss = 0.32576018093981907:\n",
            "iteration 464 loss = 0.32576018093981907:\n",
            "iteration 465 loss = 0.32576018093981907:\n",
            "iteration 466 loss = 0.32576018093981907:\n",
            "iteration 467 loss = 0.32576018093981907:\n",
            "iteration 468 loss = 0.32576018093981907:\n",
            "iteration 469 loss = 0.32576018093981907:\n",
            "iteration 470 loss = 0.32576018093981907:\n",
            "iteration 471 loss = 0.32576018093981907:\n",
            "iteration 472 loss = 0.32576018093981907:\n",
            "iteration 473 loss = 0.32576018093981907:\n",
            "iteration 474 loss = 0.32576018093981907:\n",
            "iteration 475 loss = 0.32576018093981907:\n",
            "iteration 476 loss = 0.32576018093981907:\n",
            "iteration 477 loss = 0.32576018093981907:\n",
            "iteration 478 loss = 0.32576018093981907:\n",
            "iteration 479 loss = 0.32576018093981907:\n",
            "iteration 480 loss = 0.32576018093981907:\n",
            "iteration 481 loss = 0.32576018093981907:\n",
            "iteration 482 loss = 0.32576018093981907:\n",
            "iteration 483 loss = 0.32576018093981907:\n",
            "iteration 484 loss = 0.32576018093981907:\n",
            "iteration 485 loss = 0.32576018093981907:\n",
            "iteration 486 loss = 0.32576018093981907:\n",
            "iteration 487 loss = 0.32576018093981907:\n",
            "iteration 488 loss = 0.32576018093981907:\n",
            "iteration 489 loss = 0.32576018093981907:\n",
            "iteration 490 loss = 0.32576018093981907:\n",
            "iteration 491 loss = 0.32576018093981907:\n",
            "iteration 492 loss = 0.32576018093981907:\n",
            "iteration 493 loss = 0.32576018093981907:\n",
            "iteration 494 loss = 0.32576018093981907:\n",
            "iteration 495 loss = 0.32576018093981907:\n",
            "iteration 496 loss = 0.32576018093981907:\n",
            "iteration 497 loss = 0.32576018093981907:\n",
            "iteration 498 loss = 0.32576018093981907:\n",
            "iteration 499 loss = 0.32576018093981907:\n",
            "iteration 500 loss = 0.32576018093981907:\n",
            "iteration 501 loss = 0.32576018093981907:\n",
            "iteration 502 loss = 0.32576018093981907:\n",
            "iteration 503 loss = 0.32576018093981907:\n",
            "iteration 504 loss = 0.32576018093981907:\n",
            "iteration 505 loss = 0.32576018093981907:\n",
            "iteration 506 loss = 0.32576018093981907:\n",
            "iteration 507 loss = 0.32576018093981907:\n",
            "iteration 508 loss = 0.32576018093981907:\n",
            "iteration 509 loss = 0.32576018093981907:\n",
            "iteration 510 loss = 0.32576018093981907:\n",
            "iteration 511 loss = 0.32576018093981907:\n",
            "iteration 512 loss = 0.32576018093981907:\n",
            "iteration 513 loss = 0.32576018093981907:\n",
            "iteration 514 loss = 0.32576018093981907:\n",
            "iteration 515 loss = 0.32576018093981907:\n",
            "iteration 516 loss = 0.32576018093981907:\n",
            "iteration 517 loss = 0.32576018093981907:\n",
            "iteration 518 loss = 0.32576018093981907:\n",
            "iteration 519 loss = 0.32576018093981907:\n",
            "iteration 520 loss = 0.32576018093981907:\n",
            "iteration 521 loss = 0.32576018093981907:\n",
            "iteration 522 loss = 0.32576018093981907:\n",
            "iteration 523 loss = 0.32576018093981907:\n",
            "iteration 524 loss = 0.32576018093981907:\n",
            "iteration 525 loss = 0.32576018093981907:\n",
            "iteration 526 loss = 0.32576018093981907:\n",
            "iteration 527 loss = 0.32576018093981907:\n",
            "iteration 528 loss = 0.32576018093981907:\n",
            "iteration 529 loss = 0.32576018093981907:\n",
            "iteration 530 loss = 0.32576018093981907:\n",
            "iteration 531 loss = 0.32576018093981907:\n",
            "iteration 532 loss = 0.32576018093981907:\n",
            "iteration 533 loss = 0.32576018093981907:\n",
            "iteration 534 loss = 0.32576018093981907:\n",
            "iteration 535 loss = 0.32576018093981907:\n",
            "iteration 536 loss = 0.32576018093981907:\n",
            "iteration 537 loss = 0.32576018093981907:\n",
            "iteration 538 loss = 0.32576018093981907:\n",
            "iteration 539 loss = 0.32576018093981907:\n",
            "iteration 540 loss = 0.32576018093981907:\n",
            "iteration 541 loss = 0.32576018093981907:\n",
            "iteration 542 loss = 0.32576018093981907:\n",
            "iteration 543 loss = 0.32576018093981907:\n",
            "iteration 544 loss = 0.32576018093981907:\n",
            "iteration 545 loss = 0.32576018093981907:\n",
            "iteration 546 loss = 0.32576018093981907:\n",
            "iteration 547 loss = 0.32576018093981907:\n",
            "iteration 548 loss = 0.32576018093981907:\n",
            "iteration 549 loss = 0.32576018093981907:\n",
            "iteration 550 loss = 0.32576018093981907:\n",
            "iteration 551 loss = 0.32576018093981907:\n",
            "iteration 552 loss = 0.32576018093981907:\n",
            "iteration 553 loss = 0.32576018093981907:\n",
            "iteration 554 loss = 0.32576018093981907:\n",
            "iteration 555 loss = 0.32576018093981907:\n",
            "iteration 556 loss = 0.32576018093981907:\n",
            "iteration 557 loss = 0.32576018093981907:\n",
            "iteration 558 loss = 0.32576018093981907:\n",
            "iteration 559 loss = 0.32576018093981907:\n",
            "iteration 560 loss = 0.32576018093981907:\n",
            "iteration 561 loss = 0.32576018093981907:\n",
            "iteration 562 loss = 0.32576018093981907:\n",
            "iteration 563 loss = 0.32576018093981907:\n",
            "iteration 564 loss = 0.32576018093981907:\n",
            "iteration 565 loss = 0.32576018093981907:\n",
            "iteration 566 loss = 0.32576018093981907:\n",
            "iteration 567 loss = 0.32576018093981907:\n",
            "iteration 568 loss = 0.32576018093981907:\n",
            "iteration 569 loss = 0.32576018093981907:\n",
            "iteration 570 loss = 0.32576018093981907:\n",
            "iteration 571 loss = 0.32576018093981907:\n",
            "iteration 572 loss = 0.32576018093981907:\n",
            "iteration 573 loss = 0.32576018093981907:\n",
            "iteration 574 loss = 0.32576018093981907:\n",
            "iteration 575 loss = 0.32576018093981907:\n",
            "iteration 576 loss = 0.32576018093981907:\n",
            "iteration 577 loss = 0.32576018093981907:\n",
            "iteration 578 loss = 0.32576018093981907:\n",
            "iteration 579 loss = 0.32576018093981907:\n",
            "iteration 580 loss = 0.32576018093981907:\n",
            "iteration 581 loss = 0.32576018093981907:\n",
            "iteration 582 loss = 0.32576018093981907:\n",
            "iteration 583 loss = 0.32576018093981907:\n",
            "iteration 584 loss = 0.32576018093981907:\n",
            "iteration 585 loss = 0.32576018093981907:\n",
            "iteration 586 loss = 0.32576018093981907:\n",
            "iteration 587 loss = 0.32576018093981907:\n",
            "iteration 588 loss = 0.32576018093981907:\n",
            "iteration 589 loss = 0.32576018093981907:\n",
            "iteration 590 loss = 0.32576018093981907:\n",
            "iteration 591 loss = 0.32576018093981907:\n",
            "iteration 592 loss = 0.32576018093981907:\n",
            "iteration 593 loss = 0.32576018093981907:\n",
            "iteration 594 loss = 0.32576018093981907:\n",
            "iteration 595 loss = 0.32576018093981907:\n",
            "iteration 596 loss = 0.32576018093981907:\n",
            "iteration 597 loss = 0.32576018093981907:\n",
            "iteration 598 loss = 0.32576018093981907:\n",
            "iteration 599 loss = 0.32576018093981907:\n",
            "iteration 600 loss = 0.32576018093981907:\n",
            "iteration 601 loss = 0.32576018093981907:\n",
            "iteration 602 loss = 0.32576018093981907:\n",
            "iteration 603 loss = 0.32576018093981907:\n",
            "iteration 604 loss = 0.32576018093981907:\n",
            "iteration 605 loss = 0.32576018093981907:\n",
            "iteration 606 loss = 0.32576018093981907:\n",
            "iteration 607 loss = 0.32576018093981907:\n",
            "iteration 608 loss = 0.32576018093981907:\n",
            "iteration 609 loss = 0.32576018093981907:\n",
            "iteration 610 loss = 0.32576018093981907:\n",
            "iteration 611 loss = 0.32576018093981907:\n",
            "iteration 612 loss = 0.32576018093981907:\n",
            "iteration 613 loss = 0.32576018093981907:\n",
            "iteration 614 loss = 0.32576018093981907:\n",
            "iteration 615 loss = 0.32576018093981907:\n",
            "iteration 616 loss = 0.32576018093981907:\n",
            "iteration 617 loss = 0.32576018093981907:\n",
            "iteration 618 loss = 0.32576018093981907:\n",
            "iteration 619 loss = 0.32576018093981907:\n",
            "iteration 620 loss = 0.32576018093981907:\n",
            "iteration 621 loss = 0.32576018093981907:\n",
            "iteration 622 loss = 0.32576018093981907:\n",
            "iteration 623 loss = 0.32576018093981907:\n",
            "iteration 624 loss = 0.32576018093981907:\n",
            "iteration 625 loss = 0.32576018093981907:\n",
            "iteration 626 loss = 0.32576018093981907:\n",
            "iteration 627 loss = 0.32576018093981907:\n",
            "iteration 628 loss = 0.32576018093981907:\n",
            "iteration 629 loss = 0.32576018093981907:\n",
            "iteration 630 loss = 0.32576018093981907:\n",
            "iteration 631 loss = 0.32576018093981907:\n",
            "iteration 632 loss = 0.32576018093981907:\n",
            "iteration 633 loss = 0.32576018093981907:\n",
            "iteration 634 loss = 0.32576018093981907:\n",
            "iteration 635 loss = 0.32576018093981907:\n",
            "iteration 636 loss = 0.32576018093981907:\n",
            "iteration 637 loss = 0.32576018093981907:\n",
            "iteration 638 loss = 0.32576018093981907:\n",
            "iteration 639 loss = 0.32576018093981907:\n",
            "iteration 640 loss = 0.32576018093981907:\n",
            "iteration 641 loss = 0.32576018093981907:\n",
            "iteration 642 loss = 0.32576018093981907:\n",
            "iteration 643 loss = 0.32576018093981907:\n",
            "iteration 644 loss = 0.32576018093981907:\n",
            "iteration 645 loss = 0.32576018093981907:\n",
            "iteration 646 loss = 0.32576018093981907:\n",
            "iteration 647 loss = 0.32576018093981907:\n",
            "iteration 648 loss = 0.32576018093981907:\n",
            "iteration 649 loss = 0.32576018093981907:\n",
            "iteration 650 loss = 0.32576018093981907:\n",
            "iteration 651 loss = 0.32576018093981907:\n",
            "iteration 652 loss = 0.32576018093981907:\n",
            "iteration 653 loss = 0.32576018093981907:\n",
            "iteration 654 loss = 0.32576018093981907:\n",
            "iteration 655 loss = 0.32576018093981907:\n",
            "iteration 656 loss = 0.32576018093981907:\n",
            "iteration 657 loss = 0.32576018093981907:\n",
            "iteration 658 loss = 0.32576018093981907:\n",
            "iteration 659 loss = 0.32576018093981907:\n",
            "iteration 660 loss = 0.32576018093981907:\n",
            "iteration 661 loss = 0.32576018093981907:\n",
            "iteration 662 loss = 0.32576018093981907:\n",
            "iteration 663 loss = 0.32576018093981907:\n",
            "iteration 664 loss = 0.32576018093981907:\n",
            "iteration 665 loss = 0.32576018093981907:\n",
            "iteration 666 loss = 0.32576018093981907:\n",
            "iteration 667 loss = 0.32576018093981907:\n",
            "iteration 668 loss = 0.32576018093981907:\n",
            "iteration 669 loss = 0.32576018093981907:\n",
            "iteration 670 loss = 0.32576018093981907:\n",
            "iteration 671 loss = 0.32576018093981907:\n",
            "iteration 672 loss = 0.32576018093981907:\n",
            "iteration 673 loss = 0.32576018093981907:\n",
            "iteration 674 loss = 0.32576018093981907:\n",
            "iteration 675 loss = 0.32576018093981907:\n",
            "iteration 676 loss = 0.32576018093981907:\n",
            "iteration 677 loss = 0.32576018093981907:\n",
            "iteration 678 loss = 0.32576018093981907:\n",
            "iteration 679 loss = 0.32576018093981907:\n",
            "iteration 680 loss = 0.32576018093981907:\n",
            "iteration 681 loss = 0.32576018093981907:\n",
            "iteration 682 loss = 0.32576018093981907:\n",
            "iteration 683 loss = 0.32576018093981907:\n",
            "iteration 684 loss = 0.32576018093981907:\n",
            "iteration 685 loss = 0.32576018093981907:\n",
            "iteration 686 loss = 0.32576018093981907:\n",
            "iteration 687 loss = 0.32576018093981907:\n",
            "iteration 688 loss = 0.32576018093981907:\n",
            "iteration 689 loss = 0.32576018093981907:\n",
            "iteration 690 loss = 0.32576018093981907:\n",
            "iteration 691 loss = 0.32576018093981907:\n",
            "iteration 692 loss = 0.32576018093981907:\n",
            "iteration 693 loss = 0.32576018093981907:\n",
            "iteration 694 loss = 0.32576018093981907:\n",
            "iteration 695 loss = 0.32576018093981907:\n",
            "iteration 696 loss = 0.32576018093981907:\n",
            "iteration 697 loss = 0.32576018093981907:\n",
            "iteration 698 loss = 0.32576018093981907:\n",
            "iteration 699 loss = 0.32576018093981907:\n",
            "iteration 700 loss = 0.32576018093981907:\n",
            "iteration 701 loss = 0.32576018093981907:\n",
            "iteration 702 loss = 0.32576018093981907:\n",
            "iteration 703 loss = 0.32576018093981907:\n",
            "iteration 704 loss = 0.32576018093981907:\n",
            "iteration 705 loss = 0.32576018093981907:\n",
            "iteration 706 loss = 0.32576018093981907:\n",
            "iteration 707 loss = 0.32576018093981907:\n",
            "iteration 708 loss = 0.32576018093981907:\n",
            "iteration 709 loss = 0.32576018093981907:\n",
            "iteration 710 loss = 0.32576018093981907:\n",
            "iteration 711 loss = 0.32576018093981907:\n",
            "iteration 712 loss = 0.32576018093981907:\n",
            "iteration 713 loss = 0.32576018093981907:\n",
            "iteration 714 loss = 0.32576018093981907:\n",
            "iteration 715 loss = 0.32576018093981907:\n",
            "iteration 716 loss = 0.32576018093981907:\n",
            "iteration 717 loss = 0.32576018093981907:\n",
            "iteration 718 loss = 0.32576018093981907:\n",
            "iteration 719 loss = 0.32576018093981907:\n",
            "iteration 720 loss = 0.32576018093981907:\n",
            "iteration 721 loss = 0.32576018093981907:\n",
            "iteration 722 loss = 0.32576018093981907:\n",
            "iteration 723 loss = 0.32576018093981907:\n",
            "iteration 724 loss = 0.32576018093981907:\n",
            "iteration 725 loss = 0.32576018093981907:\n",
            "iteration 726 loss = 0.32576018093981907:\n",
            "iteration 727 loss = 0.32576018093981907:\n",
            "iteration 728 loss = 0.32576018093981907:\n",
            "iteration 729 loss = 0.32576018093981907:\n",
            "iteration 730 loss = 0.32576018093981907:\n",
            "iteration 731 loss = 0.32576018093981907:\n",
            "iteration 732 loss = 0.32576018093981907:\n",
            "iteration 733 loss = 0.32576018093981907:\n",
            "iteration 734 loss = 0.32576018093981907:\n",
            "iteration 735 loss = 0.32576018093981907:\n",
            "iteration 736 loss = 0.32576018093981907:\n",
            "iteration 737 loss = 0.32576018093981907:\n",
            "iteration 738 loss = 0.32576018093981907:\n",
            "iteration 739 loss = 0.32576018093981907:\n",
            "iteration 740 loss = 0.32576018093981907:\n",
            "iteration 741 loss = 0.32576018093981907:\n",
            "iteration 742 loss = 0.32576018093981907:\n",
            "iteration 743 loss = 0.32576018093981907:\n",
            "iteration 744 loss = 0.32576018093981907:\n",
            "iteration 745 loss = 0.32576018093981907:\n",
            "iteration 746 loss = 0.32576018093981907:\n",
            "iteration 747 loss = 0.32576018093981907:\n",
            "iteration 748 loss = 0.32576018093981907:\n",
            "iteration 749 loss = 0.32576018093981907:\n",
            "iteration 750 loss = 0.32576018093981907:\n",
            "iteration 751 loss = 0.32576018093981907:\n",
            "iteration 752 loss = 0.32576018093981907:\n",
            "iteration 753 loss = 0.32576018093981907:\n",
            "iteration 754 loss = 0.32576018093981907:\n",
            "iteration 755 loss = 0.32576018093981907:\n",
            "iteration 756 loss = 0.32576018093981907:\n",
            "iteration 757 loss = 0.32576018093981907:\n",
            "iteration 758 loss = 0.32576018093981907:\n",
            "iteration 759 loss = 0.32576018093981907:\n",
            "iteration 760 loss = 0.32576018093981907:\n",
            "iteration 761 loss = 0.32576018093981907:\n",
            "iteration 762 loss = 0.32576018093981907:\n",
            "iteration 763 loss = 0.32576018093981907:\n",
            "iteration 764 loss = 0.32576018093981907:\n",
            "iteration 765 loss = 0.32576018093981907:\n",
            "iteration 766 loss = 0.32576018093981907:\n",
            "iteration 767 loss = 0.32576018093981907:\n",
            "iteration 768 loss = 0.32576018093981907:\n",
            "iteration 769 loss = 0.32576018093981907:\n",
            "iteration 770 loss = 0.32576018093981907:\n",
            "iteration 771 loss = 0.32576018093981907:\n",
            "iteration 772 loss = 0.32576018093981907:\n",
            "iteration 773 loss = 0.32576018093981907:\n",
            "iteration 774 loss = 0.32576018093981907:\n",
            "iteration 775 loss = 0.32576018093981907:\n",
            "iteration 776 loss = 0.32576018093981907:\n",
            "iteration 777 loss = 0.32576018093981907:\n",
            "iteration 778 loss = 0.32576018093981907:\n",
            "iteration 779 loss = 0.32576018093981907:\n",
            "iteration 780 loss = 0.32576018093981907:\n",
            "iteration 781 loss = 0.32576018093981907:\n",
            "iteration 782 loss = 0.32576018093981907:\n",
            "iteration 783 loss = 0.32576018093981907:\n",
            "iteration 784 loss = 0.32576018093981907:\n",
            "iteration 785 loss = 0.32576018093981907:\n",
            "iteration 786 loss = 0.32576018093981907:\n",
            "iteration 787 loss = 0.32576018093981907:\n",
            "iteration 788 loss = 0.32576018093981907:\n",
            "iteration 789 loss = 0.32576018093981907:\n",
            "iteration 790 loss = 0.32576018093981907:\n",
            "iteration 791 loss = 0.32576018093981907:\n",
            "iteration 792 loss = 0.32576018093981907:\n",
            "iteration 793 loss = 0.32576018093981907:\n",
            "iteration 794 loss = 0.32576018093981907:\n",
            "iteration 795 loss = 0.32576018093981907:\n",
            "iteration 796 loss = 0.32576018093981907:\n",
            "iteration 797 loss = 0.32576018093981907:\n",
            "iteration 798 loss = 0.32576018093981907:\n",
            "iteration 799 loss = 0.32576018093981907:\n",
            "iteration 800 loss = 0.32576018093981907:\n",
            "iteration 801 loss = 0.32576018093981907:\n",
            "iteration 802 loss = 0.32576018093981907:\n",
            "iteration 803 loss = 0.32576018093981907:\n",
            "iteration 804 loss = 0.32576018093981907:\n",
            "iteration 805 loss = 0.32576018093981907:\n",
            "iteration 806 loss = 0.32576018093981907:\n",
            "iteration 807 loss = 0.32576018093981907:\n",
            "iteration 808 loss = 0.32576018093981907:\n",
            "iteration 809 loss = 0.32576018093981907:\n",
            "iteration 810 loss = 0.32576018093981907:\n",
            "iteration 811 loss = 0.32576018093981907:\n",
            "iteration 812 loss = 0.32576018093981907:\n",
            "iteration 813 loss = 0.32576018093981907:\n",
            "iteration 814 loss = 0.32576018093981907:\n",
            "iteration 815 loss = 0.32576018093981907:\n",
            "iteration 816 loss = 0.32576018093981907:\n",
            "iteration 817 loss = 0.32576018093981907:\n",
            "iteration 818 loss = 0.32576018093981907:\n",
            "iteration 819 loss = 0.32576018093981907:\n",
            "iteration 820 loss = 0.32576018093981907:\n",
            "iteration 821 loss = 0.32576018093981907:\n",
            "iteration 822 loss = 0.32576018093981907:\n",
            "iteration 823 loss = 0.32576018093981907:\n",
            "iteration 824 loss = 0.32576018093981907:\n",
            "iteration 825 loss = 0.32576018093981907:\n",
            "iteration 826 loss = 0.32576018093981907:\n",
            "iteration 827 loss = 0.32576018093981907:\n",
            "iteration 828 loss = 0.32576018093981907:\n",
            "iteration 829 loss = 0.32576018093981907:\n",
            "iteration 830 loss = 0.32576018093981907:\n",
            "iteration 831 loss = 0.32576018093981907:\n",
            "iteration 832 loss = 0.32576018093981907:\n",
            "iteration 833 loss = 0.32576018093981907:\n",
            "iteration 834 loss = 0.32576018093981907:\n",
            "iteration 835 loss = 0.32576018093981907:\n",
            "iteration 836 loss = 0.32576018093981907:\n",
            "iteration 837 loss = 0.32576018093981907:\n",
            "iteration 838 loss = 0.32576018093981907:\n",
            "iteration 839 loss = 0.32576018093981907:\n",
            "iteration 840 loss = 0.32576018093981907:\n",
            "iteration 841 loss = 0.32576018093981907:\n",
            "iteration 842 loss = 0.32576018093981907:\n",
            "iteration 843 loss = 0.32576018093981907:\n",
            "iteration 844 loss = 0.32576018093981907:\n",
            "iteration 845 loss = 0.32576018093981907:\n",
            "iteration 846 loss = 0.32576018093981907:\n",
            "iteration 847 loss = 0.32576018093981907:\n",
            "iteration 848 loss = 0.32576018093981907:\n",
            "iteration 849 loss = 0.32576018093981907:\n",
            "iteration 850 loss = 0.32576018093981907:\n",
            "iteration 851 loss = 0.32576018093981907:\n",
            "iteration 852 loss = 0.32576018093981907:\n",
            "iteration 853 loss = 0.32576018093981907:\n",
            "iteration 854 loss = 0.32576018093981907:\n",
            "iteration 855 loss = 0.32576018093981907:\n",
            "iteration 856 loss = 0.32576018093981907:\n",
            "iteration 857 loss = 0.32576018093981907:\n",
            "iteration 858 loss = 0.32576018093981907:\n",
            "iteration 859 loss = 0.32576018093981907:\n",
            "iteration 860 loss = 0.32576018093981907:\n",
            "iteration 861 loss = 0.32576018093981907:\n",
            "iteration 862 loss = 0.32576018093981907:\n",
            "iteration 863 loss = 0.32576018093981907:\n",
            "iteration 864 loss = 0.32576018093981907:\n",
            "iteration 865 loss = 0.32576018093981907:\n",
            "iteration 866 loss = 0.32576018093981907:\n",
            "iteration 867 loss = 0.32576018093981907:\n",
            "iteration 868 loss = 0.32576018093981907:\n",
            "iteration 869 loss = 0.32576018093981907:\n",
            "iteration 870 loss = 0.32576018093981907:\n",
            "iteration 871 loss = 0.32576018093981907:\n",
            "iteration 872 loss = 0.32576018093981907:\n",
            "iteration 873 loss = 0.32576018093981907:\n",
            "iteration 874 loss = 0.32576018093981907:\n",
            "iteration 875 loss = 0.32576018093981907:\n",
            "iteration 876 loss = 0.32576018093981907:\n",
            "iteration 877 loss = 0.32576018093981907:\n",
            "iteration 878 loss = 0.32576018093981907:\n",
            "iteration 879 loss = 0.32576018093981907:\n",
            "iteration 880 loss = 0.32576018093981907:\n",
            "iteration 881 loss = 0.32576018093981907:\n",
            "iteration 882 loss = 0.32576018093981907:\n",
            "iteration 883 loss = 0.32576018093981907:\n",
            "iteration 884 loss = 0.32576018093981907:\n",
            "iteration 885 loss = 0.32576018093981907:\n",
            "iteration 886 loss = 0.32576018093981907:\n",
            "iteration 887 loss = 0.32576018093981907:\n",
            "iteration 888 loss = 0.32576018093981907:\n",
            "iteration 889 loss = 0.32576018093981907:\n",
            "iteration 890 loss = 0.32576018093981907:\n",
            "iteration 891 loss = 0.32576018093981907:\n",
            "iteration 892 loss = 0.32576018093981907:\n",
            "iteration 893 loss = 0.32576018093981907:\n",
            "iteration 894 loss = 0.32576018093981907:\n",
            "iteration 895 loss = 0.32576018093981907:\n",
            "iteration 896 loss = 0.32576018093981907:\n",
            "iteration 897 loss = 0.32576018093981907:\n",
            "iteration 898 loss = 0.32576018093981907:\n",
            "iteration 899 loss = 0.32576018093981907:\n",
            "iteration 900 loss = 0.32576018093981907:\n",
            "iteration 901 loss = 0.32576018093981907:\n",
            "iteration 902 loss = 0.32576018093981907:\n",
            "iteration 903 loss = 0.32576018093981907:\n",
            "iteration 904 loss = 0.32576018093981907:\n",
            "iteration 905 loss = 0.32576018093981907:\n",
            "iteration 906 loss = 0.32576018093981907:\n",
            "iteration 907 loss = 0.32576018093981907:\n",
            "iteration 908 loss = 0.32576018093981907:\n",
            "iteration 909 loss = 0.32576018093981907:\n",
            "iteration 910 loss = 0.32576018093981907:\n",
            "iteration 911 loss = 0.32576018093981907:\n",
            "iteration 912 loss = 0.32576018093981907:\n",
            "iteration 913 loss = 0.32576018093981907:\n",
            "iteration 914 loss = 0.32576018093981907:\n",
            "iteration 915 loss = 0.32576018093981907:\n",
            "iteration 916 loss = 0.32576018093981907:\n",
            "iteration 917 loss = 0.32576018093981907:\n",
            "iteration 918 loss = 0.32576018093981907:\n",
            "iteration 919 loss = 0.32576018093981907:\n",
            "iteration 920 loss = 0.32576018093981907:\n",
            "iteration 921 loss = 0.32576018093981907:\n",
            "iteration 922 loss = 0.32576018093981907:\n",
            "iteration 923 loss = 0.32576018093981907:\n",
            "iteration 924 loss = 0.32576018093981907:\n",
            "iteration 925 loss = 0.32576018093981907:\n",
            "iteration 926 loss = 0.32576018093981907:\n",
            "iteration 927 loss = 0.32576018093981907:\n",
            "iteration 928 loss = 0.32576018093981907:\n",
            "iteration 929 loss = 0.32576018093981907:\n",
            "iteration 930 loss = 0.32576018093981907:\n",
            "iteration 931 loss = 0.32576018093981907:\n",
            "iteration 932 loss = 0.32576018093981907:\n",
            "iteration 933 loss = 0.32576018093981907:\n",
            "iteration 934 loss = 0.32576018093981907:\n",
            "iteration 935 loss = 0.32576018093981907:\n",
            "iteration 936 loss = 0.32576018093981907:\n",
            "iteration 937 loss = 0.32576018093981907:\n",
            "iteration 938 loss = 0.32576018093981907:\n",
            "iteration 939 loss = 0.32576018093981907:\n",
            "iteration 940 loss = 0.32576018093981907:\n",
            "iteration 941 loss = 0.32576018093981907:\n",
            "iteration 942 loss = 0.32576018093981907:\n",
            "iteration 943 loss = 0.32576018093981907:\n",
            "iteration 944 loss = 0.32576018093981907:\n",
            "iteration 945 loss = 0.32576018093981907:\n",
            "iteration 946 loss = 0.32576018093981907:\n",
            "iteration 947 loss = 0.32576018093981907:\n",
            "iteration 948 loss = 0.32576018093981907:\n",
            "iteration 949 loss = 0.32576018093981907:\n",
            "iteration 950 loss = 0.32576018093981907:\n",
            "iteration 951 loss = 0.32576018093981907:\n",
            "iteration 952 loss = 0.32576018093981907:\n",
            "iteration 953 loss = 0.32576018093981907:\n",
            "iteration 954 loss = 0.32576018093981907:\n",
            "iteration 955 loss = 0.32576018093981907:\n",
            "iteration 956 loss = 0.32576018093981907:\n",
            "iteration 957 loss = 0.32576018093981907:\n",
            "iteration 958 loss = 0.32576018093981907:\n",
            "iteration 959 loss = 0.32576018093981907:\n",
            "iteration 960 loss = 0.32576018093981907:\n",
            "iteration 961 loss = 0.32576018093981907:\n",
            "iteration 962 loss = 0.32576018093981907:\n",
            "iteration 963 loss = 0.32576018093981907:\n",
            "iteration 964 loss = 0.32576018093981907:\n",
            "iteration 965 loss = 0.32576018093981907:\n",
            "iteration 966 loss = 0.32576018093981907:\n",
            "iteration 967 loss = 0.32576018093981907:\n",
            "iteration 968 loss = 0.32576018093981907:\n",
            "iteration 969 loss = 0.32576018093981907:\n",
            "iteration 970 loss = 0.32576018093981907:\n",
            "iteration 971 loss = 0.32576018093981907:\n",
            "iteration 972 loss = 0.32576018093981907:\n",
            "iteration 973 loss = 0.32576018093981907:\n",
            "iteration 974 loss = 0.32576018093981907:\n",
            "iteration 975 loss = 0.32576018093981907:\n",
            "iteration 976 loss = 0.32576018093981907:\n",
            "iteration 977 loss = 0.32576018093981907:\n",
            "iteration 978 loss = 0.32576018093981907:\n",
            "iteration 979 loss = 0.32576018093981907:\n",
            "iteration 980 loss = 0.32576018093981907:\n",
            "iteration 981 loss = 0.32576018093981907:\n",
            "iteration 982 loss = 0.32576018093981907:\n",
            "iteration 983 loss = 0.32576018093981907:\n",
            "iteration 984 loss = 0.32576018093981907:\n",
            "iteration 985 loss = 0.32576018093981907:\n",
            "iteration 986 loss = 0.32576018093981907:\n",
            "iteration 987 loss = 0.32576018093981907:\n",
            "iteration 988 loss = 0.32576018093981907:\n",
            "iteration 989 loss = 0.32576018093981907:\n",
            "iteration 990 loss = 0.32576018093981907:\n",
            "iteration 991 loss = 0.32576018093981907:\n",
            "iteration 992 loss = 0.32576018093981907:\n",
            "iteration 993 loss = 0.32576018093981907:\n",
            "iteration 994 loss = 0.32576018093981907:\n",
            "iteration 995 loss = 0.32576018093981907:\n",
            "iteration 996 loss = 0.32576018093981907:\n",
            "iteration 997 loss = 0.32576018093981907:\n",
            "iteration 998 loss = 0.32576018093981907:\n",
            "iteration 999 loss = 0.32576018093981907:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "-XpRRdiAuKU7",
        "outputId": "88b4996a-90fa-4757-9504-09343ac859d6"
      },
      "source": [
        "plot_curve(range(num_iterations), losses)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sfSSgFEXI2p"
      },
      "source": [
        "### Gradient Descent Optimization\n",
        "\n",
        "In this technique, we would calculate partial derivatives w.r.t weights and biases of a convex function (differentiable functions).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqNktvHdXGiN",
        "outputId": "1c172280-23ca-48e1-cb97-3ec133286ac9"
      },
      "source": [
        "num_iterations = 1000\n",
        "lr = 0.001\n",
        "N = num_train_examples\n",
        "Wj = W.copy()\n",
        "bj = b.copy()\n",
        "best_loss = float(\"inf\")\n",
        "losses = []\n",
        "\n",
        "for i in range(num_iterations): \n",
        "  # error = (y - (W.x + b))\n",
        "  # L = 1/2 * (y - (W.x + b))**2\n",
        "\n",
        "  y_hat = X_train.dot(Wj) + bj \n",
        "  error = y_train - y_hat\n",
        "  loss = 1/2 * (y_train - y_hat)**2\n",
        "  \n",
        "  # dL/dW = [y - (W.x + b)] . (-x)\n",
        "  # dL/dW =  error . (-x)\n",
        "\n",
        "  dW = np.dot(-X_train.T, error) / N \n",
        "  db = -error / N\n",
        "\n",
        "  Wj = Wj - lr * dW \n",
        "  bj = bj - lr * db\n",
        "\n",
        "  mean_loss = np.sum(loss) / N\n",
        "  losses.append(mean_loss)\n",
        "\n",
        "  if mean_loss < best_loss:\n",
        "    best_loss = mean_loss\n",
        "\n",
        "  print(\"iter %s, loss is %s\" % (i, best_loss)) \n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss is 78.35036235953841\n",
            "iter 1, loss is 77.55345739716022\n",
            "iter 2, loss is 76.7647222072433\n",
            "iter 3, loss is 75.98407302683736\n",
            "iter 4, loss is 75.21142695179736\n",
            "iter 5, loss is 74.44670192797814\n",
            "iter 6, loss is 73.68981674251961\n",
            "iter 7, loss is 72.94069101522128\n",
            "iter 8, loss is 72.19924519000506\n",
            "iter 9, loss is 71.46540052646577\n",
            "iter 10, loss is 70.7390790915082\n",
            "iter 11, loss is 70.02020375106989\n",
            "iter 12, loss is 69.30869816192879\n",
            "iter 13, loss is 68.60448676359498\n",
            "iter 14, loss is 67.90749477028542\n",
            "iter 15, loss is 67.2176481629811\n",
            "iter 16, loss is 66.53487368156547\n",
            "iter 17, loss is 65.85909881704359\n",
            "iter 18, loss is 65.19025180384108\n",
            "iter 19, loss is 64.52826161218195\n",
            "iter 20, loss is 63.87305794054453\n",
            "iter 21, loss is 63.22457120819491\n",
            "iter 22, loss is 62.58273254779685\n",
            "iter 23, loss is 61.94747379809736\n",
            "iter 24, loss is 61.318727496687444\n",
            "iter 25, loss is 60.69642687283697\n",
            "iter 26, loss is 60.08050584040304\n",
            "iter 27, loss is 59.470898990811015\n",
            "iter 28, loss is 58.867541586107535\n",
            "iter 29, loss is 58.27036955208483\n",
            "iter 30, loss is 57.6793194714754\n",
            "iter 31, loss is 57.09432857721657\n",
            "iter 32, loss is 56.51533474578411\n",
            "iter 33, loss is 55.942276490594004\n",
            "iter 34, loss is 55.37509295547223\n",
            "iter 35, loss is 54.81372390819116\n",
            "iter 36, loss is 54.258109734072384\n",
            "iter 37, loss is 53.70819142965513\n",
            "iter 38, loss is 53.16391059642952\n",
            "iter 39, loss is 52.625209434634186\n",
            "iter 40, loss is 52.092030737117355\n",
            "iter 41, loss is 51.56431788326099\n",
            "iter 42, loss is 51.04201483296714\n",
            "iter 43, loss is 50.52506612070602\n",
            "iter 44, loss is 50.01341684962504\n",
            "iter 45, loss is 49.50701268571825\n",
            "iter 46, loss is 49.005799852055674\n",
            "iter 47, loss is 48.509725123071604\n",
            "iter 48, loss is 48.01873581891165\n",
            "iter 49, loss is 47.53277979983764\n",
            "iter 50, loss is 47.05180546068992\n",
            "iter 51, loss is 46.57576172540644\n",
            "iter 52, loss is 46.10459804159795\n",
            "iter 53, loss is 45.638264375179006\n",
            "iter 54, loss is 45.176711205053756\n",
            "iter 55, loss is 44.719889517856515\n",
            "iter 56, loss is 44.26775080274598\n",
            "iter 57, loss is 43.820247046253016\n",
            "iter 58, loss is 43.377330727181146\n",
            "iter 59, loss is 42.93895481155943\n",
            "iter 60, loss is 42.505072747647006\n",
            "iter 61, loss is 42.0756384609889\n",
            "iter 62, loss is 41.65060634952242\n",
            "iter 63, loss is 41.229931278733986\n",
            "iter 64, loss is 40.81356857686526\n",
            "iter 65, loss is 40.40147403016872\n",
            "iter 66, loss is 39.993603878211694\n",
            "iter 67, loss is 39.58991480922868\n",
            "iter 68, loss is 39.1903639555212\n",
            "iter 69, loss is 38.7949088889049\n",
            "iter 70, loss is 38.40350761620323\n",
            "iter 71, loss is 38.01611857478749\n",
            "iter 72, loss is 37.632700628162446\n",
            "iter 73, loss is 37.253213061597286\n",
            "iter 74, loss is 36.87761557780134\n",
            "iter 75, loss is 36.50586829264417\n",
            "iter 76, loss is 36.137931730919476\n",
            "iter 77, loss is 35.773766822152545\n",
            "iter 78, loss is 35.41333489645054\n",
            "iter 79, loss is 35.056597680395484\n",
            "iter 80, loss is 34.703517292979214\n",
            "iter 81, loss is 34.354056241580125\n",
            "iter 82, loss is 34.00817741798112\n",
            "iter 83, loss is 33.66584409442835\n",
            "iter 84, loss is 33.32701991973045\n",
            "iter 85, loss is 32.99166891539765\n",
            "iter 86, loss is 32.6597554718206\n",
            "iter 87, loss is 32.3312443444883\n",
            "iter 88, loss is 32.00610065024476\n",
            "iter 89, loss is 31.684289863584205\n",
            "iter 90, loss is 31.36577781298404\n",
            "iter 91, loss is 31.050530677275624\n",
            "iter 92, loss is 30.738514982052106\n",
            "iter 93, loss is 30.42969759611315\n",
            "iter 94, loss is 30.124045727946136\n",
            "iter 95, loss is 29.821526922243372\n",
            "iter 96, loss is 29.522109056455076\n",
            "iter 97, loss is 29.225760337377643\n",
            "iter 98, loss is 28.932449297776976\n",
            "iter 99, loss is 28.642144793046317\n",
            "iter 100, loss is 28.354815997898488\n",
            "iter 101, loss is 28.070432403091893\n",
            "iter 102, loss is 27.788963812190225\n",
            "iter 103, loss is 27.510380338355294\n",
            "iter 104, loss is 27.2346524011728\n",
            "iter 105, loss is 26.961750723510626\n",
            "iter 106, loss is 26.69164632840934\n",
            "iter 107, loss is 26.424310536004597\n",
            "iter 108, loss is 26.159714960481043\n",
            "iter 109, loss is 25.89783150705754\n",
            "iter 110, loss is 25.638632369003208\n",
            "iter 111, loss is 25.382090024684125\n",
            "iter 112, loss is 25.128177234640283\n",
            "iter 113, loss is 24.87686703869249\n",
            "iter 114, loss is 24.628132753079\n",
            "iter 115, loss is 24.38194796762141\n",
            "iter 116, loss is 24.138286542919705\n",
            "iter 117, loss is 23.897122607576\n",
            "iter 118, loss is 23.658430555446774\n",
            "iter 119, loss is 23.422185042923246\n",
            "iter 120, loss is 23.188360986239704\n",
            "iter 121, loss is 22.95693355880934\n",
            "iter 122, loss is 22.727878188587468\n",
            "iter 123, loss is 22.501170555461755\n",
            "iter 124, loss is 22.27678658866918\n",
            "iter 125, loss is 22.05470246423955\n",
            "iter 126, loss is 21.834894602465145\n",
            "iter 127, loss is 21.61733966539635\n",
            "iter 128, loss is 21.402014554362967\n",
            "iter 129, loss is 21.18889640752093\n",
            "iter 130, loss is 20.97796259742414\n",
            "iter 131, loss is 20.769190728621293\n",
            "iter 132, loss is 20.562558635277217\n",
            "iter 133, loss is 20.3580443788187\n",
            "iter 134, loss is 20.15562624560441\n",
            "iter 135, loss is 19.955282744618707\n",
            "iter 136, loss is 19.75699260518911\n",
            "iter 137, loss is 19.560734774727155\n",
            "iter 138, loss is 19.366488416492444\n",
            "iter 139, loss is 19.174232907379565\n",
            "iter 140, loss is 18.983947835727747\n",
            "iter 141, loss is 18.79561299915298\n",
            "iter 142, loss is 18.609208402402317\n",
            "iter 143, loss is 18.424714255230192\n",
            "iter 144, loss is 18.24211097029656\n",
            "iter 145, loss is 18.061379161086503\n",
            "iter 146, loss is 17.88249963985122\n",
            "iter 147, loss is 17.705453415570144\n",
            "iter 148, loss is 17.53022169193389\n",
            "iter 149, loss is 17.356785865347938\n",
            "iter 150, loss is 17.18512752295677\n",
            "iter 151, loss is 17.01522844068824\n",
            "iter 152, loss is 16.847070581318036\n",
            "iter 153, loss is 16.680636092553954\n",
            "iter 154, loss is 16.515907305139823\n",
            "iter 155, loss is 16.35286673097889\n",
            "iter 156, loss is 16.1914970612764\n",
            "iter 157, loss is 16.03178116470128\n",
            "iter 158, loss is 15.873702085566608\n",
            "iter 159, loss is 15.717243042028786\n",
            "iter 160, loss is 15.562387424305154\n",
            "iter 161, loss is 15.409118792909883\n",
            "iter 162, loss is 15.25742087690796\n",
            "iter 163, loss is 15.107277572187053\n",
            "iter 164, loss is 14.958672939747133\n",
            "iter 165, loss is 14.811591204007591\n",
            "iter 166, loss is 14.666016751131746\n",
            "iter 167, loss is 14.521934127368501\n",
            "iter 168, loss is 14.379328037411025\n",
            "iter 169, loss is 14.23818334277226\n",
            "iter 170, loss is 14.098485060177067\n",
            "iter 171, loss is 13.96021835997088\n",
            "iter 172, loss is 13.823368564544653\n",
            "iter 173, loss is 13.687921146775977\n",
            "iter 174, loss is 13.553861728486162\n",
            "iter 175, loss is 13.421176078913154\n",
            "iter 176, loss is 13.289850113200094\n",
            "iter 177, loss is 13.159869890899381\n",
            "iter 178, loss is 13.031221614492086\n",
            "iter 179, loss is 12.903891627922507\n",
            "iter 180, loss is 12.777866415147793\n",
            "iter 181, loss is 12.653132598702415\n",
            "iter 182, loss is 12.529676938277364\n",
            "iter 183, loss is 12.407486329313903\n",
            "iter 184, loss is 12.28654780161175\n",
            "iter 185, loss is 12.166848517951529\n",
            "iter 186, loss is 12.048375772731342\n",
            "iter 187, loss is 11.931116990617317\n",
            "iter 188, loss is 11.815059725208007\n",
            "iter 189, loss is 11.700191657712452\n",
            "iter 190, loss is 11.58650059564183\n",
            "iter 191, loss is 11.4739744715145\n",
            "iter 192, loss is 11.362601341574328\n",
            "iter 193, loss is 11.252369384522147\n",
            "iter 194, loss is 11.14326690026025\n",
            "iter 195, loss is 11.035282308649712\n",
            "iter 196, loss is 10.92840414828048\n",
            "iter 197, loss is 10.822621075254089\n",
            "iter 198, loss is 10.717921861978814\n",
            "iter 199, loss is 10.614295395977209\n",
            "iter 200, loss is 10.51173067870587\n",
            "iter 201, loss is 10.410216824387284\n",
            "iter 202, loss is 10.30974305885366\n",
            "iter 203, loss is 10.210298718402628\n",
            "iter 204, loss is 10.111873248664637\n",
            "iter 205, loss is 10.014456203482014\n",
            "iter 206, loss is 9.91803724379947\n",
            "iter 207, loss is 9.822606136566003\n",
            "iter 208, loss is 9.728152753648065\n",
            "iter 209, loss is 9.634667070753855\n",
            "iter 210, loss is 9.542139166368651\n",
            "iter 211, loss is 9.450559220701066\n",
            "iter 212, loss is 9.35991751464009\n",
            "iter 213, loss is 9.270204428722824\n",
            "iter 214, loss is 9.181410442112835\n",
            "iter 215, loss is 9.093526131588929\n",
            "iter 216, loss is 9.006542170544343\n",
            "iter 217, loss is 8.920449327996161\n",
            "iter 218, loss is 8.835238467604908\n",
            "iter 219, loss is 8.750900546704193\n",
            "iter 220, loss is 8.667426615340288\n",
            "iter 221, loss is 8.584807815321572\n",
            "iter 222, loss is 8.503035379277703\n",
            "iter 223, loss is 8.422100629728456\n",
            "iter 224, loss is 8.34199497816209\n",
            "iter 225, loss is 8.262709924123168\n",
            "iter 226, loss is 8.18423705430975\n",
            "iter 227, loss is 8.106568041679795\n",
            "iter 228, loss is 8.029694644566794\n",
            "iter 229, loss is 7.953608705804395\n",
            "iter 230, loss is 7.878302151860065\n",
            "iter 231, loss is 7.803766991977592\n",
            "iter 232, loss is 7.729995317328407\n",
            "iter 233, loss is 7.65697930017159\n",
            "iter 234, loss is 7.584711193022497\n",
            "iter 235, loss is 7.513183327829904\n",
            "iter 236, loss is 7.442388115161597\n",
            "iter 237, loss is 7.372318043398306\n",
            "iter 238, loss is 7.302965677935895\n",
            "iter 239, loss is 7.234323660395757\n",
            "iter 240, loss is 7.166384707843273\n",
            "iter 241, loss is 7.099141612014308\n",
            "iter 242, loss is 7.032587238549632\n",
            "iter 243, loss is 6.966714526237178\n",
            "iter 244, loss is 6.901516486262082\n",
            "iter 245, loss is 6.836986201464416\n",
            "iter 246, loss is 6.773116825604511\n",
            "iter 247, loss is 6.709901582635838\n",
            "iter 248, loss is 6.647333765985323\n",
            "iter 249, loss is 6.585406737841053\n",
            "iter 250, loss is 6.52411392844728\n",
            "iter 251, loss is 6.463448835406655\n",
            "iter 252, loss is 6.403405022989614\n",
            "iter 253, loss is 6.343976121450852\n",
            "iter 254, loss is 6.285155826352789\n",
            "iter 255, loss is 6.226937897895993\n",
            "iter 256, loss is 6.16931616025645\n",
            "iter 257, loss is 6.1122845009296425\n",
            "iter 258, loss is 6.055836870081343\n",
            "iter 259, loss is 5.99996727990507\n",
            "iter 260, loss is 5.944669803986128\n",
            "iter 261, loss is 5.88993857667217\n",
            "iter 262, loss is 5.835767792450216\n",
            "iter 263, loss is 5.78215170533005\n",
            "iter 264, loss is 5.7290846282339505\n",
            "iter 265, loss is 5.676560932392667\n",
            "iter 266, loss is 5.6245750467476014\n",
            "iter 267, loss is 5.573121457359105\n",
            "iter 268, loss is 5.522194706820851\n",
            "iter 269, loss is 5.471789393680211\n",
            "iter 270, loss is 5.421900171864577\n",
            "iter 271, loss is 5.372521750113549\n",
            "iter 272, loss is 5.323648891416966\n",
            "iter 273, loss is 5.275276412458689\n",
            "iter 274, loss is 5.227399183066079\n",
            "iter 275, loss is 5.180012125665135\n",
            "iter 276, loss is 5.133110214741205\n",
            "iter 277, loss is 5.086688476305235\n",
            "iter 278, loss is 5.040741987365487\n",
            "iter 279, loss is 4.995265875404673\n",
            "iter 280, loss is 4.950255317862448\n",
            "iter 281, loss is 4.905705541623212\n",
            "iter 282, loss is 4.861611822509169\n",
            "iter 283, loss is 4.817969484778569\n",
            "iter 284, loss is 4.774773900629102\n",
            "iter 285, loss is 4.732020489706397\n",
            "iter 286, loss is 4.689704718617528\n",
            "iter 287, loss is 4.6478221004495515\n",
            "iter 288, loss is 4.60636819429293\n",
            "iter 289, loss is 4.565338604769888\n",
            "iter 290, loss is 4.524728981567577\n",
            "iter 291, loss is 4.48453501897603\n",
            "iter 292, loss is 4.444752455430864\n",
            "iter 293, loss is 4.405377073060658\n",
            "iter 294, loss is 4.366404697238981\n",
            "iter 295, loss is 4.327831196141009\n",
            "iter 296, loss is 4.289652480304685\n",
            "iter 297, loss is 4.251864502196391\n",
            "iter 298, loss is 4.214463255781049\n",
            "iter 299, loss is 4.177444776096663\n",
            "iter 300, loss is 4.140805138833188\n",
            "iter 301, loss is 4.104540459915744\n",
            "iter 302, loss is 4.068646895092088\n",
            "iter 303, loss is 4.033120639524322\n",
            "iter 304, loss is 3.997957927384785\n",
            "iter 305, loss is 3.963155031456089\n",
            "iter 306, loss is 3.928708262735255\n",
            "iter 307, loss is 3.8946139700419087\n",
            "iter 308, loss is 3.860868539630492\n",
            "iter 309, loss is 3.82746839480645\n",
            "iter 310, loss is 3.794409995546358\n",
            "iter 311, loss is 3.761689838121938\n",
            "iter 312, loss is 3.729304454727928\n",
            "iter 313, loss is 3.697250413113775\n",
            "iter 314, loss is 3.6655243162190994\n",
            "iter 315, loss is 3.634122801812893\n",
            "iter 316, loss is 3.603042542136424\n",
            "iter 317, loss is 3.5722802435497987\n",
            "iter 318, loss is 3.5418326461821468\n",
            "iter 319, loss is 3.5116965235853947\n",
            "iter 320, loss is 3.481868682391589\n",
            "iter 321, loss is 3.4523459619737302\n",
            "iter 322, loss is 3.4231252341100884\n",
            "iter 323, loss is 3.394203402651959\n",
            "iter 324, loss is 3.3655774031948202\n",
            "iter 325, loss is 3.33724420275287\n",
            "iter 326, loss is 3.3092007994368955\n",
            "iter 327, loss is 3.2814442221354447\n",
            "iter 328, loss is 3.253971530199268\n",
            "iter 329, loss is 3.2267798131289953\n",
            "iter 330, loss is 3.1998661902660186\n",
            "iter 331, loss is 3.1732278104865337\n",
            "iter 332, loss is 3.146861851898732\n",
            "iter 333, loss is 3.1207655215430874\n",
            "iter 334, loss is 3.0949360550957197\n",
            "iter 335, loss is 3.0693707165747997\n",
            "iter 336, loss is 3.0440667980499625\n",
            "iter 337, loss is 3.0190216193547017\n",
            "iter 338, loss is 2.9942325278017132\n",
            "iter 339, loss is 2.969696897901155\n",
            "iter 340, loss is 2.9454121310817976\n",
            "iter 341, loss is 2.9213756554150314\n",
            "iter 342, loss is 2.897584925341708\n",
            "iter 343, loss is 2.874037421401776\n",
            "iter 344, loss is 2.8507306499666933\n",
            "iter 345, loss is 2.827662142974584\n",
            "iter 346, loss is 2.804829457668103\n",
            "iter 347, loss is 2.782230176335001\n",
            "iter 348, loss is 2.7598619060513343\n",
            "iter 349, loss is 2.737722278427321\n",
            "iter 350, loss is 2.7158089493557944\n",
            "iter 351, loss is 2.694119598763237\n",
            "iter 352, loss is 2.6726519303633713\n",
            "iter 353, loss is 2.6514036714132727\n",
            "iter 354, loss is 2.6303725724719818\n",
            "iter 355, loss is 2.609556407161599\n",
            "iter 356, loss is 2.5889529719308166\n",
            "iter 357, loss is 2.5685600858208884\n",
            "iter 358, loss is 2.548375590233989\n",
            "iter 359, loss is 2.5283973487039537\n",
            "iter 360, loss is 2.508623246669365\n",
            "iter 361, loss is 2.489051191248968\n",
            "iter 362, loss is 2.469679111019391\n",
            "iter 363, loss is 2.4505049557951377\n",
            "iter 364, loss is 2.431526696410841\n",
            "iter 365, loss is 2.412742324505747\n",
            "iter 366, loss is 2.394149852310413\n",
            "iter 367, loss is 2.3757473124355846\n",
            "iter 368, loss is 2.3575327576632414\n",
            "iter 369, loss is 2.339504260739791\n",
            "iter 370, loss is 2.3216599141713696\n",
            "iter 371, loss is 2.303997830021255\n",
            "iter 372, loss is 2.2865161397093527\n",
            "iter 373, loss is 2.2692129938137264\n",
            "iter 374, loss is 2.2520865618741848\n",
            "iter 375, loss is 2.235135032197863\n",
            "iter 376, loss is 2.2183566116668088\n",
            "iter 377, loss is 2.2017495255475352\n",
            "iter 378, loss is 2.1853120173025284\n",
            "iter 379, loss is 2.169042348403691\n",
            "iter 380, loss is 2.15293879814769\n",
            "iter 381, loss is 2.1369996634732074\n",
            "iter 382, loss is 2.1212232587800584\n",
            "iter 383, loss is 2.105607915750164\n",
            "iter 384, loss is 2.090151983170365\n",
            "iter 385, loss is 2.0748538267570447\n",
            "iter 386, loss is 2.059711828982556\n",
            "iter 387, loss is 2.044724388903424\n",
            "iter 388, loss is 2.0298899219903146\n",
            "iter 389, loss is 2.0152068599597395\n",
            "iter 390, loss is 2.0006736506074945\n",
            "iter 391, loss is 1.986288757643798\n",
            "iter 392, loss is 1.9720506605301287\n",
            "iter 393, loss is 1.9579578543177247\n",
            "iter 394, loss is 1.9440088494877505\n",
            "iter 395, loss is 1.930202171793093\n",
            "iter 396, loss is 1.9165363621017855\n",
            "iter 397, loss is 1.9030099762420347\n",
            "iter 398, loss is 1.8896215848488365\n",
            "iter 399, loss is 1.8763697732121656\n",
            "iter 400, loss is 1.8632531411267201\n",
            "iter 401, loss is 1.8502703027432084\n",
            "iter 402, loss is 1.8374198864211582\n",
            "iter 403, loss is 1.8247005345832379\n",
            "iter 404, loss is 1.8121109035710703\n",
            "iter 405, loss is 1.7996496635025214\n",
            "iter 406, loss is 1.787315498130459\n",
            "iter 407, loss is 1.7751071047029534\n",
            "iter 408, loss is 1.763023193824914\n",
            "iter 409, loss is 1.7510624893211437\n",
            "iter 410, loss is 1.7392237281008005\n",
            "iter 411, loss is 1.7275056600232437\n",
            "iter 412, loss is 1.7159070477652585\n",
            "iter 413, loss is 1.7044266666896417\n",
            "iter 414, loss is 1.6930633047151342\n",
            "iter 415, loss is 1.6818157621876884\n",
            "iter 416, loss is 1.6706828517530532\n",
            "iter 417, loss is 1.659663398230667\n",
            "iter 418, loss is 1.6487562384888432\n",
            "iter 419, loss is 1.6379602213212365\n",
            "iter 420, loss is 1.6272742073245723\n",
            "iter 421, loss is 1.6166970687776354\n",
            "iter 422, loss is 1.6062276895214926\n",
            "iter 423, loss is 1.5958649648409493\n",
            "iter 424, loss is 1.5856078013472186\n",
            "iter 425, loss is 1.5754551168617938\n",
            "iter 426, loss is 1.5654058403015112\n",
            "iter 427, loss is 1.5554589115647914\n",
            "iter 428, loss is 1.545613281419049\n",
            "iter 429, loss is 1.5358679113892528\n",
            "iter 430, loss is 1.5262217736476333\n",
            "iter 431, loss is 1.5166738509045143\n",
            "iter 432, loss is 1.5072231363002726\n",
            "iter 433, loss is 1.4978686332983968\n",
            "iter 434, loss is 1.48860935557965\n",
            "iter 435, loss is 1.4794443269373112\n",
            "iter 436, loss is 1.4703725811734942\n",
            "iter 437, loss is 1.4613931619965306\n",
            "iter 438, loss is 1.4525051229194015\n",
            "iter 439, loss is 1.443707527159213\n",
            "iter 440, loss is 1.4349994475377028\n",
            "iter 441, loss is 1.426379966382763\n",
            "iter 442, loss is 1.4178481754309782\n",
            "iter 443, loss is 1.4094031757311591\n",
            "iter 444, loss is 1.4010440775488648\n",
            "iter 445, loss is 1.3927700002719081\n",
            "iter 446, loss is 1.384580072316823\n",
            "iter 447, loss is 1.376473431036299\n",
            "iter 448, loss is 1.3684492226275566\n",
            "iter 449, loss is 1.36050660204167\n",
            "iter 450, loss is 1.3526447328938125\n",
            "iter 451, loss is 1.344862787374426\n",
            "iter 452, loss is 1.337159946161301\n",
            "iter 453, loss is 1.3295353983325577\n",
            "iter 454, loss is 1.3219883412805176\n",
            "iter 455, loss is 1.314517980626461\n",
            "iter 456, loss is 1.3071235301362565\n",
            "iter 457, loss is 1.299804211636857\n",
            "iter 458, loss is 1.292559254933649\n",
            "iter 459, loss is 1.2853878977286535\n",
            "iter 460, loss is 1.2782893855395623\n",
            "iter 461, loss is 1.271262971619605\n",
            "iter 462, loss is 1.2643079168782396\n",
            "iter 463, loss is 1.2574234898026528\n",
            "iter 464, loss is 1.250608966380068\n",
            "iter 465, loss is 1.2438636300208497\n",
            "iter 466, loss is 1.2371867714823934\n",
            "iter 467, loss is 1.2305776887938005\n",
            "iter 468, loss is 1.22403568718132\n",
            "iter 469, loss is 1.2175600789945609\n",
            "iter 470, loss is 1.2111501836334542\n",
            "iter 471, loss is 1.2048053274759711\n",
            "iter 472, loss is 1.1985248438065748\n",
            "iter 473, loss is 1.1923080727454125\n",
            "iter 474, loss is 1.1861543611782284\n",
            "iter 475, loss is 1.1800630626869997\n",
            "iter 476, loss is 1.174033537481279\n",
            "iter 477, loss is 1.1680651523302439\n",
            "iter 478, loss is 1.162157280495443\n",
            "iter 479, loss is 1.1563093016642314\n",
            "iter 480, loss is 1.1505206018838863\n",
            "iter 481, loss is 1.144790573496401\n",
            "iter 482, loss is 1.1391186150739467\n",
            "iter 483, loss is 1.1335041313549958\n",
            "iter 484, loss is 1.1279465331810987\n",
            "iter 485, loss is 1.1224452374343117\n",
            "iter 486, loss is 1.116999666975264\n",
            "iter 487, loss is 1.111609250581861\n",
            "iter 488, loss is 1.1062734228886157\n",
            "iter 489, loss is 1.1009916243266011\n",
            "iter 490, loss is 1.0957633010640202\n",
            "iter 491, loss is 1.0905879049473841\n",
            "iter 492, loss is 1.0854648934432938\n",
            "iter 493, loss is 1.080393729580817\n",
            "iter 494, loss is 1.0753738818944594\n",
            "iter 495, loss is 1.0704048243677164\n",
            "iter 496, loss is 1.0654860363772078\n",
            "iter 497, loss is 1.0606170026373811\n",
            "iter 498, loss is 1.0557972131457853\n",
            "iter 499, loss is 1.0510261631289035\n",
            "iter 500, loss is 1.0463033529885417\n",
            "iter 501, loss is 1.0416282882487684\n",
            "iter 502, loss is 1.0370004795033951\n",
            "iter 503, loss is 1.0324194423639983\n",
            "iter 504, loss is 1.0278846974084739\n",
            "iter 505, loss is 1.023395770130117\n",
            "iter 506, loss is 1.018952190887227\n",
            "iter 507, loss is 1.0145534948532262\n",
            "iter 508, loss is 1.0101992219672924\n",
            "iter 509, loss is 1.0058889168854952\n",
            "iter 510, loss is 1.001622128932436\n",
            "iter 511, loss is 0.9973984120533823\n",
            "iter 512, loss is 0.993217324766892\n",
            "iter 513, loss is 0.989078430117925\n",
            "iter 514, loss is 0.9849812956314346\n",
            "iter 515, loss is 0.9809254932664359\n",
            "iter 516, loss is 0.9769105993705426\n",
            "iter 517, loss is 0.9729361946349736\n",
            "iter 518, loss is 0.9690018640500163\n",
            "iter 519, loss is 0.9651071968609523\n",
            "iter 520, loss is 0.9612517865244283\n",
            "iter 521, loss is 0.9574352306652806\n",
            "iter 522, loss is 0.9536571310337982\n",
            "iter 523, loss is 0.9499170934634258\n",
            "iter 524, loss is 0.946214727828899\n",
            "iter 525, loss is 0.9425496480048112\n",
            "iter 526, loss is 0.938921471824603\n",
            "iter 527, loss is 0.9353298210399729\n",
            "iter 528, loss is 0.931774321280704\n",
            "iter 529, loss is 0.9282546020149042\n",
            "iter 530, loss is 0.92477029650965\n",
            "iter 531, loss is 0.9213210417920396\n",
            "iter 532, loss is 0.9179064786106388\n",
            "iter 533, loss is 0.9145262513973285\n",
            "iter 534, loss is 0.9111800082295382\n",
            "iter 535, loss is 0.9078674007928703\n",
            "iter 536, loss is 0.9045880843441055\n",
            "iter 537, loss is 0.9013417176745886\n",
            "iter 538, loss is 0.8981279630739889\n",
            "iter 539, loss is 0.8949464862944324\n",
            "iter 540, loss is 0.8917969565150033\n",
            "iter 541, loss is 0.8886790463066063\n",
            "iter 542, loss is 0.8855924315971919\n",
            "iter 543, loss is 0.882536791637337\n",
            "iter 544, loss is 0.8795118089661791\n",
            "iter 545, loss is 0.8765171693776984\n",
            "iter 546, loss is 0.8735525618873475\n",
            "iter 547, loss is 0.8706176786990221\n",
            "iter 548, loss is 0.8677122151723696\n",
            "iter 549, loss is 0.864835869790435\n",
            "iter 550, loss is 0.8619883441276369\n",
            "iter 551, loss is 0.8591693428180717\n",
            "iter 552, loss is 0.8563785735241444\n",
            "iter 553, loss is 0.8536157469055189\n",
            "iter 554, loss is 0.8508805765883884\n",
            "iter 555, loss is 0.8481727791350601\n",
            "iter 556, loss is 0.8454920740138507\n",
            "iter 557, loss is 0.8428381835692937\n",
            "iter 558, loss is 0.8402108329926482\n",
            "iter 559, loss is 0.8376097502927135\n",
            "iter 560, loss is 0.8350346662669396\n",
            "iter 561, loss is 0.8324853144728378\n",
            "iter 562, loss is 0.8299614311996808\n",
            "iter 563, loss is 0.8274627554404952\n",
            "iter 564, loss is 0.8249890288643401\n",
            "iter 565, loss is 0.8225399957888708\n",
            "iter 566, loss is 0.8201154031531824\n",
            "iter 567, loss is 0.8177150004909335\n",
            "iter 568, loss is 0.815338539903744\n",
            "iter 569, loss is 0.8129857760348674\n",
            "iter 570, loss is 0.8106564660431301\n",
            "iter 571, loss is 0.8083503695771418\n",
            "iter 572, loss is 0.8060672487497663\n",
            "iter 573, loss is 0.803806868112858\n",
            "iter 574, loss is 0.801568994632253\n",
            "iter 575, loss is 0.7993533976630215\n",
            "iter 576, loss is 0.7971598489249699\n",
            "iter 577, loss is 0.794988122478396\n",
            "iter 578, loss is 0.7928379947000924\n",
            "iter 579, loss is 0.7907092442595962\n",
            "iter 580, loss is 0.7886016520956814\n",
            "iter 581, loss is 0.7865150013930933\n",
            "iter 582, loss is 0.784449077559521\n",
            "iter 583, loss is 0.7824036682028057\n",
            "iter 584, loss is 0.7803785631083833\n",
            "iter 585, loss is 0.7783735542169578\n",
            "iter 586, loss is 0.7763884356024036\n",
            "iter 587, loss is 0.7744230034498945\n",
            "iter 588, loss is 0.7724770560342578\n",
            "iter 589, loss is 0.7705503936985484\n",
            "iter 590, loss is 0.7686428188328442\n",
            "iter 591, loss is 0.766754135853258\n",
            "iter 592, loss is 0.7648841511811659\n",
            "iter 593, loss is 0.7630326732226465\n",
            "iter 594, loss is 0.7611995123481328\n",
            "iter 595, loss is 0.7593844808722724\n",
            "iter 596, loss is 0.757587393033993\n",
            "iter 597, loss is 0.7558080649767739\n",
            "iter 598, loss is 0.7540463147291182\n",
            "iter 599, loss is 0.752301962185227\n",
            "iter 600, loss is 0.7505748290858703\n",
            "iter 601, loss is 0.7488647389994539\n",
            "iter 602, loss is 0.7471715173032822\n",
            "iter 603, loss is 0.745494991165011\n",
            "iter 604, loss is 0.7438349895242916\n",
            "iter 605, loss is 0.7421913430746031\n",
            "iter 606, loss is 0.7405638842452706\n",
            "iter 607, loss is 0.7389524471836681\n",
            "iter 608, loss is 0.7373568677376031\n",
            "iter 609, loss is 0.7357769834378832\n",
            "iter 610, loss is 0.7342126334810598\n",
            "iter 611, loss is 0.7326636587123504\n",
            "iter 612, loss is 0.7311299016087346\n",
            "iter 613, loss is 0.7296112062622241\n",
            "iter 614, loss is 0.7281074183633041\n",
            "iter 615, loss is 0.7266183851845446\n",
            "iter 616, loss is 0.725143955564379\n",
            "iter 617, loss is 0.7236839798910508\n",
            "iter 618, loss is 0.7222383100867222\n",
            "iter 619, loss is 0.7208067995917489\n",
            "iter 620, loss is 0.7193893033491124\n",
            "iter 621, loss is 0.7179856777890156\n",
            "iter 622, loss is 0.7165957808136333\n",
            "iter 623, loss is 0.7152194717820212\n",
            "iter 624, loss is 0.7138566114951788\n",
            "iter 625, loss is 0.7125070621812645\n",
            "iter 626, loss is 0.7111706874809645\n",
            "iter 627, loss is 0.7098473524330097\n",
            "iter 628, loss is 0.7085369234598415\n",
            "iter 629, loss is 0.7072392683534258\n",
            "iter 630, loss is 0.7059542562612111\n",
            "iter 631, loss is 0.7046817576722307\n",
            "iter 632, loss is 0.7034216444033483\n",
            "iter 633, loss is 0.7021737895856429\n",
            "iter 634, loss is 0.7009380676509364\n",
            "iter 635, loss is 0.6997143543184551\n",
            "iter 636, loss is 0.6985025265816318\n",
            "iter 637, loss is 0.697302462695041\n",
            "iter 638, loss is 0.6961140421614684\n",
            "iter 639, loss is 0.6949371457191142\n",
            "iter 640, loss is 0.693771655328925\n",
            "iter 641, loss is 0.6926174541620594\n",
            "iter 642, loss is 0.6914744265874777\n",
            "iter 643, loss is 0.6903424581596629\n",
            "iter 644, loss is 0.6892214356064644\n",
            "iter 645, loss is 0.6881112468170691\n",
            "iter 646, loss is 0.6870117808300928\n",
            "iter 647, loss is 0.6859229278217969\n",
            "iter 648, loss is 0.6848445790944228\n",
            "iter 649, loss is 0.6837766270646491\n",
            "iter 650, loss is 0.6827189652521644\n",
            "iter 651, loss is 0.6816714882683584\n",
            "iter 652, loss is 0.6806340918051297\n",
            "iter 653, loss is 0.679606672623806\n",
            "iter 654, loss is 0.6785891285441817\n",
            "iter 655, loss is 0.6775813584336636\n",
            "iter 656, loss is 0.6765832621965312\n",
            "iter 657, loss is 0.6755947407633046\n",
            "iter 658, loss is 0.6746156960802245\n",
            "iter 659, loss is 0.6736460310988365\n",
            "iter 660, loss is 0.6726856497656847\n",
            "iter 661, loss is 0.6717344570121102\n",
            "iter 662, loss is 0.6707923587441542\n",
            "iter 663, loss is 0.6698592618325637\n",
            "iter 664, loss is 0.6689350741029024\n",
            "iter 665, loss is 0.668019704325759\n",
            "iter 666, loss is 0.6671130622070599\n",
            "iter 667, loss is 0.6662150583784778\n",
            "iter 668, loss is 0.6653256043879417\n",
            "iter 669, loss is 0.6644446126902417\n",
            "iter 670, loss is 0.6635719966377311\n",
            "iter 671, loss is 0.6627076704711249\n",
            "iter 672, loss is 0.6618515493103899\n",
            "iter 673, loss is 0.6610035491457318\n",
            "iter 674, loss is 0.6601635868286715\n",
            "iter 675, loss is 0.6593315800632147\n",
            "iter 676, loss is 0.6585074473971113\n",
            "iter 677, loss is 0.6576911082132049\n",
            "iter 678, loss is 0.6568824827208708\n",
            "iter 679, loss is 0.6560814919475415\n",
            "iter 680, loss is 0.6552880577303195\n",
            "iter 681, loss is 0.6545021027076756\n",
            "iter 682, loss is 0.6537235503112331\n",
            "iter 683, loss is 0.6529523247576354\n",
            "iter 684, loss is 0.6521883510404978\n",
            "iter 685, loss is 0.6514315549224405\n",
            "iter 686, loss is 0.6506818629272053\n",
            "iter 687, loss is 0.6499392023318511\n",
            "iter 688, loss is 0.6492035011590301\n",
            "iter 689, loss is 0.6484746881693446\n",
            "iter 690, loss is 0.6477526928537799\n",
            "iter 691, loss is 0.6470374454262159\n",
            "iter 692, loss is 0.646328876816016\n",
            "iter 693, loss is 0.6456269186606907\n",
            "iter 694, loss is 0.6449315032986374\n",
            "iter 695, loss is 0.6442425637619547\n",
            "iter 696, loss is 0.6435600337693292\n",
            "iter 697, loss is 0.6428838477189972\n",
            "iter 698, loss is 0.6422139406817764\n",
            "iter 699, loss is 0.6415502483941701\n",
            "iter 700, loss is 0.6408927072515429\n",
            "iter 701, loss is 0.6402412543013645\n",
            "iter 702, loss is 0.6395958272365249\n",
            "iter 703, loss is 0.6389563643887158\n",
            "iter 704, loss is 0.6383228047218825\n",
            "iter 705, loss is 0.6376950878257398\n",
            "iter 706, loss is 0.6370731539093579\n",
            "iter 707, loss is 0.636456943794811\n",
            "iter 708, loss is 0.6358463989108927\n",
            "iter 709, loss is 0.6352414612868958\n",
            "iter 710, loss is 0.6346420735464551\n",
            "iter 711, loss is 0.6340481789014535\n",
            "iter 712, loss is 0.6334597211459914\n",
            "iter 713, loss is 0.6328766446504167\n",
            "iter 714, loss is 0.6322988943554175\n",
            "iter 715, loss is 0.6317264157661738\n",
            "iter 716, loss is 0.63115915494657\n",
            "iter 717, loss is 0.6305970585134668\n",
            "iter 718, loss is 0.6300400736310318\n",
            "iter 719, loss is 0.6294881480051271\n",
            "iter 720, loss is 0.6289412298777572\n",
            "iter 721, loss is 0.6283992680215699\n",
            "iter 722, loss is 0.6278622117344177\n",
            "iter 723, loss is 0.6273300108339716\n",
            "iter 724, loss is 0.6268026156523924\n",
            "iter 725, loss is 0.6262799770310549\n",
            "iter 726, loss is 0.6257620463153275\n",
            "iter 727, loss is 0.6252487753494052\n",
            "iter 728, loss is 0.6247401164711944\n",
            "iter 729, loss is 0.6242360225072523\n",
            "iter 730, loss is 0.623736446767775\n",
            "iter 731, loss is 0.6232413430416401\n",
            "iter 732, loss is 0.622750665591499\n",
            "iter 733, loss is 0.6222643691489189\n",
            "iter 734, loss is 0.6217824089095749\n",
            "iter 735, loss is 0.6213047405284923\n",
            "iter 736, loss is 0.6208313201153363\n",
            "iter 737, loss is 0.6203621042297512\n",
            "iter 738, loss is 0.6198970498767463\n",
            "iter 739, loss is 0.6194361145021307\n",
            "iter 740, loss is 0.618979255987992\n",
            "iter 741, loss is 0.6185264326482255\n",
            "iter 742, loss is 0.6180776032241051\n",
            "iter 743, loss is 0.6176327268799028\n",
            "iter 744, loss is 0.6171917631985505\n",
            "iter 745, loss is 0.6167546721773489\n",
            "iter 746, loss is 0.6163214142237183\n",
            "iter 747, loss is 0.6158919501509935\n",
            "iter 748, loss is 0.6154662411742626\n",
            "iter 749, loss is 0.6150442489062481\n",
            "iter 750, loss is 0.6146259353532292\n",
            "iter 751, loss is 0.6142112629110068\n",
            "iter 752, loss is 0.6138001943609112\n",
            "iter 753, loss is 0.6133926928658475\n",
            "iter 754, loss is 0.6129887219663849\n",
            "iter 755, loss is 0.6125882455768831\n",
            "iter 756, loss is 0.6121912279816616\n",
            "iter 757, loss is 0.6117976338312052\n",
            "iter 758, loss is 0.6114074281384103\n",
            "iter 759, loss is 0.6110205762748693\n",
            "iter 760, loss is 0.610637043967193\n",
            "iter 761, loss is 0.6102567972933697\n",
            "iter 762, loss is 0.6098798026791643\n",
            "iter 763, loss is 0.6095060268945515\n",
            "iter 764, loss is 0.6091354370501865\n",
            "iter 765, loss is 0.6087680005939126\n",
            "iter 766, loss is 0.6084036853073043\n",
            "iter 767, loss is 0.6080424593022449\n",
            "iter 768, loss is 0.6076842910175403\n",
            "iter 769, loss is 0.6073291492155675\n",
            "iter 770, loss is 0.6069770029789562\n",
            "iter 771, loss is 0.606627821707306\n",
            "iter 772, loss is 0.6062815751139367\n",
            "iter 773, loss is 0.605938233222671\n",
            "iter 774, loss is 0.6055977663646513\n",
            "iter 775, loss is 0.6052601451751901\n",
            "iter 776, loss is 0.6049253405906492\n",
            "iter 777, loss is 0.6045933238453542\n",
            "iter 778, loss is 0.6042640664685403\n",
            "iter 779, loss is 0.6039375402813272\n",
            "iter 780, loss is 0.6036137173937275\n",
            "iter 781, loss is 0.6032925702016839\n",
            "iter 782, loss is 0.6029740713841389\n",
            "iter 783, loss is 0.6026581939001322\n",
            "iter 784, loss is 0.6023449109859285\n",
            "iter 785, loss is 0.6020341961521767\n",
            "iter 786, loss is 0.6017260231810954\n",
            "iter 787, loss is 0.6014203661236893\n",
            "iter 788, loss is 0.6011171992969929\n",
            "iter 789, loss is 0.6008164972813433\n",
            "iter 790, loss is 0.6005182349176803\n",
            "iter 791, loss is 0.6002223873048749\n",
            "iter 792, loss is 0.5999289297970841\n",
            "iter 793, loss is 0.5996378380011346\n",
            "iter 794, loss is 0.5993490877739313\n",
            "iter 795, loss is 0.599062655219893\n",
            "iter 796, loss is 0.5987785166884163\n",
            "iter 797, loss is 0.5984966487713623\n",
            "iter 798, loss is 0.5982170283005711\n",
            "iter 799, loss is 0.5979396323454019\n",
            "iter 800, loss is 0.5976644382102972\n",
            "iter 801, loss is 0.5973914234323724\n",
            "iter 802, loss is 0.5971205657790309\n",
            "iter 803, loss is 0.5968518432456027\n",
            "iter 804, loss is 0.5965852340530069\n",
            "iter 805, loss is 0.5963207166454401\n",
            "iter 806, loss is 0.596058269688086\n",
            "iter 807, loss is 0.5957978720648499\n",
            "iter 808, loss is 0.5955395028761169\n",
            "iter 809, loss is 0.5952831414365316\n",
            "iter 810, loss is 0.5950287672728014\n",
            "iter 811, loss is 0.5947763601215231\n",
            "iter 812, loss is 0.5945258999270298\n",
            "iter 813, loss is 0.5942773668392614\n",
            "iter 814, loss is 0.5940307412116574\n",
            "iter 815, loss is 0.5937860035990691\n",
            "iter 816, loss is 0.5935431347556954\n",
            "iter 817, loss is 0.5933021156330394\n",
            "iter 818, loss is 0.5930629273778837\n",
            "iter 819, loss is 0.5928255513302902\n",
            "iter 820, loss is 0.5925899690216176\n",
            "iter 821, loss is 0.5923561621725596\n",
            "iter 822, loss is 0.592124112691205\n",
            "iter 823, loss is 0.591893802671115\n",
            "iter 824, loss is 0.5916652143894222\n",
            "iter 825, loss is 0.5914383303049486\n",
            "iter 826, loss is 0.5912131330563427\n",
            "iter 827, loss is 0.5909896054602355\n",
            "iter 828, loss is 0.5907677305094161\n",
            "iter 829, loss is 0.5905474913710251\n",
            "iter 830, loss is 0.5903288713847679\n",
            "iter 831, loss is 0.590111854061144\n",
            "iter 832, loss is 0.5898964230796973\n",
            "iter 833, loss is 0.5896825622872821\n",
            "iter 834, loss is 0.5894702556963481\n",
            "iter 835, loss is 0.5892594874832422\n",
            "iter 836, loss is 0.5890502419865286\n",
            "iter 837, loss is 0.5888425037053248\n",
            "iter 838, loss is 0.5886362572976568\n",
            "iter 839, loss is 0.5884314875788286\n",
            "iter 840, loss is 0.5882281795198102\n",
            "iter 841, loss is 0.5880263182456416\n",
            "iter 842, loss is 0.5878258890338527\n",
            "iter 843, loss is 0.587626877312901\n",
            "iter 844, loss is 0.5874292686606224\n",
            "iter 845, loss is 0.5872330488027019\n",
            "iter 846, loss is 0.5870382036111552\n",
            "iter 847, loss is 0.5868447191028306\n",
            "iter 848, loss is 0.586652581437923\n",
            "iter 849, loss is 0.5864617769185038\n",
            "iter 850, loss is 0.5862722919870677\n",
            "iter 851, loss is 0.5860841132250915\n",
            "iter 852, loss is 0.5858972273516099\n",
            "iter 853, loss is 0.5857116212218049\n",
            "iter 854, loss is 0.5855272818256106\n",
            "iter 855, loss is 0.5853441962863302\n",
            "iter 856, loss is 0.5851623518592699\n",
            "iter 857, loss is 0.5849817359303847\n",
            "iter 858, loss is 0.5848023360149399\n",
            "iter 859, loss is 0.5846241397561838\n",
            "iter 860, loss is 0.5844471349240368\n",
            "iter 861, loss is 0.5842713094137922\n",
            "iter 862, loss is 0.5840966512448307\n",
            "iter 863, loss is 0.583923148559348\n",
            "iter 864, loss is 0.5837507896210965\n",
            "iter 865, loss is 0.5835795628141376\n",
            "iter 866, loss is 0.5834094566416095\n",
            "iter 867, loss is 0.5832404597245056\n",
            "iter 868, loss is 0.5830725608004661\n",
            "iter 869, loss is 0.5829057487225827\n",
            "iter 870, loss is 0.5827400124582146\n",
            "iter 871, loss is 0.5825753410878161\n",
            "iter 872, loss is 0.5824117238037783\n",
            "iter 873, loss is 0.582249149909281\n",
            "iter 874, loss is 0.5820876088171567\n",
            "iter 875, loss is 0.5819270900487663\n",
            "iter 876, loss is 0.5817675832328865\n",
            "iter 877, loss is 0.5816090781046082\n",
            "iter 878, loss is 0.5814515645042472\n",
            "iter 879, loss is 0.581295032376265\n",
            "iter 880, loss is 0.5811394717682004\n",
            "iter 881, loss is 0.5809848728296134\n",
            "iter 882, loss is 0.5808312258110391\n",
            "iter 883, loss is 0.5806785210629525\n",
            "iter 884, loss is 0.5805267490347432\n",
            "iter 885, loss is 0.5803759002737022\n",
            "iter 886, loss is 0.5802259654240172\n",
            "iter 887, loss is 0.5800769352257799\n",
            "iter 888, loss is 0.5799288005140023\n",
            "iter 889, loss is 0.5797815522176438\n",
            "iter 890, loss is 0.5796351813586474\n",
            "iter 891, loss is 0.5794896790509875\n",
            "iter 892, loss is 0.5793450364997244\n",
            "iter 893, loss is 0.5792012450000726\n",
            "iter 894, loss is 0.5790582959364754\n",
            "iter 895, loss is 0.5789161807816895\n",
            "iter 896, loss is 0.5787748910958809\n",
            "iter 897, loss is 0.5786344185257275\n",
            "iter 898, loss is 0.5784947548035326\n",
            "iter 899, loss is 0.5783558917463473\n",
            "iter 900, loss is 0.5782178212551012\n",
            "iter 901, loss is 0.5780805353137425\n",
            "iter 902, loss is 0.5779440259883865\n",
            "iter 903, loss is 0.577808285426474\n",
            "iter 904, loss is 0.577673305855937\n",
            "iter 905, loss is 0.577539079584373\n",
            "iter 906, loss is 0.5774055989982291\n",
            "iter 907, loss is 0.5772728565619927\n",
            "iter 908, loss is 0.5771408448173921\n",
            "iter 909, loss is 0.5770095563826044\n",
            "iter 910, loss is 0.5768789839514709\n",
            "iter 911, loss is 0.5767491202927227\n",
            "iter 912, loss is 0.5766199582492119\n",
            "iter 913, loss is 0.5764914907371518\n",
            "iter 914, loss is 0.5763637107453647\n",
            "iter 915, loss is 0.5762366113345384\n",
            "iter 916, loss is 0.5761101856364874\n",
            "iter 917, loss is 0.5759844268534261\n",
            "iter 918, loss is 0.5758593282572447\n",
            "iter 919, loss is 0.5757348831887965\n",
            "iter 920, loss is 0.5756110850571902\n",
            "iter 921, loss is 0.5754879273390897\n",
            "iter 922, loss is 0.5753654035780223\n",
            "iter 923, loss is 0.575243507383693\n",
            "iter 924, loss is 0.5751222324313047\n",
            "iter 925, loss is 0.575001572460889\n",
            "iter 926, loss is 0.5748815212766396\n",
            "iter 927, loss is 0.5747620727462549\n",
            "iter 928, loss is 0.5746432208002877\n",
            "iter 929, loss is 0.5745249594315\n",
            "iter 930, loss is 0.5744072826942248\n",
            "iter 931, loss is 0.5742901847037362\n",
            "iter 932, loss is 0.5741736596356233\n",
            "iter 933, loss is 0.5740577017251725\n",
            "iter 934, loss is 0.573942305266755\n",
            "iter 935, loss is 0.5738274646132218\n",
            "iter 936, loss is 0.5737131741753029\n",
            "iter 937, loss is 0.5735994284210149\n",
            "iter 938, loss is 0.573486221875073\n",
            "iter 939, loss is 0.5733735491183106\n",
            "iter 940, loss is 0.5732614047871027\n",
            "iter 941, loss is 0.5731497835727974\n",
            "iter 942, loss is 0.573038680221152\n",
            "iter 943, loss is 0.5729280895317747\n",
            "iter 944, loss is 0.5728180063575734\n",
            "iter 945, loss is 0.5727084256042082\n",
            "iter 946, loss is 0.5725993422295516\n",
            "iter 947, loss is 0.572490751243152\n",
            "iter 948, loss is 0.5723826477057053\n",
            "iter 949, loss is 0.572275026728529\n",
            "iter 950, loss is 0.5721678834730445\n",
            "iter 951, loss is 0.5720612131502621\n",
            "iter 952, loss is 0.5719550110202741\n",
            "iter 953, loss is 0.5718492723917503\n",
            "iter 954, loss is 0.5717439926214403\n",
            "iter 955, loss is 0.5716391671136809\n",
            "iter 956, loss is 0.5715347913199079\n",
            "iter 957, loss is 0.5714308607381733\n",
            "iter 958, loss is 0.5713273709126674\n",
            "iter 959, loss is 0.5712243174332453\n",
            "iter 960, loss is 0.5711216959349597\n",
            "iter 961, loss is 0.5710195020975961\n",
            "iter 962, loss is 0.5709177316452155\n",
            "iter 963, loss is 0.5708163803457003\n",
            "iter 964, loss is 0.5707154440103038\n",
            "iter 965, loss is 0.5706149184932067\n",
            "iter 966, loss is 0.5705147996910768\n",
            "iter 967, loss is 0.5704150835426328\n",
            "iter 968, loss is 0.570315766028214\n",
            "iter 969, loss is 0.5702168431693531\n",
            "iter 970, loss is 0.5701183110283539\n",
            "iter 971, loss is 0.570020165707873\n",
            "iter 972, loss is 0.5699224033505067\n",
            "iter 973, loss is 0.5698250201383812\n",
            "iter 974, loss is 0.5697280122927472\n",
            "iter 975, loss is 0.5696313760735789\n",
            "iter 976, loss is 0.569535107779177\n",
            "iter 977, loss is 0.5694392037457754\n",
            "iter 978, loss is 0.569343660347153\n",
            "iter 979, loss is 0.5692484739942483\n",
            "iter 980, loss is 0.5691536411347778\n",
            "iter 981, loss is 0.5690591582528606\n",
            "iter 982, loss is 0.5689650218686434\n",
            "iter 983, loss is 0.5688712285379321\n",
            "iter 984, loss is 0.5687777748518261\n",
            "iter 985, loss is 0.5686846574363561\n",
            "iter 986, loss is 0.5685918729521264\n",
            "iter 987, loss is 0.56849941809396\n",
            "iter 988, loss is 0.5684072895905478\n",
            "iter 989, loss is 0.5683154842041023\n",
            "iter 990, loss is 0.5682239987300122\n",
            "iter 991, loss is 0.5681328299965038\n",
            "iter 992, loss is 0.5680419748643034\n",
            "iter 993, loss is 0.5679514302263049\n",
            "iter 994, loss is 0.5678611930072388\n",
            "iter 995, loss is 0.567771260163347\n",
            "iter 996, loss is 0.5676816286820592\n",
            "iter 997, loss is 0.5675922955816729\n",
            "iter 998, loss is 0.5675032579110375\n",
            "iter 999, loss is 0.5674145127492403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "v7ZDKh6xtuk7",
        "outputId": "70dfe274-1b01-41f0-a993-86a125d08380"
      },
      "source": [
        "plot_curve(range(num_iterations), losses)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85a4LKMup4Fi"
      },
      "source": [
        "### L2 Regularization\n",
        "\n",
        "Adding the L2 penalty to the loss function. lambda is a hyperparameter.  \n",
        "\n",
        "loss = (X.w + b) + lambda * sum(w^2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxVVnBYYT1bF",
        "outputId": "ed2b07fe-e40b-4d21-e697-7a478543a229"
      },
      "source": [
        "lr = 0.001\n",
        "lmda = 0.001\n",
        "N = num_train_examples\n",
        "Wj = W.copy()\n",
        "bj = b.copy()\n",
        "best_loss = float(\"inf\")\n",
        "losses = []\n",
        "\n",
        "for i in range(num_iterations): \n",
        "  y_hat = X_train.dot(Wj) + bj + lmda * np.sum(Wj**2)\n",
        "  error = y_train - y_hat\n",
        "  loss = 1/2 * (y_train - y_hat)**2\n",
        "  \n",
        "  # Derivative of the L2 penalty is (2 * lambda * W)\n",
        "  dW = (np.dot(-X_train.T, error) - 2 * lmda * Wj) / N \n",
        "  db = -error / N\n",
        "\n",
        "  Wj = Wj - lr * dW \n",
        "  bj = bj - lr * db\n",
        "\n",
        "  mean_loss = np.sum(loss) / N\n",
        "  losses.append(mean_loss)\n",
        "\n",
        "  if mean_loss < best_loss:\n",
        "    best_loss = mean_loss\n",
        "\n",
        "  print(\"iter %s, loss is %s\" % (i, best_loss)) \n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss is 79.31977831420559\n",
            "iter 1, loss is 78.50506055971292\n",
            "iter 2, loss is 77.6988206374902\n",
            "iter 3, loss is 76.90096964135016\n",
            "iter 4, loss is 76.11141960796311\n",
            "iter 5, loss is 75.33008350669562\n",
            "iter 6, loss is 74.55687522956084\n",
            "iter 7, loss is 73.79170958128017\n",
            "iter 8, loss is 73.03450226945397\n",
            "iter 9, loss is 72.28516989484054\n",
            "iter 10, loss is 71.54362994174247\n",
            "iter 11, loss is 70.80980076849823\n",
            "iter 12, loss is 70.08360159807894\n",
            "iter 13, loss is 69.36495250878805\n",
            "iter 14, loss is 68.65377442506365\n",
            "iter 15, loss is 67.94998910838157\n",
            "iter 16, loss is 67.25351914825869\n",
            "iter 17, loss is 66.56428795335474\n",
            "iter 18, loss is 65.88221974267209\n",
            "iter 19, loss is 65.20723953685209\n",
            "iter 20, loss is 64.53927314956653\n",
            "iter 21, loss is 63.878247179004084\n",
            "iter 22, loss is 63.224088999449684\n",
            "iter 23, loss is 62.576726752956354\n",
            "iter 24, loss is 61.93608934110831\n",
            "iter 25, loss is 61.302106416874174\n",
            "iter 26, loss is 60.674708376549596\n",
            "iter 27, loss is 60.05382635178779\n",
            "iter 28, loss is 59.439392201717624\n",
            "iter 29, loss is 58.83133850514767\n",
            "iter 30, loss is 58.229598552855606\n",
            "iter 31, loss is 57.63410633996197\n",
            "iter 32, loss is 57.04479655838717\n",
            "iter 33, loss is 56.46160458939098\n",
            "iter 34, loss is 55.88446649619353\n",
            "iter 35, loss is 55.3133190166768\n",
            "iter 36, loss is 54.74809955616588\n",
            "iter 37, loss is 54.18874618028892\n",
            "iter 38, loss is 53.635197607915025\n",
            "iter 39, loss is 53.08739320416915\n",
            "iter 40, loss is 52.54527297352323\n",
            "iter 41, loss is 52.00877755296246\n",
            "iter 42, loss is 51.47784820522619\n",
            "iter 43, loss is 50.95242681212234\n",
            "iter 44, loss is 50.43245586791473\n",
            "iter 45, loss is 49.91787847278223\n",
            "iter 46, loss is 49.408638326349354\n",
            "iter 47, loss is 48.904679721286946\n",
            "iter 48, loss is 48.405947536982744\n",
            "iter 49, loss is 47.91238723328059\n",
            "iter 50, loss is 47.42394484428773\n",
            "iter 51, loss is 46.94056697224955\n",
            "iter 52, loss is 46.462200781490665\n",
            "iter 53, loss is 45.98879399242195\n",
            "iter 54, loss is 45.52029487561261\n",
            "iter 55, loss is 45.05665224592659\n",
            "iter 56, loss is 44.597815456722614\n",
            "iter 57, loss is 44.14373439411727\n",
            "iter 58, loss is 43.694359471310015\n",
            "iter 59, loss is 43.24964162297015\n",
            "iter 60, loss is 42.80953229968422\n",
            "iter 61, loss is 42.37398346246395\n",
            "iter 62, loss is 41.94294757731338\n",
            "iter 63, loss is 41.51637760985507\n",
            "iter 64, loss is 41.094227020014365\n",
            "iter 65, loss is 40.67644975676135\n",
            "iter 66, loss is 40.263000252909436\n",
            "iter 67, loss is 39.85383341997061\n",
            "iter 68, loss is 39.448904643066015\n",
            "iter 69, loss is 39.048169775891715\n",
            "iter 70, loss is 38.65158513573889\n",
            "iter 71, loss is 38.25910749856777\n",
            "iter 72, loss is 37.870694094134926\n",
            "iter 73, loss is 37.48630260117307\n",
            "iter 74, loss is 37.10589114262304\n",
            "iter 75, loss is 36.729418280917145\n",
            "iter 76, loss is 36.3568430133135\n",
            "iter 77, loss is 35.988124767280745\n",
            "iter 78, loss is 35.62322339593246\n",
            "iter 79, loss is 35.26209917351091\n",
            "iter 80, loss is 34.90471279091943\n",
            "iter 81, loss is 34.55102535130304\n",
            "iter 82, loss is 34.200998365676654\n",
            "iter 83, loss is 33.85459374860042\n",
            "iter 84, loss is 33.51177381390158\n",
            "iter 85, loss is 33.17250127044256\n",
            "iter 86, loss is 32.836739217934465\n",
            "iter 87, loss is 32.5044511427957\n",
            "iter 88, loss is 32.175600914055266\n",
            "iter 89, loss is 31.850152779299947\n",
            "iter 90, loss is 31.528071360665223\n",
            "iter 91, loss is 31.20932165086933\n",
            "iter 92, loss is 30.893869009289823\n",
            "iter 93, loss is 30.581679158082444\n",
            "iter 94, loss is 30.272718178341652\n",
            "iter 95, loss is 29.966952506302388\n",
            "iter 96, loss is 29.66434892958271\n",
            "iter 97, loss is 29.364874583466676\n",
            "iter 98, loss is 29.068496947227235\n",
            "iter 99, loss is 28.775183840488566\n",
            "iter 100, loss is 28.48490341962743\n",
            "iter 101, loss is 28.19762417421312\n",
            "iter 102, loss is 27.913314923485704\n",
            "iter 103, loss is 27.631944812871915\n",
            "iter 104, loss is 27.353483310538472\n",
            "iter 105, loss is 27.07790020398232\n",
            "iter 106, loss is 26.805165596657453\n",
            "iter 107, loss is 26.53524990463784\n",
            "iter 108, loss is 26.2681238533161\n",
            "iter 109, loss is 26.00375847413761\n",
            "iter 110, loss is 25.742125101369457\n",
            "iter 111, loss is 25.483195368904134\n",
            "iter 112, loss is 25.226941207097315\n",
            "iter 113, loss is 24.973334839639534\n",
            "iter 114, loss is 24.722348780461374\n",
            "iter 115, loss is 24.473955830671645\n",
            "iter 116, loss is 24.22812907552843\n",
            "iter 117, loss is 23.98484188144244\n",
            "iter 118, loss is 23.744067893012435\n",
            "iter 119, loss is 23.50578103009231\n",
            "iter 120, loss is 23.26995548488957\n",
            "iter 121, loss is 23.036565719094725\n",
            "iter 122, loss is 22.80558646104142\n",
            "iter 123, loss is 22.576992702896842\n",
            "iter 124, loss is 22.350759697882165\n",
            "iter 125, loss is 22.126862957522626\n",
            "iter 126, loss is 21.90527824892697\n",
            "iter 127, loss is 21.685981592095956\n",
            "iter 128, loss is 21.468949257259496\n",
            "iter 129, loss is 21.25415776224228\n",
            "iter 130, loss is 21.04158386985744\n",
            "iter 131, loss is 20.831204585328045\n",
            "iter 132, loss is 20.62299715373602\n",
            "iter 133, loss is 20.416939057498325\n",
            "iter 134, loss is 20.213008013869988\n",
            "iter 135, loss is 20.01118197247374\n",
            "iter 136, loss is 19.81143911285595\n",
            "iter 137, loss is 19.61375784206867\n",
            "iter 138, loss is 19.418116792277278\n",
            "iter 139, loss is 19.22449481839373\n",
            "iter 140, loss is 19.032870995734932\n",
            "iter 141, loss is 18.84322461770602\n",
            "iter 142, loss is 18.655535193508356\n",
            "iter 143, loss is 18.469782445871804\n",
            "iter 144, loss is 18.28594630881123\n",
            "iter 145, loss is 18.104006925406786\n",
            "iter 146, loss is 17.923944645607804\n",
            "iter 147, loss is 17.74574002406006\n",
            "iter 148, loss is 17.56937381795611\n",
            "iter 149, loss is 17.394826984908462\n",
            "iter 150, loss is 17.22208068084539\n",
            "iter 151, loss is 17.051116257928985\n",
            "iter 152, loss is 16.88191526249546\n",
            "iter 153, loss is 16.714459433017225\n",
            "iter 154, loss is 16.548730698086633\n",
            "iter 155, loss is 16.384711174421113\n",
            "iter 156, loss is 16.222383164889504\n",
            "iter 157, loss is 16.06172915655927\n",
            "iter 158, loss is 15.902731818764515\n",
            "iter 159, loss is 15.745374001194415\n",
            "iter 160, loss is 15.58963873200195\n",
            "iter 161, loss is 15.435509215932713\n",
            "iter 162, loss is 15.28296883247351\n",
            "iter 163, loss is 15.132001134020634\n",
            "iter 164, loss is 14.982589844067496\n",
            "iter 165, loss is 14.83471885541152\n",
            "iter 166, loss is 14.688372228379977\n",
            "iter 167, loss is 14.543534189074663\n",
            "iter 168, loss is 14.400189127635121\n",
            "iter 169, loss is 14.258321596520283\n",
            "iter 170, loss is 14.117916308808306\n",
            "iter 171, loss is 13.978958136514388\n",
            "iter 172, loss is 13.841432108926384\n",
            "iter 173, loss is 13.705323410958064\n",
            "iter 174, loss is 13.570617381519785\n",
            "iter 175, loss is 13.437299511906346\n",
            "iter 176, loss is 13.305355444201993\n",
            "iter 177, loss is 13.174770969702191\n",
            "iter 178, loss is 13.045532027352165\n",
            "iter 179, loss is 12.917624702201874\n",
            "iter 180, loss is 12.791035223877406\n",
            "iter 181, loss is 12.665749965068436\n",
            "iter 182, loss is 12.541755440031714\n",
            "iter 183, loss is 12.419038303110383\n",
            "iter 184, loss is 12.297585347268878\n",
            "iter 185, loss is 12.177383502643371\n",
            "iter 186, loss is 12.058419835107456\n",
            "iter 187, loss is 11.940681544853037\n",
            "iter 188, loss is 11.824155964986147\n",
            "iter 189, loss is 11.708830560137638\n",
            "iter 190, loss is 11.594692925088502\n",
            "iter 191, loss is 11.481730783409681\n",
            "iter 192, loss is 11.369931986116303\n",
            "iter 193, loss is 11.259284510336016\n",
            "iter 194, loss is 11.149776457991445\n",
            "iter 195, loss is 11.041396054496513\n",
            "iter 196, loss is 10.934131647466502\n",
            "iter 197, loss is 10.827971705441737\n",
            "iter 198, loss is 10.72290481662471\n",
            "iter 199, loss is 10.618919687630516\n",
            "iter 200, loss is 10.51600514225045\n",
            "iter 201, loss is 10.414150120228657\n",
            "iter 202, loss is 10.313343676051636\n",
            "iter 203, loss is 10.213574977750548\n",
            "iter 204, loss is 10.114833305716072\n",
            "iter 205, loss is 10.017108051525785\n",
            "iter 206, loss is 9.920388716783895\n",
            "iter 207, loss is 9.824664911973164\n",
            "iter 208, loss is 9.729926355318923\n",
            "iter 209, loss is 9.636162871665064\n",
            "iter 210, loss is 9.543364391361832\n",
            "iter 211, loss is 9.451520949165351\n",
            "iter 212, loss is 9.360622683148689\n",
            "iter 213, loss is 9.27065983362443\n",
            "iter 214, loss is 9.181622742078519\n",
            "iter 215, loss is 9.093501850115388\n",
            "iter 216, loss is 9.006287698414102\n",
            "iter 217, loss is 8.919970925695539\n",
            "iter 218, loss is 8.8345422677004\n",
            "iter 219, loss is 8.749992556177963\n",
            "iter 220, loss is 8.66631271788548\n",
            "iter 221, loss is 8.583493773598065\n",
            "iter 222, loss is 8.501526837129012\n",
            "iter 223, loss is 8.420403114360386\n",
            "iter 224, loss is 8.340113902283802\n",
            "iter 225, loss is 8.260650588051297\n",
            "iter 226, loss is 8.182004648036132\n",
            "iter 227, loss is 8.1041676469035\n",
            "iter 228, loss is 8.027131236690957\n",
            "iter 229, loss is 7.950887155898535\n",
            "iter 230, loss is 7.875427228588368\n",
            "iter 231, loss is 7.800743363493812\n",
            "iter 232, loss is 7.7268275531378885\n",
            "iter 233, loss is 7.653671872960982\n",
            "iter 234, loss is 7.581268480457689\n",
            "iter 235, loss is 7.509609614322727\n",
            "iter 236, loss is 7.438687593605799\n",
            "iter 237, loss is 7.368494816875327\n",
            "iter 238, loss is 7.299023761390937\n",
            "iter 239, loss is 7.230266982284651\n",
            "iter 240, loss is 7.162217111750644\n",
            "iter 241, loss is 7.0948668582434875\n",
            "iter 242, loss is 7.028209005684819\n",
            "iter 243, loss is 6.962236412678312\n",
            "iter 244, loss is 6.896942011732877\n",
            "iter 245, loss is 6.832318808493995\n",
            "iter 246, loss is 6.768359880983123\n",
            "iter 247, loss is 6.70505837884504\n",
            "iter 248, loss is 6.642407522603108\n",
            "iter 249, loss is 6.580400602922297\n",
            "iter 250, loss is 6.519030979879967\n",
            "iter 251, loss is 6.458292082244259\n",
            "iter 252, loss is 6.398177406760049\n",
            "iter 253, loss is 6.338680517442369\n",
            "iter 254, loss is 6.279795044877257\n",
            "iter 255, loss is 6.221514685529869\n",
            "iter 256, loss is 6.163833201059893\n",
            "iter 257, loss is 6.106744417644077\n",
            "iter 258, loss is 6.050242225305874\n",
            "iter 259, loss is 5.994320577252092\n",
            "iter 260, loss is 5.938973489216482\n",
            "iter 261, loss is 5.884195038810202\n",
            "iter 262, loss is 5.829979364879048\n",
            "iter 263, loss is 5.776320666867446\n",
            "iter 264, loss is 5.723213204189059\n",
            "iter 265, loss is 5.6706512956039985\n",
            "iter 266, loss is 5.618629318602533\n",
            "iter 267, loss is 5.56714170879524\n",
            "iter 268, loss is 5.516182959309546\n",
            "iter 269, loss is 5.46574762019256\n",
            "iter 270, loss is 5.415830297820143\n",
            "iter 271, loss is 5.366425654312172\n",
            "iter 272, loss is 5.317528406953892\n",
            "iter 273, loss is 5.269133327623335\n",
            "iter 274, loss is 5.221235242224689\n",
            "iter 275, loss is 5.173829030127628\n",
            "iter 276, loss is 5.126909623612456\n",
            "iter 277, loss is 5.080472007321071\n",
            "iter 278, loss is 5.034511217713659\n",
            "iter 279, loss is 4.989022342531039\n",
            "iter 280, loss is 4.944000520262668\n",
            "iter 281, loss is 4.89944093962014\n",
            "iter 282, loss is 4.855338839016228\n",
            "iter 283, loss is 4.811689506049338\n",
            "iter 284, loss is 4.768488276993358\n",
            "iter 285, loss is 4.725730536292819\n",
            "iter 286, loss is 4.683411716063326\n",
            "iter 287, loss is 4.641527295597209\n",
            "iter 288, loss is 4.600072800874315\n",
            "iter 289, loss is 4.5590438040779295\n",
            "iter 290, loss is 4.518435923115711\n",
            "iter 291, loss is 4.478244821145658\n",
            "iter 292, loss is 4.438466206106994\n",
            "iter 293, loss is 4.399095830255952\n",
            "iter 294, loss is 4.360129489706407\n",
            "iter 295, loss is 4.3215630239752825\n",
            "iter 296, loss is 4.283392315532713\n",
            "iter 297, loss is 4.245613289356876\n",
            "iter 298, loss is 4.208221912493482\n",
            "iter 299, loss is 4.171214193619847\n",
            "iter 300, loss is 4.134586182613505\n",
            "iter 301, loss is 4.098333970125321\n",
            "iter 302, loss is 4.062453687157049\n",
            "iter 303, loss is 4.026941504643295\n",
            "iter 304, loss is 3.9917936330378296\n",
            "iter 305, loss is 3.9570063219042066\n",
            "iter 306, loss is 3.9225758595106623\n",
            "iter 307, loss is 3.8884985724292105\n",
            "iter 308, loss is 3.854770825138931\n",
            "iter 309, loss is 3.8213890196333824\n",
            "iter 310, loss is 3.788349595032096\n",
            "iter 311, loss is 3.7556490271961294\n",
            "iter 312, loss is 3.7232838283476077\n",
            "iter 313, loss is 3.69125054669323\n",
            "iter 314, loss is 3.659545766051705\n",
            "iter 315, loss is 3.6281661054850414\n",
            "iter 316, loss is 3.597108218933701\n",
            "iter 317, loss is 3.566368794855529\n",
            "iter 318, loss is 3.5359445558684546\n",
            "iter 319, loss is 3.5058322583968953\n",
            "iter 320, loss is 3.476028692321855\n",
            "iter 321, loss is 3.446530680634652\n",
            "iter 322, loss is 3.4173350790942503\n",
            "iter 323, loss is 3.388438775888162\n",
            "iter 324, loss is 3.3598386912968645\n",
            "iter 325, loss is 3.3315317773617172\n",
            "iter 326, loss is 3.3035150175563275\n",
            "iter 327, loss is 3.275785426461338\n",
            "iter 328, loss is 3.248340049442588\n",
            "iter 329, loss is 3.2211759623326244\n",
            "iter 330, loss is 3.194290271115527\n",
            "iter 331, loss is 3.167680111615006\n",
            "iter 332, loss is 3.1413426491857397\n",
            "iter 333, loss is 3.115275078407927\n",
            "iter 334, loss is 3.089474622785005\n",
            "iter 335, loss is 3.0639385344445205\n",
            "iter 336, loss is 3.038664093842093\n",
            "iter 337, loss is 3.0136486094684694\n",
            "iter 338, loss is 2.988889417559614\n",
            "iter 339, loss is 2.9643838818098103\n",
            "iter 340, loss is 2.9401293930877483\n",
            "iter 341, loss is 2.916123369155553\n",
            "iter 342, loss is 2.8923632543907396\n",
            "iter 343, loss is 2.868846519511049\n",
            "iter 344, loss is 2.845570661302147\n",
            "iter 345, loss is 2.8225332023481484\n",
            "iter 346, loss is 2.799731690764936\n",
            "iter 347, loss is 2.7771636999362563\n",
            "iter 348, loss is 2.754826828252549\n",
            "iter 349, loss is 2.7327186988524876\n",
            "iter 350, loss is 2.710836959367208\n",
            "iter 351, loss is 2.689179281667187\n",
            "iter 352, loss is 2.667743361611751\n",
            "iter 353, loss is 2.6465269188011824\n",
            "iter 354, loss is 2.625527696331401\n",
            "iter 355, loss is 2.604743460551191\n",
            "iter 356, loss is 2.584172000821942\n",
            "iter 357, loss is 2.563811129279891\n",
            "iter 358, loss is 2.543658680600825\n",
            "iter 359, loss is 2.5237125117672288\n",
            "iter 360, loss is 2.503970501837844\n",
            "iter 361, loss is 2.4844305517196226\n",
            "iter 362, loss is 2.4650905839420436\n",
            "iter 363, loss is 2.4459485424337744\n",
            "iter 364, loss is 2.42700239230164\n",
            "iter 365, loss is 2.4082501196118904\n",
            "iter 366, loss is 2.389689731173742\n",
            "iter 367, loss is 2.3713192543251496\n",
            "iter 368, loss is 2.3531367367208107\n",
            "iter 369, loss is 2.33514024612236\n",
            "iter 370, loss is 2.3173278701907534\n",
            "iter 371, loss is 2.299697716280783\n",
            "iter 372, loss is 2.2822479112377367\n",
            "iter 373, loss is 2.264976601196167\n",
            "iter 374, loss is 2.2478819513807347\n",
            "iter 375, loss is 2.2309621459091256\n",
            "iter 376, loss is 2.2142153875970068\n",
            "iter 377, loss is 2.197639897765002\n",
            "iter 378, loss is 2.181233916047673\n",
            "iter 379, loss is 2.164995700204475\n",
            "iter 380, loss is 2.14892352593267\n",
            "iter 381, loss is 2.1330156866821848\n",
            "iter 382, loss is 2.117270493472381\n",
            "iter 383, loss is 2.101686274710727\n",
            "iter 384, loss is 2.086261376013344\n",
            "iter 385, loss is 2.0709941600274155\n",
            "iter 386, loss is 2.0558830062554354\n",
            "iter 387, loss is 2.0409263108812707\n",
            "iter 388, loss is 2.0261224865980374\n",
            "iter 389, loss is 2.0114699624377432\n",
            "iter 390, loss is 1.9969671836027088\n",
            "iter 391, loss is 1.9826126112987263\n",
            "iter 392, loss is 1.9684047225699481\n",
            "iter 393, loss is 1.954342010135484\n",
            "iter 394, loss is 1.9404229822276904\n",
            "iter 395, loss is 1.9266461624321367\n",
            "iter 396, loss is 1.9130100895292212\n",
            "iter 397, loss is 1.8995133173374348\n",
            "iter 398, loss is 1.8861544145582398\n",
            "iter 399, loss is 1.8729319646225608\n",
            "iter 400, loss is 1.8598445655388574\n",
            "iter 401, loss is 1.8468908297427784\n",
            "iter 402, loss is 1.8340693839483577\n",
            "iter 403, loss is 1.8213788690007697\n",
            "iter 404, loss is 1.808817939730587\n",
            "iter 405, loss is 1.7963852648095635\n",
            "iter 406, loss is 1.7840795266079004\n",
            "iter 407, loss is 1.771899421052989\n",
            "iter 408, loss is 1.7598436574896203\n",
            "iter 409, loss is 1.7479109585416361\n",
            "iter 410, loss is 1.736100059975012\n",
            "iter 411, loss is 1.7244097105623564\n",
            "iter 412, loss is 1.7128386719488147\n",
            "iter 413, loss is 1.7013857185193544\n",
            "iter 414, loss is 1.6900496372674285\n",
            "iter 415, loss is 1.6788292276649928\n",
            "iter 416, loss is 1.6677233015338702\n",
            "iter 417, loss is 1.6567306829184423\n",
            "iter 418, loss is 1.645850207959662\n",
            "iter 419, loss is 1.6350807247703647\n",
            "iter 420, loss is 1.6244210933118695\n",
            "iter 421, loss is 1.6138701852718578\n",
            "iter 422, loss is 1.6034268839435133\n",
            "iter 423, loss is 1.5930900841059101\n",
            "iter 424, loss is 1.5828586919056402\n",
            "iter 425, loss is 1.5727316247396645\n",
            "iter 426, loss is 1.5627078111393682\n",
            "iter 427, loss is 1.5527861906558247\n",
            "iter 428, loss is 1.5429657137462396\n",
            "iter 429, loss is 1.5332453416615661\n",
            "iter 430, loss is 1.5236240463352881\n",
            "iter 431, loss is 1.5141008102733475\n",
            "iter 432, loss is 1.5046746264452107\n",
            "iter 433, loss is 1.495344498176062\n",
            "iter 434, loss is 1.4861094390401088\n",
            "iter 435, loss is 1.4769684727549894\n",
            "iter 436, loss is 1.467920633077275\n",
            "iter 437, loss is 1.4589649636990458\n",
            "iter 438, loss is 1.4501005181455386\n",
            "iter 439, loss is 1.4413263596738555\n",
            "iter 440, loss is 1.4326415611727101\n",
            "iter 441, loss is 1.4240452050632175\n",
            "iter 442, loss is 1.4155363832007046\n",
            "iter 443, loss is 1.4071141967775354\n",
            "iter 444, loss is 1.3987777562269406\n",
            "iter 445, loss is 1.390526181127842\n",
            "iter 446, loss is 1.382358600110655\n",
            "iter 447, loss is 1.374274150764067\n",
            "iter 448, loss is 1.3662719795427818\n",
            "iter 449, loss is 1.3583512416762071\n",
            "iter 450, loss is 1.3505111010780977\n",
            "iter 451, loss is 1.3427507302571187\n",
            "iter 452, loss is 1.3350693102283417\n",
            "iter 453, loss is 1.3274660304256516\n",
            "iter 454, loss is 1.3199400886150539\n",
            "iter 455, loss is 1.3124906908088838\n",
            "iter 456, loss is 1.305117051180896\n",
            "iter 457, loss is 1.2978183919822328\n",
            "iter 458, loss is 1.29059394345826\n",
            "iter 459, loss is 1.2834429437662582\n",
            "iter 460, loss is 1.2763646388939702\n",
            "iter 461, loss is 1.269358282578981\n",
            "iter 462, loss is 1.262423136228935\n",
            "iter 463, loss is 1.255558468842576\n",
            "iter 464, loss is 1.2487635569316002\n",
            "iter 465, loss is 1.2420376844433183\n",
            "iter 466, loss is 1.2353801426841131\n",
            "iter 467, loss is 1.2287902302436875\n",
            "iter 468, loss is 1.2222672529200964\n",
            "iter 469, loss is 1.2158105236455483\n",
            "iter 470, loss is 1.2094193624129754\n",
            "iter 471, loss is 1.203093096203362\n",
            "iter 472, loss is 1.1968310589138162\n",
            "iter 473, loss is 1.1906325912863955\n",
            "iter 474, loss is 1.1844970408376536\n",
            "iter 475, loss is 1.1784237617889215\n",
            "iter 476, loss is 1.1724121149973088\n",
            "iter 477, loss is 1.166461467887411\n",
            "iter 478, loss is 1.1605711943837258\n",
            "iter 479, loss is 1.1547406748437647\n",
            "iter 480, loss is 1.148969295991855\n",
            "iter 481, loss is 1.143256450853624\n",
            "iter 482, loss is 1.1376015386911607\n",
            "iter 483, loss is 1.1320039649388418\n",
            "iter 484, loss is 1.1264631411398245\n",
            "iter 485, loss is 1.1209784848831925\n",
            "iter 486, loss is 1.1155494197417475\n",
            "iter 487, loss is 1.1101753752104464\n",
            "iter 488, loss is 1.1048557866454705\n",
            "iter 489, loss is 1.0995900952039221\n",
            "iter 490, loss is 1.0943777477841483\n",
            "iter 491, loss is 1.0892181969666712\n",
            "iter 492, loss is 1.0841109009557364\n",
            "iter 493, loss is 1.0790553235214568\n",
            "iter 494, loss is 1.0740509339425572\n",
            "iter 495, loss is 1.0690972069497076\n",
            "iter 496, loss is 1.0641936226694395\n",
            "iter 497, loss is 1.0593396665686445\n",
            "iter 498, loss is 1.0545348293996375\n",
            "iter 499, loss is 1.0497786071457942\n",
            "iter 500, loss is 1.0450705009677457\n",
            "iter 501, loss is 1.040410017150124\n",
            "iter 502, loss is 1.035796667048863\n",
            "iter 503, loss is 1.031229967039037\n",
            "iter 504, loss is 1.026709438463241\n",
            "iter 505, loss is 1.0222346075804996\n",
            "iter 506, loss is 1.0178050055157042\n",
            "iter 507, loss is 1.0134201682095711\n",
            "iter 508, loss is 1.0090796363691152\n",
            "iter 509, loss is 1.0047829554186327\n",
            "iter 510, loss is 1.0005296754511916\n",
            "iter 511, loss is 0.9963193511806184\n",
            "iter 512, loss is 0.992151541893984\n",
            "iter 513, loss is 0.9880258114045741\n",
            "iter 514, loss is 0.9839417280053486\n",
            "iter 515, loss is 0.9798988644228772\n",
            "iter 516, loss is 0.9758967977717514\n",
            "iter 517, loss is 0.9719351095094657\n",
            "iter 518, loss is 0.9680133853917635\n",
            "iter 519, loss is 0.964131215428442\n",
            "iter 520, loss is 0.9602881938396155\n",
            "iter 521, loss is 0.9564839190124244\n",
            "iter 522, loss is 0.9527179934581943\n",
            "iter 523, loss is 0.9489900237700344\n",
            "iter 524, loss is 0.9452996205808726\n",
            "iter 525, loss is 0.9416463985219246\n",
            "iter 526, loss is 0.9380299761815869\n",
            "iter 527, loss is 0.9344499760647578\n",
            "iter 528, loss is 0.9309060245525742\n",
            "iter 529, loss is 0.9273977518625635\n",
            "iter 530, loss is 0.9239247920092074\n",
            "iter 531, loss is 0.9204867827649094\n",
            "iter 532, loss is 0.9170833656213662\n",
            "iter 533, loss is 0.9137141857513366\n",
            "iter 534, loss is 0.9103788919708028\n",
            "iter 535, loss is 0.9070771367015222\n",
            "iter 536, loss is 0.9038085759339655\n",
            "iter 537, loss is 0.9005728691906341\n",
            "iter 538, loss is 0.8973696794897589\n",
            "iter 539, loss is 0.894198673309368\n",
            "iter 540, loss is 0.8910595205517291\n",
            "iter 541, loss is 0.8879518945081547\n",
            "iter 542, loss is 0.88487547182417\n",
            "iter 543, loss is 0.8818299324650426\n",
            "iter 544, loss is 0.8788149596816613\n",
            "iter 545, loss is 0.8758302399767722\n",
            "iter 546, loss is 0.8728754630715568\n",
            "iter 547, loss is 0.8699503218725593\n",
            "iter 548, loss is 0.8670545124389502\n",
            "iter 549, loss is 0.8641877339501295\n",
            "iter 550, loss is 0.8613496886736616\n",
            "iter 551, loss is 0.858540081933541\n",
            "iter 552, loss is 0.8557586220787847\n",
            "iter 553, loss is 0.8530050204523489\n",
            "iter 554, loss is 0.8502789913603617\n",
            "iter 555, loss is 0.8475802520416778\n",
            "iter 556, loss is 0.8449085226377426\n",
            "iter 557, loss is 0.842263526162767\n",
            "iter 558, loss is 0.8396449884742117\n",
            "iter 559, loss is 0.8370526382435713\n",
            "iter 560, loss is 0.8344862069274632\n",
            "iter 561, loss is 0.8319454287390088\n",
            "iter 562, loss is 0.8294300406195152\n",
            "iter 563, loss is 0.826939782210444\n",
            "iter 564, loss is 0.8244743958256695\n",
            "iter 565, loss is 0.8220336264240237\n",
            "iter 566, loss is 0.8196172215821208\n",
            "iter 567, loss is 0.8172249314674656\n",
            "iter 568, loss is 0.8148565088118342\n",
            "iter 569, loss is 0.8125117088849322\n",
            "iter 570, loss is 0.8101902894683202\n",
            "iter 571, loss is 0.8078920108296115\n",
            "iter 572, loss is 0.8056166356969326\n",
            "iter 573, loss is 0.8033639292336461\n",
            "iter 574, loss is 0.801133659013336\n",
            "iter 575, loss is 0.7989255949950489\n",
            "iter 576, loss is 0.7967395094987897\n",
            "iter 577, loss is 0.7945751771812686\n",
            "iter 578, loss is 0.7924323750119002\n",
            "iter 579, loss is 0.7903108822490464\n",
            "iter 580, loss is 0.7882104804165058\n",
            "iter 581, loss is 0.7861309532802451\n",
            "iter 582, loss is 0.7840720868253661\n",
            "iter 583, loss is 0.7820336692333163\n",
            "iter 584, loss is 0.7800154908593263\n",
            "iter 585, loss is 0.7780173442100851\n",
            "iter 586, loss is 0.7760390239216426\n",
            "iter 587, loss is 0.7740803267375386\n",
            "iter 588, loss is 0.7721410514871588\n",
            "iter 589, loss is 0.7702209990643114\n",
            "iter 590, loss is 0.7683199724060256\n",
            "iter 591, loss is 0.7664377764715669\n",
            "iter 592, loss is 0.7645742182216687\n",
            "iter 593, loss is 0.7627291065979777\n",
            "iter 594, loss is 0.76090225250271\n",
            "iter 595, loss is 0.7590934687785175\n",
            "iter 596, loss is 0.7573025701885596\n",
            "iter 597, loss is 0.7555293733967815\n",
            "iter 598, loss is 0.7537736969483945\n",
            "iter 599, loss is 0.7520353612505564\n",
            "iter 600, loss is 0.7503141885532523\n",
            "iter 601, loss is 0.7486100029303699\n",
            "iter 602, loss is 0.746922630260971\n",
            "iter 603, loss is 0.745251898210754\n",
            "iter 604, loss is 0.7435976362137093\n",
            "iter 605, loss is 0.7419596754539597\n",
            "iter 606, loss is 0.7403378488477921\n",
            "iter 607, loss is 0.7387319910258695\n",
            "iter 608, loss is 0.7371419383156284\n",
            "iter 609, loss is 0.7355675287238571\n",
            "iter 610, loss is 0.734008601919452\n",
            "iter 611, loss is 0.7324649992163512\n",
            "iter 612, loss is 0.7309365635566455\n",
            "iter 613, loss is 0.7294231394938595\n",
            "iter 614, loss is 0.7279245731764084\n",
            "iter 615, loss is 0.726440712331222\n",
            "iter 616, loss is 0.7249714062475381\n",
            "iter 617, loss is 0.723516505760863\n",
            "iter 618, loss is 0.7220758632370955\n",
            "iter 619, loss is 0.7206493325568151\n",
            "iter 620, loss is 0.7192367690997314\n",
            "iter 621, loss is 0.7178380297292938\n",
            "iter 622, loss is 0.7164529727774578\n",
            "iter 623, loss is 0.7150814580296103\n",
            "iter 624, loss is 0.7137233467096474\n",
            "iter 625, loss is 0.7123785014652069\n",
            "iter 626, loss is 0.7110467863530529\n",
            "iter 627, loss is 0.7097280668246094\n",
            "iter 628, loss is 0.7084222097116427\n",
            "iter 629, loss is 0.7071290832120929\n",
            "iter 630, loss is 0.7058485568760474\n",
            "iter 631, loss is 0.7045805015918631\n",
            "iter 632, loss is 0.7033247895724261\n",
            "iter 633, loss is 0.7020812943415571\n",
            "iter 634, loss is 0.7008498907205537\n",
            "iter 635, loss is 0.6996304548148722\n",
            "iter 636, loss is 0.6984228640009461\n",
            "iter 637, loss is 0.6972269969131392\n",
            "iter 638, loss is 0.6960427334308337\n",
            "iter 639, loss is 0.6948699546656509\n",
            "iter 640, loss is 0.6937085429488032\n",
            "iter 641, loss is 0.692558381818575\n",
            "iter 642, loss is 0.6914193560079335\n",
            "iter 643, loss is 0.6902913514322662\n",
            "iter 644, loss is 0.689174255177245\n",
            "iter 645, loss is 0.688067955486814\n",
            "iter 646, loss is 0.6869723417513008\n",
            "iter 647, loss is 0.6858873044956517\n",
            "iter 648, loss is 0.6848127353677848\n",
            "iter 649, loss is 0.6837485271270649\n",
            "iter 650, loss is 0.6826945736328973\n",
            "iter 651, loss is 0.6816507698334355\n",
            "iter 652, loss is 0.6806170117544096\n",
            "iter 653, loss is 0.6795931964880649\n",
            "iter 654, loss is 0.6785792221822176\n",
            "iter 655, loss is 0.6775749880294206\n",
            "iter 656, loss is 0.6765803942562416\n",
            "iter 657, loss is 0.6755953421126525\n",
            "iter 658, loss is 0.6746197338615244\n",
            "iter 659, loss is 0.6736534727682343\n",
            "iter 660, loss is 0.672696463090376\n",
            "iter 661, loss is 0.6717486100675784\n",
            "iter 662, loss is 0.6708098199114264\n",
            "iter 663, loss is 0.6698799997954872\n",
            "iter 664, loss is 0.6689590578454385\n",
            "iter 665, loss is 0.6680469031292972\n",
            "iter 666, loss is 0.6671434456477493\n",
            "iter 667, loss is 0.6662485963245798\n",
            "iter 668, loss is 0.6653622669971989\n",
            "iter 669, loss is 0.6644843704072672\n",
            "iter 670, loss is 0.6636148201914165\n",
            "iter 671, loss is 0.6627535308720661\n",
            "iter 672, loss is 0.6619004178483336\n",
            "iter 673, loss is 0.6610553973870381\n",
            "iter 674, loss is 0.6602183866137968\n",
            "iter 675, loss is 0.6593893035042134\n",
            "iter 676, loss is 0.6585680668751545\n",
            "iter 677, loss is 0.6577545963761195\n",
            "iter 678, loss is 0.6569488124806958\n",
            "iter 679, loss is 0.6561506364781028\n",
            "iter 680, loss is 0.6553599904648233\n",
            "iter 681, loss is 0.6545767973363202\n",
            "iter 682, loss is 0.6538009807788391\n",
            "iter 683, loss is 0.6530324652612934\n",
            "iter 684, loss is 0.6522711760272344\n",
            "iter 685, loss is 0.6515170390869027\n",
            "iter 686, loss is 0.650769981209363\n",
            "iter 687, loss is 0.6500299299147165\n",
            "iter 688, loss is 0.6492968134663966\n",
            "iter 689, loss is 0.6485705608635416\n",
            "iter 690, loss is 0.6478511018334467\n",
            "iter 691, loss is 0.6471383668240928\n",
            "iter 692, loss is 0.6464322869967528\n",
            "iter 693, loss is 0.6457327942186727\n",
            "iter 694, loss is 0.6450398210558291\n",
            "iter 695, loss is 0.6443533007657609\n",
            "iter 696, loss is 0.6436731672904729\n",
            "iter 697, loss is 0.6429993552494145\n",
            "iter 698, loss is 0.6423317999325291\n",
            "iter 699, loss is 0.6416704372933757\n",
            "iter 700, loss is 0.6410152039423201\n",
            "iter 701, loss is 0.6403660371397972\n",
            "iter 702, loss is 0.6397228747896416\n",
            "iter 703, loss is 0.6390856554324864\n",
            "iter 704, loss is 0.6384543182392313\n",
            "iter 705, loss is 0.6378288030045759\n",
            "iter 706, loss is 0.6372090501406207\n",
            "iter 707, loss is 0.6365950006705331\n",
            "iter 708, loss is 0.6359865962222787\n",
            "iter 709, loss is 0.6353837790224174\n",
            "iter 710, loss is 0.6347864918899612\n",
            "iter 711, loss is 0.6341946782302982\n",
            "iter 712, loss is 0.6336082820291767\n",
            "iter 713, loss is 0.6330272478467509\n",
            "iter 714, loss is 0.6324515208116895\n",
            "iter 715, loss is 0.631881046615344\n",
            "iter 716, loss is 0.6313157715059746\n",
            "iter 717, loss is 0.6307556422830398\n",
            "iter 718, loss is 0.6302006062915407\n",
            "iter 719, loss is 0.629650611416425\n",
            "iter 720, loss is 0.6291056060770492\n",
            "iter 721, loss is 0.628565539221695\n",
            "iter 722, loss is 0.6280303603221458\n",
            "iter 723, loss is 0.6275000193683146\n",
            "iter 724, loss is 0.6269744668629311\n",
            "iter 725, loss is 0.6264536538162803\n",
            "iter 726, loss is 0.6259375317409961\n",
            "iter 727, loss is 0.6254260526469096\n",
            "iter 728, loss is 0.6249191690359476\n",
            "iter 729, loss is 0.6244168338970864\n",
            "iter 730, loss is 0.6239190007013551\n",
            "iter 731, loss is 0.6234256233968917\n",
            "iter 732, loss is 0.6229366564040479\n",
            "iter 733, loss is 0.6224520546105482\n",
            "iter 734, loss is 0.6219717733666925\n",
            "iter 735, loss is 0.6214957684806154\n",
            "iter 736, loss is 0.6210239962135863\n",
            "iter 737, loss is 0.6205564132753645\n",
            "iter 738, loss is 0.6200929768195981\n",
            "iter 739, loss is 0.6196336444392707\n",
            "iter 740, loss is 0.619178374162196\n",
            "iter 741, loss is 0.6187271244465574\n",
            "iter 742, loss is 0.6182798541764941\n",
            "iter 743, loss is 0.6178365226577323\n",
            "iter 744, loss is 0.6173970896132605\n",
            "iter 745, loss is 0.6169615151790513\n",
            "iter 746, loss is 0.6165297598998242\n",
            "iter 747, loss is 0.6161017847248537\n",
            "iter 748, loss is 0.6156775510038212\n",
            "iter 749, loss is 0.6152570204827064\n",
            "iter 750, loss is 0.6148401552997244\n",
            "iter 751, loss is 0.6144269179813026\n",
            "iter 752, loss is 0.6140172714380975\n",
            "iter 753, loss is 0.613611178961057\n",
            "iter 754, loss is 0.6132086042175174\n",
            "iter 755, loss is 0.6128095112473448\n",
            "iter 756, loss is 0.6124138644591135\n",
            "iter 757, loss is 0.6120216286263255\n",
            "iter 758, loss is 0.6116327688836674\n",
            "iter 759, loss is 0.6112472507233054\n",
            "iter 760, loss is 0.6108650399912211\n",
            "iter 761, loss is 0.6104861028835807\n",
            "iter 762, loss is 0.6101104059431451\n",
            "iter 763, loss is 0.6097379160557148\n",
            "iter 764, loss is 0.609368600446612\n",
            "iter 765, loss is 0.6090024266771985\n",
            "iter 766, loss is 0.6086393626414297\n",
            "iter 767, loss is 0.6082793765624435\n",
            "iter 768, loss is 0.6079224369891845\n",
            "iter 769, loss is 0.607568512793063\n",
            "iter 770, loss is 0.6072175731646474\n",
            "iter 771, loss is 0.6068695876103922\n",
            "iter 772, loss is 0.6065245259493979\n",
            "iter 773, loss is 0.6061823583102042\n",
            "iter 774, loss is 0.6058430551276179\n",
            "iter 775, loss is 0.6055065871395712\n",
            "iter 776, loss is 0.6051729253840138\n",
            "iter 777, loss is 0.6048420411958358\n",
            "iter 778, loss is 0.604513906203823\n",
            "iter 779, loss is 0.6041884923276418\n",
            "iter 780, loss is 0.6038657717748592\n",
            "iter 781, loss is 0.6035457170379865\n",
            "iter 782, loss is 0.6032283008915607\n",
            "iter 783, loss is 0.6029134963892501\n",
            "iter 784, loss is 0.6026012768609934\n",
            "iter 785, loss is 0.6022916159101661\n",
            "iter 786, loss is 0.6019844874107763\n",
            "iter 787, loss is 0.6016798655046898\n",
            "iter 788, loss is 0.6013777245988831\n",
            "iter 789, loss is 0.6010780393627253\n",
            "iter 790, loss is 0.6007807847252868\n",
            "iter 791, loss is 0.600485935872677\n",
            "iter 792, loss is 0.6001934682454079\n",
            "iter 793, loss is 0.5999033575357852\n",
            "iter 794, loss is 0.5996155796853274\n",
            "iter 795, loss is 0.5993301108822093\n",
            "iter 796, loss is 0.5990469275587331\n",
            "iter 797, loss is 0.5987660063888252\n",
            "iter 798, loss is 0.5984873242855581\n",
            "iter 799, loss is 0.5982108583986987\n",
            "iter 800, loss is 0.5979365861122816\n",
            "iter 801, loss is 0.5976644850422056\n",
            "iter 802, loss is 0.5973945330338571\n",
            "iter 803, loss is 0.5971267081597574\n",
            "iter 804, loss is 0.5968609887172324\n",
            "iter 805, loss is 0.5965973532261084\n",
            "iter 806, loss is 0.5963357804264304\n",
            "iter 807, loss is 0.5960762492762042\n",
            "iter 808, loss is 0.5958187389491607\n",
            "iter 809, loss is 0.5955632288325453\n",
            "iter 810, loss is 0.5953096985249272\n",
            "iter 811, loss is 0.5950581278340336\n",
            "iter 812, loss is 0.5948084967746043\n",
            "iter 813, loss is 0.5945607855662696\n",
            "iter 814, loss is 0.5943149746314496\n",
            "iter 815, loss is 0.5940710445932734\n",
            "iter 816, loss is 0.5938289762735236\n",
            "iter 817, loss is 0.5935887506905975\n",
            "iter 818, loss is 0.5933503490574916\n",
            "iter 819, loss is 0.5931137527798066\n",
            "iter 820, loss is 0.5928789434537727\n",
            "iter 821, loss is 0.5926459028642939\n",
            "iter 822, loss is 0.5924146129830148\n",
            "iter 823, loss is 0.5921850559664052\n",
            "iter 824, loss is 0.5919572141538653\n",
            "iter 825, loss is 0.5917310700658499\n",
            "iter 826, loss is 0.5915066064020119\n",
            "iter 827, loss is 0.5912838060393655\n",
            "iter 828, loss is 0.5910626520304668\n",
            "iter 829, loss is 0.590843127601615\n",
            "iter 830, loss is 0.5906252161510699\n",
            "iter 831, loss is 0.59040890124729\n",
            "iter 832, loss is 0.5901941666271859\n",
            "iter 833, loss is 0.5899809961943949\n",
            "iter 834, loss is 0.5897693740175703\n",
            "iter 835, loss is 0.58955928432869\n",
            "iter 836, loss is 0.5893507115213816\n",
            "iter 837, loss is 0.5891436401492652\n",
            "iter 838, loss is 0.5889380549243138\n",
            "iter 839, loss is 0.5887339407152282\n",
            "iter 840, loss is 0.5885312825458312\n",
            "iter 841, loss is 0.588330065593477\n",
            "iter 842, loss is 0.5881302751874765\n",
            "iter 843, loss is 0.5879318968075404\n",
            "iter 844, loss is 0.5877349160822359\n",
            "iter 845, loss is 0.5875393187874618\n",
            "iter 846, loss is 0.5873450908449368\n",
            "iter 847, loss is 0.5871522183207053\n",
            "iter 848, loss is 0.5869606874236569\n",
            "iter 849, loss is 0.5867704845040634\n",
            "iter 850, loss is 0.5865815960521268\n",
            "iter 851, loss is 0.5863940086965471\n",
            "iter 852, loss is 0.5862077092031007\n",
            "iter 853, loss is 0.5860226844732362\n",
            "iter 854, loss is 0.585838921542682\n",
            "iter 855, loss is 0.5856564075800712\n",
            "iter 856, loss is 0.5854751298855776\n",
            "iter 857, loss is 0.585295075889568\n",
            "iter 858, loss is 0.5851162331512675\n",
            "iter 859, loss is 0.584938589357437\n",
            "iter 860, loss is 0.5847621323210678\n",
            "iter 861, loss is 0.5845868499800855\n",
            "iter 862, loss is 0.5844127303960701\n",
            "iter 863, loss is 0.5842397617529884\n",
            "iter 864, loss is 0.5840679323559385\n",
            "iter 865, loss is 0.5838972306299085\n",
            "iter 866, loss is 0.5837276451185474\n",
            "iter 867, loss is 0.583559164482948\n",
            "iter 868, loss is 0.5833917775004435\n",
            "iter 869, loss is 0.5832254730634152\n",
            "iter 870, loss is 0.583060240178114\n",
            "iter 871, loss is 0.5828960679634911\n",
            "iter 872, loss is 0.5827329456500446\n",
            "iter 873, loss is 0.5825708625786749\n",
            "iter 874, loss is 0.5824098081995526\n",
            "iter 875, loss is 0.5822497720709996\n",
            "iter 876, loss is 0.5820907438583777\n",
            "iter 877, loss is 0.5819327133329943\n",
            "iter 878, loss is 0.5817756703710145\n",
            "iter 879, loss is 0.5816196049523861\n",
            "iter 880, loss is 0.581464507159776\n",
            "iter 881, loss is 0.581310367177517\n",
            "iter 882, loss is 0.5811571752905659\n",
            "iter 883, loss is 0.5810049218834709\n",
            "iter 884, loss is 0.5808535974393517\n",
            "iter 885, loss is 0.5807031925388886\n",
            "iter 886, loss is 0.5805536978593214\n",
            "iter 887, loss is 0.5804051041734617\n",
            "iter 888, loss is 0.5802574023487101\n",
            "iter 889, loss is 0.5801105833460889\n",
            "iter 890, loss is 0.5799646382192815\n",
            "iter 891, loss is 0.5798195581136818\n",
            "iter 892, loss is 0.5796753342654555\n",
            "iter 893, loss is 0.5795319580006076\n",
            "iter 894, loss is 0.5793894207340633\n",
            "iter 895, loss is 0.5792477139687543\n",
            "iter 896, loss is 0.5791068292947193\n",
            "iter 897, loss is 0.5789667583882083\n",
            "iter 898, loss is 0.5788274930108003\n",
            "iter 899, loss is 0.5786890250085278\n",
            "iter 900, loss is 0.5785513463110121\n",
            "iter 901, loss is 0.5784144489306049\n",
            "iter 902, loss is 0.5782783249615409\n",
            "iter 903, loss is 0.5781429665790989\n",
            "iter 904, loss is 0.5780083660387697\n",
            "iter 905, loss is 0.577874515675435\n",
            "iter 906, loss is 0.5777414079025524\n",
            "iter 907, loss is 0.5776090352113511\n",
            "iter 908, loss is 0.5774773901700332\n",
            "iter 909, loss is 0.5773464654229857\n",
            "iter 910, loss is 0.5772162536899992\n",
            "iter 911, loss is 0.5770867477654946\n",
            "iter 912, loss is 0.5769579405177583\n",
            "iter 913, loss is 0.5768298248881846\n",
            "iter 914, loss is 0.5767023938905267\n",
            "iter 915, loss is 0.5765756406101545\n",
            "iter 916, loss is 0.5764495582033207\n",
            "iter 917, loss is 0.5763241398964337\n",
            "iter 918, loss is 0.5761993789853391\n",
            "iter 919, loss is 0.5760752688346071\n",
            "iter 920, loss is 0.5759518028768283\n",
            "iter 921, loss is 0.575828974611917\n",
            "iter 922, loss is 0.5757067776064191\n",
            "iter 923, loss is 0.5755852054928315\n",
            "iter 924, loss is 0.5754642519689243\n",
            "iter 925, loss is 0.575343910797072\n",
            "iter 926, loss is 0.5752241758035918\n",
            "iter 927, loss is 0.5751050408780869\n",
            "iter 928, loss is 0.5749864999727994\n",
            "iter 929, loss is 0.574868547101966\n",
            "iter 930, loss is 0.5747511763411848\n",
            "iter 931, loss is 0.5746343818267843\n",
            "iter 932, loss is 0.5745181577552018\n",
            "iter 933, loss is 0.5744024983823672\n",
            "iter 934, loss is 0.5742873980230925\n",
            "iter 935, loss is 0.5741728510504688\n",
            "iter 936, loss is 0.5740588518952685\n",
            "iter 937, loss is 0.5739453950453542\n",
            "iter 938, loss is 0.573832475045093\n",
            "iter 939, loss is 0.573720086494778\n",
            "iter 940, loss is 0.5736082240500538\n",
            "iter 941, loss is 0.5734968824213504\n",
            "iter 942, loss is 0.573386056373321\n",
            "iter 943, loss is 0.5732757407242851\n",
            "iter 944, loss is 0.5731659303456803\n",
            "iter 945, loss is 0.5730566201615163\n",
            "iter 946, loss is 0.5729478051478365\n",
            "iter 947, loss is 0.5728394803321839\n",
            "iter 948, loss is 0.5727316407930748\n",
            "iter 949, loss is 0.5726242816594741\n",
            "iter 950, loss is 0.57251739811028\n",
            "iter 951, loss is 0.5724109853738102\n",
            "iter 952, loss is 0.5723050387272972\n",
            "iter 953, loss is 0.5721995534963857\n",
            "iter 954, loss is 0.5720945250546366\n",
            "iter 955, loss is 0.571989948823036\n",
            "iter 956, loss is 0.5718858202695078\n",
            "iter 957, loss is 0.5717821349084349\n",
            "iter 958, loss is 0.5716788883001804\n",
            "iter 959, loss is 0.5715760760506176\n",
            "iter 960, loss is 0.5714736938106632\n",
            "iter 961, loss is 0.5713717372758156\n",
            "iter 962, loss is 0.5712702021856977\n",
            "iter 963, loss is 0.5711690843236045\n",
            "iter 964, loss is 0.5710683795160555\n",
            "iter 965, loss is 0.5709680836323516\n",
            "iter 966, loss is 0.5708681925841369\n",
            "iter 967, loss is 0.5707687023249634\n",
            "iter 968, loss is 0.5706696088498627\n",
            "iter 969, loss is 0.5705709081949201\n",
            "iter 970, loss is 0.5704725964368538\n",
            "iter 971, loss is 0.5703746696925986\n",
            "iter 972, loss is 0.5702771241188933\n",
            "iter 973, loss is 0.5701799559118728\n",
            "iter 974, loss is 0.5700831613066639\n",
            "iter 975, loss is 0.5699867365769862\n",
            "iter 976, loss is 0.569890678034756\n",
            "iter 977, loss is 0.5697949820296948\n",
            "iter 978, loss is 0.5696996449489415\n",
            "iter 979, loss is 0.5696046632166698\n",
            "iter 980, loss is 0.5695100332937066\n",
            "iter 981, loss is 0.5694157516771585\n",
            "iter 982, loss is 0.5693218149000383\n",
            "iter 983, loss is 0.5692282195308973\n",
            "iter 984, loss is 0.5691349621734613\n",
            "iter 985, loss is 0.5690420394662695\n",
            "iter 986, loss is 0.5689494480823178\n",
            "iter 987, loss is 0.5688571847287063\n",
            "iter 988, loss is 0.5687652461462878\n",
            "iter 989, loss is 0.5686736291093242\n",
            "iter 990, loss is 0.5685823304251415\n",
            "iter 991, loss is 0.5684913469337928\n",
            "iter 992, loss is 0.5684006755077221\n",
            "iter 993, loss is 0.5683103130514318\n",
            "iter 994, loss is 0.5682202565011546\n",
            "iter 995, loss is 0.5681305028245279\n",
            "iter 996, loss is 0.5680410490202722\n",
            "iter 997, loss is 0.567951892117872\n",
            "iter 998, loss is 0.5678630291772607\n",
            "iter 999, loss is 0.5677744572885082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "_t-xVsh1idj4",
        "outputId": "ad5c0de1-144e-407b-da03-09724b4a4243"
      },
      "source": [
        "plot_curve(range(num_iterations), losses)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQyLyRLovI_C"
      },
      "source": [
        "### Create loss and gradient functions\n",
        "\n",
        "Let's create functions and re-write the SGD optimization code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfDSC3rGidzl"
      },
      "source": [
        "def l2_loss(y, y_hat): \n",
        "  return np.sum((y - y_hat)**2) / (2*len(y))\n",
        "\n",
        "def l1_loss(y, y_hat):\n",
        "  return np.sum(np.abs(y - y_hat)) / len(y)\n",
        "\n",
        "def grad_l2_loss(y, y_hat):\n",
        "  return y - y_hat\n",
        "\n",
        "def gradient(loss_func, preds, X, y, weight, bias, lr): \n",
        "  error = loss_func(y, preds)\n",
        "  \n",
        "  dW = np.dot(-X.T, error) / N\n",
        "  db = -error / N\n",
        "\n",
        "  weight = weight - lr * dW \n",
        "  bias = bias - lr * db\n",
        "\n",
        "  return y_hat, weight, bias\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JlP2nxzUQwd",
        "outputId": "a530a094-2ff4-4a47-af5e-3373e6f76ab0"
      },
      "source": [
        "lr = 0.001\n",
        "weight = W.copy()\n",
        "bias = b.copy()\n",
        "best_loss = float(\"inf\")\n",
        "losses = []\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  y_hat = np.dot(X_train, weight) + bias\n",
        "  y_hat, weight, bias = gradient(grad_l2_loss, y_hat, X_train, y_train, weight, bias, lr)\n",
        "  loss = l2_loss(y_train, y_hat)\n",
        "\n",
        "  losses.append(loss)\n",
        "  if loss < best_loss:\n",
        "    best_loss = loss\n",
        "  print(\"iter %s, loss = %s\" % (i, loss))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss = 78.35036235953841\n",
            "iter 1, loss = 77.55345739716022\n",
            "iter 2, loss = 76.7647222072433\n",
            "iter 3, loss = 75.98407302683736\n",
            "iter 4, loss = 75.21142695179736\n",
            "iter 5, loss = 74.44670192797814\n",
            "iter 6, loss = 73.68981674251961\n",
            "iter 7, loss = 72.94069101522128\n",
            "iter 8, loss = 72.19924519000506\n",
            "iter 9, loss = 71.46540052646577\n",
            "iter 10, loss = 70.7390790915082\n",
            "iter 11, loss = 70.02020375106989\n",
            "iter 12, loss = 69.30869816192879\n",
            "iter 13, loss = 68.60448676359498\n",
            "iter 14, loss = 67.90749477028542\n",
            "iter 15, loss = 67.2176481629811\n",
            "iter 16, loss = 66.53487368156547\n",
            "iter 17, loss = 65.85909881704359\n",
            "iter 18, loss = 65.19025180384108\n",
            "iter 19, loss = 64.52826161218195\n",
            "iter 20, loss = 63.87305794054453\n",
            "iter 21, loss = 63.22457120819491\n",
            "iter 22, loss = 62.58273254779685\n",
            "iter 23, loss = 61.94747379809736\n",
            "iter 24, loss = 61.318727496687444\n",
            "iter 25, loss = 60.69642687283697\n",
            "iter 26, loss = 60.08050584040304\n",
            "iter 27, loss = 59.470898990811015\n",
            "iter 28, loss = 58.867541586107535\n",
            "iter 29, loss = 58.27036955208483\n",
            "iter 30, loss = 57.6793194714754\n",
            "iter 31, loss = 57.09432857721657\n",
            "iter 32, loss = 56.51533474578411\n",
            "iter 33, loss = 55.942276490594004\n",
            "iter 34, loss = 55.37509295547223\n",
            "iter 35, loss = 54.81372390819116\n",
            "iter 36, loss = 54.258109734072384\n",
            "iter 37, loss = 53.70819142965513\n",
            "iter 38, loss = 53.16391059642952\n",
            "iter 39, loss = 52.625209434634186\n",
            "iter 40, loss = 52.092030737117355\n",
            "iter 41, loss = 51.56431788326099\n",
            "iter 42, loss = 51.04201483296714\n",
            "iter 43, loss = 50.52506612070602\n",
            "iter 44, loss = 50.01341684962504\n",
            "iter 45, loss = 49.50701268571825\n",
            "iter 46, loss = 49.005799852055674\n",
            "iter 47, loss = 48.509725123071604\n",
            "iter 48, loss = 48.01873581891165\n",
            "iter 49, loss = 47.53277979983764\n",
            "iter 50, loss = 47.05180546068992\n",
            "iter 51, loss = 46.57576172540644\n",
            "iter 52, loss = 46.10459804159795\n",
            "iter 53, loss = 45.638264375179006\n",
            "iter 54, loss = 45.176711205053756\n",
            "iter 55, loss = 44.719889517856515\n",
            "iter 56, loss = 44.26775080274598\n",
            "iter 57, loss = 43.820247046253016\n",
            "iter 58, loss = 43.377330727181146\n",
            "iter 59, loss = 42.93895481155943\n",
            "iter 60, loss = 42.505072747647006\n",
            "iter 61, loss = 42.0756384609889\n",
            "iter 62, loss = 41.65060634952242\n",
            "iter 63, loss = 41.229931278733986\n",
            "iter 64, loss = 40.81356857686526\n",
            "iter 65, loss = 40.40147403016872\n",
            "iter 66, loss = 39.993603878211694\n",
            "iter 67, loss = 39.58991480922868\n",
            "iter 68, loss = 39.1903639555212\n",
            "iter 69, loss = 38.7949088889049\n",
            "iter 70, loss = 38.40350761620323\n",
            "iter 71, loss = 38.01611857478749\n",
            "iter 72, loss = 37.632700628162446\n",
            "iter 73, loss = 37.253213061597286\n",
            "iter 74, loss = 36.87761557780134\n",
            "iter 75, loss = 36.50586829264417\n",
            "iter 76, loss = 36.137931730919476\n",
            "iter 77, loss = 35.773766822152545\n",
            "iter 78, loss = 35.41333489645054\n",
            "iter 79, loss = 35.056597680395484\n",
            "iter 80, loss = 34.703517292979214\n",
            "iter 81, loss = 34.354056241580125\n",
            "iter 82, loss = 34.00817741798112\n",
            "iter 83, loss = 33.66584409442835\n",
            "iter 84, loss = 33.32701991973045\n",
            "iter 85, loss = 32.99166891539765\n",
            "iter 86, loss = 32.6597554718206\n",
            "iter 87, loss = 32.3312443444883\n",
            "iter 88, loss = 32.00610065024476\n",
            "iter 89, loss = 31.684289863584205\n",
            "iter 90, loss = 31.36577781298404\n",
            "iter 91, loss = 31.050530677275624\n",
            "iter 92, loss = 30.738514982052106\n",
            "iter 93, loss = 30.42969759611315\n",
            "iter 94, loss = 30.124045727946136\n",
            "iter 95, loss = 29.821526922243372\n",
            "iter 96, loss = 29.522109056455076\n",
            "iter 97, loss = 29.225760337377643\n",
            "iter 98, loss = 28.932449297776976\n",
            "iter 99, loss = 28.642144793046317\n",
            "iter 100, loss = 28.354815997898488\n",
            "iter 101, loss = 28.070432403091893\n",
            "iter 102, loss = 27.788963812190225\n",
            "iter 103, loss = 27.510380338355294\n",
            "iter 104, loss = 27.2346524011728\n",
            "iter 105, loss = 26.961750723510626\n",
            "iter 106, loss = 26.69164632840934\n",
            "iter 107, loss = 26.424310536004597\n",
            "iter 108, loss = 26.159714960481043\n",
            "iter 109, loss = 25.89783150705754\n",
            "iter 110, loss = 25.638632369003208\n",
            "iter 111, loss = 25.382090024684125\n",
            "iter 112, loss = 25.128177234640283\n",
            "iter 113, loss = 24.87686703869249\n",
            "iter 114, loss = 24.628132753079\n",
            "iter 115, loss = 24.38194796762141\n",
            "iter 116, loss = 24.138286542919705\n",
            "iter 117, loss = 23.897122607576\n",
            "iter 118, loss = 23.658430555446774\n",
            "iter 119, loss = 23.422185042923246\n",
            "iter 120, loss = 23.188360986239704\n",
            "iter 121, loss = 22.95693355880934\n",
            "iter 122, loss = 22.727878188587468\n",
            "iter 123, loss = 22.501170555461755\n",
            "iter 124, loss = 22.27678658866918\n",
            "iter 125, loss = 22.05470246423955\n",
            "iter 126, loss = 21.834894602465145\n",
            "iter 127, loss = 21.61733966539635\n",
            "iter 128, loss = 21.402014554362967\n",
            "iter 129, loss = 21.18889640752093\n",
            "iter 130, loss = 20.97796259742414\n",
            "iter 131, loss = 20.769190728621293\n",
            "iter 132, loss = 20.562558635277217\n",
            "iter 133, loss = 20.3580443788187\n",
            "iter 134, loss = 20.15562624560441\n",
            "iter 135, loss = 19.955282744618707\n",
            "iter 136, loss = 19.75699260518911\n",
            "iter 137, loss = 19.560734774727155\n",
            "iter 138, loss = 19.366488416492444\n",
            "iter 139, loss = 19.174232907379565\n",
            "iter 140, loss = 18.983947835727747\n",
            "iter 141, loss = 18.79561299915298\n",
            "iter 142, loss = 18.609208402402317\n",
            "iter 143, loss = 18.424714255230192\n",
            "iter 144, loss = 18.24211097029656\n",
            "iter 145, loss = 18.061379161086503\n",
            "iter 146, loss = 17.88249963985122\n",
            "iter 147, loss = 17.705453415570144\n",
            "iter 148, loss = 17.53022169193389\n",
            "iter 149, loss = 17.356785865347938\n",
            "iter 150, loss = 17.18512752295677\n",
            "iter 151, loss = 17.01522844068824\n",
            "iter 152, loss = 16.847070581318036\n",
            "iter 153, loss = 16.680636092553954\n",
            "iter 154, loss = 16.515907305139823\n",
            "iter 155, loss = 16.35286673097889\n",
            "iter 156, loss = 16.1914970612764\n",
            "iter 157, loss = 16.03178116470128\n",
            "iter 158, loss = 15.873702085566608\n",
            "iter 159, loss = 15.717243042028786\n",
            "iter 160, loss = 15.562387424305154\n",
            "iter 161, loss = 15.409118792909883\n",
            "iter 162, loss = 15.25742087690796\n",
            "iter 163, loss = 15.107277572187053\n",
            "iter 164, loss = 14.958672939747133\n",
            "iter 165, loss = 14.811591204007591\n",
            "iter 166, loss = 14.666016751131746\n",
            "iter 167, loss = 14.521934127368501\n",
            "iter 168, loss = 14.379328037411025\n",
            "iter 169, loss = 14.23818334277226\n",
            "iter 170, loss = 14.098485060177067\n",
            "iter 171, loss = 13.96021835997088\n",
            "iter 172, loss = 13.823368564544653\n",
            "iter 173, loss = 13.687921146775977\n",
            "iter 174, loss = 13.553861728486162\n",
            "iter 175, loss = 13.421176078913154\n",
            "iter 176, loss = 13.289850113200094\n",
            "iter 177, loss = 13.159869890899381\n",
            "iter 178, loss = 13.031221614492086\n",
            "iter 179, loss = 12.903891627922507\n",
            "iter 180, loss = 12.777866415147793\n",
            "iter 181, loss = 12.653132598702415\n",
            "iter 182, loss = 12.529676938277364\n",
            "iter 183, loss = 12.407486329313903\n",
            "iter 184, loss = 12.28654780161175\n",
            "iter 185, loss = 12.166848517951529\n",
            "iter 186, loss = 12.048375772731342\n",
            "iter 187, loss = 11.931116990617317\n",
            "iter 188, loss = 11.815059725208007\n",
            "iter 189, loss = 11.700191657712452\n",
            "iter 190, loss = 11.58650059564183\n",
            "iter 191, loss = 11.4739744715145\n",
            "iter 192, loss = 11.362601341574328\n",
            "iter 193, loss = 11.252369384522147\n",
            "iter 194, loss = 11.14326690026025\n",
            "iter 195, loss = 11.035282308649712\n",
            "iter 196, loss = 10.92840414828048\n",
            "iter 197, loss = 10.822621075254089\n",
            "iter 198, loss = 10.717921861978814\n",
            "iter 199, loss = 10.614295395977209\n",
            "iter 200, loss = 10.51173067870587\n",
            "iter 201, loss = 10.410216824387284\n",
            "iter 202, loss = 10.30974305885366\n",
            "iter 203, loss = 10.210298718402628\n",
            "iter 204, loss = 10.111873248664637\n",
            "iter 205, loss = 10.014456203482014\n",
            "iter 206, loss = 9.91803724379947\n",
            "iter 207, loss = 9.822606136566003\n",
            "iter 208, loss = 9.728152753648065\n",
            "iter 209, loss = 9.634667070753855\n",
            "iter 210, loss = 9.542139166368651\n",
            "iter 211, loss = 9.450559220701066\n",
            "iter 212, loss = 9.35991751464009\n",
            "iter 213, loss = 9.270204428722824\n",
            "iter 214, loss = 9.181410442112835\n",
            "iter 215, loss = 9.093526131588929\n",
            "iter 216, loss = 9.006542170544343\n",
            "iter 217, loss = 8.920449327996161\n",
            "iter 218, loss = 8.835238467604908\n",
            "iter 219, loss = 8.750900546704193\n",
            "iter 220, loss = 8.667426615340288\n",
            "iter 221, loss = 8.584807815321572\n",
            "iter 222, loss = 8.503035379277703\n",
            "iter 223, loss = 8.422100629728456\n",
            "iter 224, loss = 8.34199497816209\n",
            "iter 225, loss = 8.262709924123168\n",
            "iter 226, loss = 8.18423705430975\n",
            "iter 227, loss = 8.106568041679795\n",
            "iter 228, loss = 8.029694644566794\n",
            "iter 229, loss = 7.953608705804395\n",
            "iter 230, loss = 7.878302151860065\n",
            "iter 231, loss = 7.803766991977592\n",
            "iter 232, loss = 7.729995317328407\n",
            "iter 233, loss = 7.65697930017159\n",
            "iter 234, loss = 7.584711193022497\n",
            "iter 235, loss = 7.513183327829904\n",
            "iter 236, loss = 7.442388115161597\n",
            "iter 237, loss = 7.372318043398306\n",
            "iter 238, loss = 7.302965677935895\n",
            "iter 239, loss = 7.234323660395757\n",
            "iter 240, loss = 7.166384707843273\n",
            "iter 241, loss = 7.099141612014308\n",
            "iter 242, loss = 7.032587238549632\n",
            "iter 243, loss = 6.966714526237178\n",
            "iter 244, loss = 6.901516486262082\n",
            "iter 245, loss = 6.836986201464416\n",
            "iter 246, loss = 6.773116825604511\n",
            "iter 247, loss = 6.709901582635838\n",
            "iter 248, loss = 6.647333765985323\n",
            "iter 249, loss = 6.585406737841053\n",
            "iter 250, loss = 6.52411392844728\n",
            "iter 251, loss = 6.463448835406655\n",
            "iter 252, loss = 6.403405022989614\n",
            "iter 253, loss = 6.343976121450852\n",
            "iter 254, loss = 6.285155826352789\n",
            "iter 255, loss = 6.226937897895993\n",
            "iter 256, loss = 6.16931616025645\n",
            "iter 257, loss = 6.1122845009296425\n",
            "iter 258, loss = 6.055836870081343\n",
            "iter 259, loss = 5.99996727990507\n",
            "iter 260, loss = 5.944669803986128\n",
            "iter 261, loss = 5.88993857667217\n",
            "iter 262, loss = 5.835767792450216\n",
            "iter 263, loss = 5.78215170533005\n",
            "iter 264, loss = 5.7290846282339505\n",
            "iter 265, loss = 5.676560932392667\n",
            "iter 266, loss = 5.6245750467476014\n",
            "iter 267, loss = 5.573121457359105\n",
            "iter 268, loss = 5.522194706820851\n",
            "iter 269, loss = 5.471789393680211\n",
            "iter 270, loss = 5.421900171864577\n",
            "iter 271, loss = 5.372521750113549\n",
            "iter 272, loss = 5.323648891416966\n",
            "iter 273, loss = 5.275276412458689\n",
            "iter 274, loss = 5.227399183066079\n",
            "iter 275, loss = 5.180012125665135\n",
            "iter 276, loss = 5.133110214741205\n",
            "iter 277, loss = 5.086688476305235\n",
            "iter 278, loss = 5.040741987365487\n",
            "iter 279, loss = 4.995265875404673\n",
            "iter 280, loss = 4.950255317862448\n",
            "iter 281, loss = 4.905705541623212\n",
            "iter 282, loss = 4.861611822509169\n",
            "iter 283, loss = 4.817969484778569\n",
            "iter 284, loss = 4.774773900629102\n",
            "iter 285, loss = 4.732020489706397\n",
            "iter 286, loss = 4.689704718617528\n",
            "iter 287, loss = 4.6478221004495515\n",
            "iter 288, loss = 4.60636819429293\n",
            "iter 289, loss = 4.565338604769888\n",
            "iter 290, loss = 4.524728981567577\n",
            "iter 291, loss = 4.48453501897603\n",
            "iter 292, loss = 4.444752455430864\n",
            "iter 293, loss = 4.405377073060658\n",
            "iter 294, loss = 4.366404697238981\n",
            "iter 295, loss = 4.327831196141009\n",
            "iter 296, loss = 4.289652480304685\n",
            "iter 297, loss = 4.251864502196391\n",
            "iter 298, loss = 4.214463255781049\n",
            "iter 299, loss = 4.177444776096663\n",
            "iter 300, loss = 4.140805138833188\n",
            "iter 301, loss = 4.104540459915744\n",
            "iter 302, loss = 4.068646895092088\n",
            "iter 303, loss = 4.033120639524322\n",
            "iter 304, loss = 3.997957927384785\n",
            "iter 305, loss = 3.963155031456089\n",
            "iter 306, loss = 3.928708262735255\n",
            "iter 307, loss = 3.8946139700419087\n",
            "iter 308, loss = 3.860868539630492\n",
            "iter 309, loss = 3.82746839480645\n",
            "iter 310, loss = 3.794409995546358\n",
            "iter 311, loss = 3.761689838121938\n",
            "iter 312, loss = 3.729304454727928\n",
            "iter 313, loss = 3.697250413113775\n",
            "iter 314, loss = 3.6655243162190994\n",
            "iter 315, loss = 3.634122801812893\n",
            "iter 316, loss = 3.603042542136424\n",
            "iter 317, loss = 3.5722802435497987\n",
            "iter 318, loss = 3.5418326461821468\n",
            "iter 319, loss = 3.5116965235853947\n",
            "iter 320, loss = 3.481868682391589\n",
            "iter 321, loss = 3.4523459619737302\n",
            "iter 322, loss = 3.4231252341100884\n",
            "iter 323, loss = 3.394203402651959\n",
            "iter 324, loss = 3.3655774031948202\n",
            "iter 325, loss = 3.33724420275287\n",
            "iter 326, loss = 3.3092007994368955\n",
            "iter 327, loss = 3.2814442221354447\n",
            "iter 328, loss = 3.253971530199268\n",
            "iter 329, loss = 3.2267798131289953\n",
            "iter 330, loss = 3.1998661902660186\n",
            "iter 331, loss = 3.1732278104865337\n",
            "iter 332, loss = 3.146861851898732\n",
            "iter 333, loss = 3.1207655215430874\n",
            "iter 334, loss = 3.0949360550957197\n",
            "iter 335, loss = 3.0693707165747997\n",
            "iter 336, loss = 3.0440667980499625\n",
            "iter 337, loss = 3.0190216193547017\n",
            "iter 338, loss = 2.9942325278017132\n",
            "iter 339, loss = 2.969696897901155\n",
            "iter 340, loss = 2.9454121310817976\n",
            "iter 341, loss = 2.9213756554150314\n",
            "iter 342, loss = 2.897584925341708\n",
            "iter 343, loss = 2.874037421401776\n",
            "iter 344, loss = 2.8507306499666933\n",
            "iter 345, loss = 2.827662142974584\n",
            "iter 346, loss = 2.804829457668103\n",
            "iter 347, loss = 2.782230176335001\n",
            "iter 348, loss = 2.7598619060513343\n",
            "iter 349, loss = 2.737722278427321\n",
            "iter 350, loss = 2.7158089493557944\n",
            "iter 351, loss = 2.694119598763237\n",
            "iter 352, loss = 2.6726519303633713\n",
            "iter 353, loss = 2.6514036714132727\n",
            "iter 354, loss = 2.6303725724719818\n",
            "iter 355, loss = 2.609556407161599\n",
            "iter 356, loss = 2.5889529719308166\n",
            "iter 357, loss = 2.5685600858208884\n",
            "iter 358, loss = 2.548375590233989\n",
            "iter 359, loss = 2.5283973487039537\n",
            "iter 360, loss = 2.508623246669365\n",
            "iter 361, loss = 2.489051191248968\n",
            "iter 362, loss = 2.469679111019391\n",
            "iter 363, loss = 2.4505049557951377\n",
            "iter 364, loss = 2.431526696410841\n",
            "iter 365, loss = 2.412742324505747\n",
            "iter 366, loss = 2.394149852310413\n",
            "iter 367, loss = 2.3757473124355846\n",
            "iter 368, loss = 2.3575327576632414\n",
            "iter 369, loss = 2.339504260739791\n",
            "iter 370, loss = 2.3216599141713696\n",
            "iter 371, loss = 2.303997830021255\n",
            "iter 372, loss = 2.2865161397093527\n",
            "iter 373, loss = 2.2692129938137264\n",
            "iter 374, loss = 2.2520865618741848\n",
            "iter 375, loss = 2.235135032197863\n",
            "iter 376, loss = 2.2183566116668088\n",
            "iter 377, loss = 2.2017495255475352\n",
            "iter 378, loss = 2.1853120173025284\n",
            "iter 379, loss = 2.169042348403691\n",
            "iter 380, loss = 2.15293879814769\n",
            "iter 381, loss = 2.1369996634732074\n",
            "iter 382, loss = 2.1212232587800584\n",
            "iter 383, loss = 2.105607915750164\n",
            "iter 384, loss = 2.090151983170365\n",
            "iter 385, loss = 2.0748538267570447\n",
            "iter 386, loss = 2.059711828982556\n",
            "iter 387, loss = 2.044724388903424\n",
            "iter 388, loss = 2.0298899219903146\n",
            "iter 389, loss = 2.0152068599597395\n",
            "iter 390, loss = 2.0006736506074945\n",
            "iter 391, loss = 1.986288757643798\n",
            "iter 392, loss = 1.9720506605301287\n",
            "iter 393, loss = 1.9579578543177247\n",
            "iter 394, loss = 1.9440088494877505\n",
            "iter 395, loss = 1.930202171793093\n",
            "iter 396, loss = 1.9165363621017855\n",
            "iter 397, loss = 1.9030099762420347\n",
            "iter 398, loss = 1.8896215848488365\n",
            "iter 399, loss = 1.8763697732121656\n",
            "iter 400, loss = 1.8632531411267201\n",
            "iter 401, loss = 1.8502703027432084\n",
            "iter 402, loss = 1.8374198864211582\n",
            "iter 403, loss = 1.8247005345832379\n",
            "iter 404, loss = 1.8121109035710703\n",
            "iter 405, loss = 1.7996496635025214\n",
            "iter 406, loss = 1.787315498130459\n",
            "iter 407, loss = 1.7751071047029534\n",
            "iter 408, loss = 1.763023193824914\n",
            "iter 409, loss = 1.7510624893211437\n",
            "iter 410, loss = 1.7392237281008005\n",
            "iter 411, loss = 1.7275056600232437\n",
            "iter 412, loss = 1.7159070477652585\n",
            "iter 413, loss = 1.7044266666896417\n",
            "iter 414, loss = 1.6930633047151342\n",
            "iter 415, loss = 1.6818157621876884\n",
            "iter 416, loss = 1.6706828517530532\n",
            "iter 417, loss = 1.659663398230667\n",
            "iter 418, loss = 1.6487562384888432\n",
            "iter 419, loss = 1.6379602213212365\n",
            "iter 420, loss = 1.6272742073245723\n",
            "iter 421, loss = 1.6166970687776354\n",
            "iter 422, loss = 1.6062276895214926\n",
            "iter 423, loss = 1.5958649648409493\n",
            "iter 424, loss = 1.5856078013472186\n",
            "iter 425, loss = 1.5754551168617938\n",
            "iter 426, loss = 1.5654058403015112\n",
            "iter 427, loss = 1.5554589115647914\n",
            "iter 428, loss = 1.545613281419049\n",
            "iter 429, loss = 1.5358679113892528\n",
            "iter 430, loss = 1.5262217736476333\n",
            "iter 431, loss = 1.5166738509045143\n",
            "iter 432, loss = 1.5072231363002726\n",
            "iter 433, loss = 1.4978686332983968\n",
            "iter 434, loss = 1.48860935557965\n",
            "iter 435, loss = 1.4794443269373112\n",
            "iter 436, loss = 1.4703725811734942\n",
            "iter 437, loss = 1.4613931619965306\n",
            "iter 438, loss = 1.4525051229194015\n",
            "iter 439, loss = 1.443707527159213\n",
            "iter 440, loss = 1.4349994475377028\n",
            "iter 441, loss = 1.426379966382763\n",
            "iter 442, loss = 1.4178481754309782\n",
            "iter 443, loss = 1.4094031757311591\n",
            "iter 444, loss = 1.4010440775488648\n",
            "iter 445, loss = 1.3927700002719081\n",
            "iter 446, loss = 1.384580072316823\n",
            "iter 447, loss = 1.376473431036299\n",
            "iter 448, loss = 1.3684492226275566\n",
            "iter 449, loss = 1.36050660204167\n",
            "iter 450, loss = 1.3526447328938125\n",
            "iter 451, loss = 1.344862787374426\n",
            "iter 452, loss = 1.337159946161301\n",
            "iter 453, loss = 1.3295353983325577\n",
            "iter 454, loss = 1.3219883412805176\n",
            "iter 455, loss = 1.314517980626461\n",
            "iter 456, loss = 1.3071235301362565\n",
            "iter 457, loss = 1.299804211636857\n",
            "iter 458, loss = 1.292559254933649\n",
            "iter 459, loss = 1.2853878977286535\n",
            "iter 460, loss = 1.2782893855395623\n",
            "iter 461, loss = 1.271262971619605\n",
            "iter 462, loss = 1.2643079168782396\n",
            "iter 463, loss = 1.2574234898026528\n",
            "iter 464, loss = 1.250608966380068\n",
            "iter 465, loss = 1.2438636300208497\n",
            "iter 466, loss = 1.2371867714823934\n",
            "iter 467, loss = 1.2305776887938005\n",
            "iter 468, loss = 1.22403568718132\n",
            "iter 469, loss = 1.2175600789945609\n",
            "iter 470, loss = 1.2111501836334542\n",
            "iter 471, loss = 1.2048053274759711\n",
            "iter 472, loss = 1.1985248438065748\n",
            "iter 473, loss = 1.1923080727454125\n",
            "iter 474, loss = 1.1861543611782284\n",
            "iter 475, loss = 1.1800630626869997\n",
            "iter 476, loss = 1.174033537481279\n",
            "iter 477, loss = 1.1680651523302439\n",
            "iter 478, loss = 1.162157280495443\n",
            "iter 479, loss = 1.1563093016642314\n",
            "iter 480, loss = 1.1505206018838863\n",
            "iter 481, loss = 1.144790573496401\n",
            "iter 482, loss = 1.1391186150739467\n",
            "iter 483, loss = 1.1335041313549958\n",
            "iter 484, loss = 1.1279465331810987\n",
            "iter 485, loss = 1.1224452374343117\n",
            "iter 486, loss = 1.116999666975264\n",
            "iter 487, loss = 1.111609250581861\n",
            "iter 488, loss = 1.1062734228886157\n",
            "iter 489, loss = 1.1009916243266011\n",
            "iter 490, loss = 1.0957633010640202\n",
            "iter 491, loss = 1.0905879049473841\n",
            "iter 492, loss = 1.0854648934432938\n",
            "iter 493, loss = 1.080393729580817\n",
            "iter 494, loss = 1.0753738818944594\n",
            "iter 495, loss = 1.0704048243677164\n",
            "iter 496, loss = 1.0654860363772078\n",
            "iter 497, loss = 1.0606170026373811\n",
            "iter 498, loss = 1.0557972131457853\n",
            "iter 499, loss = 1.0510261631289035\n",
            "iter 500, loss = 1.0463033529885417\n",
            "iter 501, loss = 1.0416282882487684\n",
            "iter 502, loss = 1.0370004795033951\n",
            "iter 503, loss = 1.0324194423639983\n",
            "iter 504, loss = 1.0278846974084739\n",
            "iter 505, loss = 1.023395770130117\n",
            "iter 506, loss = 1.018952190887227\n",
            "iter 507, loss = 1.0145534948532262\n",
            "iter 508, loss = 1.0101992219672924\n",
            "iter 509, loss = 1.0058889168854952\n",
            "iter 510, loss = 1.001622128932436\n",
            "iter 511, loss = 0.9973984120533823\n",
            "iter 512, loss = 0.993217324766892\n",
            "iter 513, loss = 0.989078430117925\n",
            "iter 514, loss = 0.9849812956314346\n",
            "iter 515, loss = 0.9809254932664359\n",
            "iter 516, loss = 0.9769105993705426\n",
            "iter 517, loss = 0.9729361946349736\n",
            "iter 518, loss = 0.9690018640500163\n",
            "iter 519, loss = 0.9651071968609523\n",
            "iter 520, loss = 0.9612517865244283\n",
            "iter 521, loss = 0.9574352306652806\n",
            "iter 522, loss = 0.9536571310337982\n",
            "iter 523, loss = 0.9499170934634258\n",
            "iter 524, loss = 0.946214727828899\n",
            "iter 525, loss = 0.9425496480048112\n",
            "iter 526, loss = 0.938921471824603\n",
            "iter 527, loss = 0.9353298210399729\n",
            "iter 528, loss = 0.931774321280704\n",
            "iter 529, loss = 0.9282546020149042\n",
            "iter 530, loss = 0.92477029650965\n",
            "iter 531, loss = 0.9213210417920396\n",
            "iter 532, loss = 0.9179064786106388\n",
            "iter 533, loss = 0.9145262513973285\n",
            "iter 534, loss = 0.9111800082295382\n",
            "iter 535, loss = 0.9078674007928703\n",
            "iter 536, loss = 0.9045880843441055\n",
            "iter 537, loss = 0.9013417176745886\n",
            "iter 538, loss = 0.8981279630739889\n",
            "iter 539, loss = 0.8949464862944324\n",
            "iter 540, loss = 0.8917969565150033\n",
            "iter 541, loss = 0.8886790463066063\n",
            "iter 542, loss = 0.8855924315971919\n",
            "iter 543, loss = 0.882536791637337\n",
            "iter 544, loss = 0.8795118089661791\n",
            "iter 545, loss = 0.8765171693776984\n",
            "iter 546, loss = 0.8735525618873475\n",
            "iter 547, loss = 0.8706176786990221\n",
            "iter 548, loss = 0.8677122151723696\n",
            "iter 549, loss = 0.864835869790435\n",
            "iter 550, loss = 0.8619883441276369\n",
            "iter 551, loss = 0.8591693428180717\n",
            "iter 552, loss = 0.8563785735241444\n",
            "iter 553, loss = 0.8536157469055189\n",
            "iter 554, loss = 0.8508805765883884\n",
            "iter 555, loss = 0.8481727791350601\n",
            "iter 556, loss = 0.8454920740138507\n",
            "iter 557, loss = 0.8428381835692937\n",
            "iter 558, loss = 0.8402108329926482\n",
            "iter 559, loss = 0.8376097502927135\n",
            "iter 560, loss = 0.8350346662669396\n",
            "iter 561, loss = 0.8324853144728378\n",
            "iter 562, loss = 0.8299614311996808\n",
            "iter 563, loss = 0.8274627554404952\n",
            "iter 564, loss = 0.8249890288643401\n",
            "iter 565, loss = 0.8225399957888708\n",
            "iter 566, loss = 0.8201154031531824\n",
            "iter 567, loss = 0.8177150004909335\n",
            "iter 568, loss = 0.815338539903744\n",
            "iter 569, loss = 0.8129857760348674\n",
            "iter 570, loss = 0.8106564660431301\n",
            "iter 571, loss = 0.8083503695771418\n",
            "iter 572, loss = 0.8060672487497663\n",
            "iter 573, loss = 0.803806868112858\n",
            "iter 574, loss = 0.801568994632253\n",
            "iter 575, loss = 0.7993533976630215\n",
            "iter 576, loss = 0.7971598489249699\n",
            "iter 577, loss = 0.794988122478396\n",
            "iter 578, loss = 0.7928379947000924\n",
            "iter 579, loss = 0.7907092442595962\n",
            "iter 580, loss = 0.7886016520956814\n",
            "iter 581, loss = 0.7865150013930933\n",
            "iter 582, loss = 0.784449077559521\n",
            "iter 583, loss = 0.7824036682028057\n",
            "iter 584, loss = 0.7803785631083833\n",
            "iter 585, loss = 0.7783735542169578\n",
            "iter 586, loss = 0.7763884356024036\n",
            "iter 587, loss = 0.7744230034498945\n",
            "iter 588, loss = 0.7724770560342578\n",
            "iter 589, loss = 0.7705503936985484\n",
            "iter 590, loss = 0.7686428188328442\n",
            "iter 591, loss = 0.766754135853258\n",
            "iter 592, loss = 0.7648841511811659\n",
            "iter 593, loss = 0.7630326732226465\n",
            "iter 594, loss = 0.7611995123481328\n",
            "iter 595, loss = 0.7593844808722724\n",
            "iter 596, loss = 0.757587393033993\n",
            "iter 597, loss = 0.7558080649767739\n",
            "iter 598, loss = 0.7540463147291182\n",
            "iter 599, loss = 0.752301962185227\n",
            "iter 600, loss = 0.7505748290858703\n",
            "iter 601, loss = 0.7488647389994539\n",
            "iter 602, loss = 0.7471715173032822\n",
            "iter 603, loss = 0.745494991165011\n",
            "iter 604, loss = 0.7438349895242916\n",
            "iter 605, loss = 0.7421913430746031\n",
            "iter 606, loss = 0.7405638842452706\n",
            "iter 607, loss = 0.7389524471836681\n",
            "iter 608, loss = 0.7373568677376031\n",
            "iter 609, loss = 0.7357769834378832\n",
            "iter 610, loss = 0.7342126334810598\n",
            "iter 611, loss = 0.7326636587123504\n",
            "iter 612, loss = 0.7311299016087346\n",
            "iter 613, loss = 0.7296112062622241\n",
            "iter 614, loss = 0.7281074183633041\n",
            "iter 615, loss = 0.7266183851845446\n",
            "iter 616, loss = 0.725143955564379\n",
            "iter 617, loss = 0.7236839798910508\n",
            "iter 618, loss = 0.7222383100867222\n",
            "iter 619, loss = 0.7208067995917489\n",
            "iter 620, loss = 0.7193893033491124\n",
            "iter 621, loss = 0.7179856777890156\n",
            "iter 622, loss = 0.7165957808136333\n",
            "iter 623, loss = 0.7152194717820212\n",
            "iter 624, loss = 0.7138566114951788\n",
            "iter 625, loss = 0.7125070621812645\n",
            "iter 626, loss = 0.7111706874809645\n",
            "iter 627, loss = 0.7098473524330097\n",
            "iter 628, loss = 0.7085369234598415\n",
            "iter 629, loss = 0.7072392683534258\n",
            "iter 630, loss = 0.7059542562612111\n",
            "iter 631, loss = 0.7046817576722307\n",
            "iter 632, loss = 0.7034216444033483\n",
            "iter 633, loss = 0.7021737895856429\n",
            "iter 634, loss = 0.7009380676509364\n",
            "iter 635, loss = 0.6997143543184551\n",
            "iter 636, loss = 0.6985025265816318\n",
            "iter 637, loss = 0.697302462695041\n",
            "iter 638, loss = 0.6961140421614684\n",
            "iter 639, loss = 0.6949371457191142\n",
            "iter 640, loss = 0.693771655328925\n",
            "iter 641, loss = 0.6926174541620594\n",
            "iter 642, loss = 0.6914744265874777\n",
            "iter 643, loss = 0.6903424581596629\n",
            "iter 644, loss = 0.6892214356064644\n",
            "iter 645, loss = 0.6881112468170691\n",
            "iter 646, loss = 0.6870117808300928\n",
            "iter 647, loss = 0.6859229278217969\n",
            "iter 648, loss = 0.6848445790944228\n",
            "iter 649, loss = 0.6837766270646491\n",
            "iter 650, loss = 0.6827189652521644\n",
            "iter 651, loss = 0.6816714882683584\n",
            "iter 652, loss = 0.6806340918051297\n",
            "iter 653, loss = 0.679606672623806\n",
            "iter 654, loss = 0.6785891285441817\n",
            "iter 655, loss = 0.6775813584336636\n",
            "iter 656, loss = 0.6765832621965312\n",
            "iter 657, loss = 0.6755947407633046\n",
            "iter 658, loss = 0.6746156960802245\n",
            "iter 659, loss = 0.6736460310988365\n",
            "iter 660, loss = 0.6726856497656847\n",
            "iter 661, loss = 0.6717344570121102\n",
            "iter 662, loss = 0.6707923587441542\n",
            "iter 663, loss = 0.6698592618325637\n",
            "iter 664, loss = 0.6689350741029024\n",
            "iter 665, loss = 0.668019704325759\n",
            "iter 666, loss = 0.6671130622070599\n",
            "iter 667, loss = 0.6662150583784778\n",
            "iter 668, loss = 0.6653256043879417\n",
            "iter 669, loss = 0.6644446126902417\n",
            "iter 670, loss = 0.6635719966377311\n",
            "iter 671, loss = 0.6627076704711249\n",
            "iter 672, loss = 0.6618515493103899\n",
            "iter 673, loss = 0.6610035491457318\n",
            "iter 674, loss = 0.6601635868286715\n",
            "iter 675, loss = 0.6593315800632147\n",
            "iter 676, loss = 0.6585074473971113\n",
            "iter 677, loss = 0.6576911082132049\n",
            "iter 678, loss = 0.6568824827208708\n",
            "iter 679, loss = 0.6560814919475415\n",
            "iter 680, loss = 0.6552880577303195\n",
            "iter 681, loss = 0.6545021027076756\n",
            "iter 682, loss = 0.6537235503112331\n",
            "iter 683, loss = 0.6529523247576354\n",
            "iter 684, loss = 0.6521883510404978\n",
            "iter 685, loss = 0.6514315549224405\n",
            "iter 686, loss = 0.6506818629272053\n",
            "iter 687, loss = 0.6499392023318511\n",
            "iter 688, loss = 0.6492035011590301\n",
            "iter 689, loss = 0.6484746881693446\n",
            "iter 690, loss = 0.6477526928537799\n",
            "iter 691, loss = 0.6470374454262159\n",
            "iter 692, loss = 0.646328876816016\n",
            "iter 693, loss = 0.6456269186606907\n",
            "iter 694, loss = 0.6449315032986374\n",
            "iter 695, loss = 0.6442425637619547\n",
            "iter 696, loss = 0.6435600337693292\n",
            "iter 697, loss = 0.6428838477189972\n",
            "iter 698, loss = 0.6422139406817764\n",
            "iter 699, loss = 0.6415502483941701\n",
            "iter 700, loss = 0.6408927072515429\n",
            "iter 701, loss = 0.6402412543013645\n",
            "iter 702, loss = 0.6395958272365249\n",
            "iter 703, loss = 0.6389563643887158\n",
            "iter 704, loss = 0.6383228047218825\n",
            "iter 705, loss = 0.6376950878257398\n",
            "iter 706, loss = 0.6370731539093579\n",
            "iter 707, loss = 0.636456943794811\n",
            "iter 708, loss = 0.6358463989108927\n",
            "iter 709, loss = 0.6352414612868958\n",
            "iter 710, loss = 0.6346420735464551\n",
            "iter 711, loss = 0.6340481789014535\n",
            "iter 712, loss = 0.6334597211459914\n",
            "iter 713, loss = 0.6328766446504167\n",
            "iter 714, loss = 0.6322988943554175\n",
            "iter 715, loss = 0.6317264157661738\n",
            "iter 716, loss = 0.63115915494657\n",
            "iter 717, loss = 0.6305970585134668\n",
            "iter 718, loss = 0.6300400736310318\n",
            "iter 719, loss = 0.6294881480051271\n",
            "iter 720, loss = 0.6289412298777572\n",
            "iter 721, loss = 0.6283992680215699\n",
            "iter 722, loss = 0.6278622117344177\n",
            "iter 723, loss = 0.6273300108339716\n",
            "iter 724, loss = 0.6268026156523924\n",
            "iter 725, loss = 0.6262799770310549\n",
            "iter 726, loss = 0.6257620463153275\n",
            "iter 727, loss = 0.6252487753494052\n",
            "iter 728, loss = 0.6247401164711944\n",
            "iter 729, loss = 0.6242360225072523\n",
            "iter 730, loss = 0.623736446767775\n",
            "iter 731, loss = 0.6232413430416401\n",
            "iter 732, loss = 0.622750665591499\n",
            "iter 733, loss = 0.6222643691489189\n",
            "iter 734, loss = 0.6217824089095749\n",
            "iter 735, loss = 0.6213047405284923\n",
            "iter 736, loss = 0.6208313201153363\n",
            "iter 737, loss = 0.6203621042297512\n",
            "iter 738, loss = 0.6198970498767463\n",
            "iter 739, loss = 0.6194361145021307\n",
            "iter 740, loss = 0.618979255987992\n",
            "iter 741, loss = 0.6185264326482255\n",
            "iter 742, loss = 0.6180776032241051\n",
            "iter 743, loss = 0.6176327268799028\n",
            "iter 744, loss = 0.6171917631985505\n",
            "iter 745, loss = 0.6167546721773489\n",
            "iter 746, loss = 0.6163214142237183\n",
            "iter 747, loss = 0.6158919501509935\n",
            "iter 748, loss = 0.6154662411742626\n",
            "iter 749, loss = 0.6150442489062481\n",
            "iter 750, loss = 0.6146259353532292\n",
            "iter 751, loss = 0.6142112629110068\n",
            "iter 752, loss = 0.6138001943609112\n",
            "iter 753, loss = 0.6133926928658475\n",
            "iter 754, loss = 0.6129887219663849\n",
            "iter 755, loss = 0.6125882455768831\n",
            "iter 756, loss = 0.6121912279816616\n",
            "iter 757, loss = 0.6117976338312052\n",
            "iter 758, loss = 0.6114074281384103\n",
            "iter 759, loss = 0.6110205762748693\n",
            "iter 760, loss = 0.610637043967193\n",
            "iter 761, loss = 0.6102567972933697\n",
            "iter 762, loss = 0.6098798026791643\n",
            "iter 763, loss = 0.6095060268945515\n",
            "iter 764, loss = 0.6091354370501865\n",
            "iter 765, loss = 0.6087680005939126\n",
            "iter 766, loss = 0.6084036853073043\n",
            "iter 767, loss = 0.6080424593022449\n",
            "iter 768, loss = 0.6076842910175403\n",
            "iter 769, loss = 0.6073291492155675\n",
            "iter 770, loss = 0.6069770029789562\n",
            "iter 771, loss = 0.606627821707306\n",
            "iter 772, loss = 0.6062815751139367\n",
            "iter 773, loss = 0.605938233222671\n",
            "iter 774, loss = 0.6055977663646513\n",
            "iter 775, loss = 0.6052601451751901\n",
            "iter 776, loss = 0.6049253405906492\n",
            "iter 777, loss = 0.6045933238453542\n",
            "iter 778, loss = 0.6042640664685403\n",
            "iter 779, loss = 0.6039375402813272\n",
            "iter 780, loss = 0.6036137173937275\n",
            "iter 781, loss = 0.6032925702016839\n",
            "iter 782, loss = 0.6029740713841389\n",
            "iter 783, loss = 0.6026581939001322\n",
            "iter 784, loss = 0.6023449109859285\n",
            "iter 785, loss = 0.6020341961521767\n",
            "iter 786, loss = 0.6017260231810954\n",
            "iter 787, loss = 0.6014203661236893\n",
            "iter 788, loss = 0.6011171992969929\n",
            "iter 789, loss = 0.6008164972813433\n",
            "iter 790, loss = 0.6005182349176803\n",
            "iter 791, loss = 0.6002223873048749\n",
            "iter 792, loss = 0.5999289297970841\n",
            "iter 793, loss = 0.5996378380011346\n",
            "iter 794, loss = 0.5993490877739313\n",
            "iter 795, loss = 0.599062655219893\n",
            "iter 796, loss = 0.5987785166884163\n",
            "iter 797, loss = 0.5984966487713623\n",
            "iter 798, loss = 0.5982170283005711\n",
            "iter 799, loss = 0.5979396323454019\n",
            "iter 800, loss = 0.5976644382102972\n",
            "iter 801, loss = 0.5973914234323724\n",
            "iter 802, loss = 0.5971205657790309\n",
            "iter 803, loss = 0.5968518432456027\n",
            "iter 804, loss = 0.5965852340530069\n",
            "iter 805, loss = 0.5963207166454401\n",
            "iter 806, loss = 0.596058269688086\n",
            "iter 807, loss = 0.5957978720648499\n",
            "iter 808, loss = 0.5955395028761169\n",
            "iter 809, loss = 0.5952831414365316\n",
            "iter 810, loss = 0.5950287672728014\n",
            "iter 811, loss = 0.5947763601215231\n",
            "iter 812, loss = 0.5945258999270298\n",
            "iter 813, loss = 0.5942773668392614\n",
            "iter 814, loss = 0.5940307412116574\n",
            "iter 815, loss = 0.5937860035990691\n",
            "iter 816, loss = 0.5935431347556954\n",
            "iter 817, loss = 0.5933021156330394\n",
            "iter 818, loss = 0.5930629273778837\n",
            "iter 819, loss = 0.5928255513302902\n",
            "iter 820, loss = 0.5925899690216176\n",
            "iter 821, loss = 0.5923561621725596\n",
            "iter 822, loss = 0.592124112691205\n",
            "iter 823, loss = 0.591893802671115\n",
            "iter 824, loss = 0.5916652143894222\n",
            "iter 825, loss = 0.5914383303049486\n",
            "iter 826, loss = 0.5912131330563427\n",
            "iter 827, loss = 0.5909896054602355\n",
            "iter 828, loss = 0.5907677305094161\n",
            "iter 829, loss = 0.5905474913710251\n",
            "iter 830, loss = 0.5903288713847679\n",
            "iter 831, loss = 0.590111854061144\n",
            "iter 832, loss = 0.5898964230796973\n",
            "iter 833, loss = 0.5896825622872821\n",
            "iter 834, loss = 0.5894702556963481\n",
            "iter 835, loss = 0.5892594874832422\n",
            "iter 836, loss = 0.5890502419865286\n",
            "iter 837, loss = 0.5888425037053248\n",
            "iter 838, loss = 0.5886362572976568\n",
            "iter 839, loss = 0.5884314875788286\n",
            "iter 840, loss = 0.5882281795198102\n",
            "iter 841, loss = 0.5880263182456416\n",
            "iter 842, loss = 0.5878258890338527\n",
            "iter 843, loss = 0.587626877312901\n",
            "iter 844, loss = 0.5874292686606224\n",
            "iter 845, loss = 0.5872330488027019\n",
            "iter 846, loss = 0.5870382036111552\n",
            "iter 847, loss = 0.5868447191028306\n",
            "iter 848, loss = 0.586652581437923\n",
            "iter 849, loss = 0.5864617769185038\n",
            "iter 850, loss = 0.5862722919870677\n",
            "iter 851, loss = 0.5860841132250915\n",
            "iter 852, loss = 0.5858972273516099\n",
            "iter 853, loss = 0.5857116212218049\n",
            "iter 854, loss = 0.5855272818256106\n",
            "iter 855, loss = 0.5853441962863302\n",
            "iter 856, loss = 0.5851623518592699\n",
            "iter 857, loss = 0.5849817359303847\n",
            "iter 858, loss = 0.5848023360149399\n",
            "iter 859, loss = 0.5846241397561838\n",
            "iter 860, loss = 0.5844471349240368\n",
            "iter 861, loss = 0.5842713094137922\n",
            "iter 862, loss = 0.5840966512448307\n",
            "iter 863, loss = 0.583923148559348\n",
            "iter 864, loss = 0.5837507896210965\n",
            "iter 865, loss = 0.5835795628141376\n",
            "iter 866, loss = 0.5834094566416095\n",
            "iter 867, loss = 0.5832404597245056\n",
            "iter 868, loss = 0.5830725608004661\n",
            "iter 869, loss = 0.5829057487225827\n",
            "iter 870, loss = 0.5827400124582146\n",
            "iter 871, loss = 0.5825753410878161\n",
            "iter 872, loss = 0.5824117238037783\n",
            "iter 873, loss = 0.582249149909281\n",
            "iter 874, loss = 0.5820876088171567\n",
            "iter 875, loss = 0.5819270900487663\n",
            "iter 876, loss = 0.5817675832328865\n",
            "iter 877, loss = 0.5816090781046082\n",
            "iter 878, loss = 0.5814515645042472\n",
            "iter 879, loss = 0.581295032376265\n",
            "iter 880, loss = 0.5811394717682004\n",
            "iter 881, loss = 0.5809848728296134\n",
            "iter 882, loss = 0.5808312258110391\n",
            "iter 883, loss = 0.5806785210629525\n",
            "iter 884, loss = 0.5805267490347432\n",
            "iter 885, loss = 0.5803759002737022\n",
            "iter 886, loss = 0.5802259654240172\n",
            "iter 887, loss = 0.5800769352257799\n",
            "iter 888, loss = 0.5799288005140023\n",
            "iter 889, loss = 0.5797815522176438\n",
            "iter 890, loss = 0.5796351813586474\n",
            "iter 891, loss = 0.5794896790509875\n",
            "iter 892, loss = 0.5793450364997244\n",
            "iter 893, loss = 0.5792012450000726\n",
            "iter 894, loss = 0.5790582959364754\n",
            "iter 895, loss = 0.5789161807816895\n",
            "iter 896, loss = 0.5787748910958809\n",
            "iter 897, loss = 0.5786344185257275\n",
            "iter 898, loss = 0.5784947548035326\n",
            "iter 899, loss = 0.5783558917463473\n",
            "iter 900, loss = 0.5782178212551012\n",
            "iter 901, loss = 0.5780805353137425\n",
            "iter 902, loss = 0.5779440259883865\n",
            "iter 903, loss = 0.577808285426474\n",
            "iter 904, loss = 0.577673305855937\n",
            "iter 905, loss = 0.577539079584373\n",
            "iter 906, loss = 0.5774055989982291\n",
            "iter 907, loss = 0.5772728565619927\n",
            "iter 908, loss = 0.5771408448173921\n",
            "iter 909, loss = 0.5770095563826044\n",
            "iter 910, loss = 0.5768789839514709\n",
            "iter 911, loss = 0.5767491202927227\n",
            "iter 912, loss = 0.5766199582492119\n",
            "iter 913, loss = 0.5764914907371518\n",
            "iter 914, loss = 0.5763637107453647\n",
            "iter 915, loss = 0.5762366113345384\n",
            "iter 916, loss = 0.5761101856364874\n",
            "iter 917, loss = 0.5759844268534261\n",
            "iter 918, loss = 0.5758593282572447\n",
            "iter 919, loss = 0.5757348831887965\n",
            "iter 920, loss = 0.5756110850571902\n",
            "iter 921, loss = 0.5754879273390897\n",
            "iter 922, loss = 0.5753654035780223\n",
            "iter 923, loss = 0.575243507383693\n",
            "iter 924, loss = 0.5751222324313047\n",
            "iter 925, loss = 0.575001572460889\n",
            "iter 926, loss = 0.5748815212766396\n",
            "iter 927, loss = 0.5747620727462549\n",
            "iter 928, loss = 0.5746432208002877\n",
            "iter 929, loss = 0.5745249594315\n",
            "iter 930, loss = 0.5744072826942248\n",
            "iter 931, loss = 0.5742901847037362\n",
            "iter 932, loss = 0.5741736596356233\n",
            "iter 933, loss = 0.5740577017251725\n",
            "iter 934, loss = 0.573942305266755\n",
            "iter 935, loss = 0.5738274646132218\n",
            "iter 936, loss = 0.5737131741753029\n",
            "iter 937, loss = 0.5735994284210149\n",
            "iter 938, loss = 0.573486221875073\n",
            "iter 939, loss = 0.5733735491183106\n",
            "iter 940, loss = 0.5732614047871027\n",
            "iter 941, loss = 0.5731497835727974\n",
            "iter 942, loss = 0.573038680221152\n",
            "iter 943, loss = 0.5729280895317747\n",
            "iter 944, loss = 0.5728180063575734\n",
            "iter 945, loss = 0.5727084256042082\n",
            "iter 946, loss = 0.5725993422295516\n",
            "iter 947, loss = 0.572490751243152\n",
            "iter 948, loss = 0.5723826477057053\n",
            "iter 949, loss = 0.572275026728529\n",
            "iter 950, loss = 0.5721678834730445\n",
            "iter 951, loss = 0.5720612131502621\n",
            "iter 952, loss = 0.5719550110202741\n",
            "iter 953, loss = 0.5718492723917503\n",
            "iter 954, loss = 0.5717439926214403\n",
            "iter 955, loss = 0.5716391671136809\n",
            "iter 956, loss = 0.5715347913199079\n",
            "iter 957, loss = 0.5714308607381733\n",
            "iter 958, loss = 0.5713273709126674\n",
            "iter 959, loss = 0.5712243174332453\n",
            "iter 960, loss = 0.5711216959349597\n",
            "iter 961, loss = 0.5710195020975961\n",
            "iter 962, loss = 0.5709177316452155\n",
            "iter 963, loss = 0.5708163803457003\n",
            "iter 964, loss = 0.5707154440103038\n",
            "iter 965, loss = 0.5706149184932067\n",
            "iter 966, loss = 0.5705147996910768\n",
            "iter 967, loss = 0.5704150835426328\n",
            "iter 968, loss = 0.570315766028214\n",
            "iter 969, loss = 0.5702168431693531\n",
            "iter 970, loss = 0.5701183110283539\n",
            "iter 971, loss = 0.570020165707873\n",
            "iter 972, loss = 0.5699224033505067\n",
            "iter 973, loss = 0.5698250201383812\n",
            "iter 974, loss = 0.5697280122927472\n",
            "iter 975, loss = 0.5696313760735789\n",
            "iter 976, loss = 0.569535107779177\n",
            "iter 977, loss = 0.5694392037457754\n",
            "iter 978, loss = 0.569343660347153\n",
            "iter 979, loss = 0.5692484739942483\n",
            "iter 980, loss = 0.5691536411347778\n",
            "iter 981, loss = 0.5690591582528606\n",
            "iter 982, loss = 0.5689650218686434\n",
            "iter 983, loss = 0.5688712285379321\n",
            "iter 984, loss = 0.5687777748518261\n",
            "iter 985, loss = 0.5686846574363561\n",
            "iter 986, loss = 0.5685918729521264\n",
            "iter 987, loss = 0.56849941809396\n",
            "iter 988, loss = 0.5684072895905478\n",
            "iter 989, loss = 0.5683154842041023\n",
            "iter 990, loss = 0.5682239987300122\n",
            "iter 991, loss = 0.5681328299965038\n",
            "iter 992, loss = 0.5680419748643034\n",
            "iter 993, loss = 0.5679514302263049\n",
            "iter 994, loss = 0.5678611930072388\n",
            "iter 995, loss = 0.567771260163347\n",
            "iter 996, loss = 0.5676816286820592\n",
            "iter 997, loss = 0.5675922955816729\n",
            "iter 998, loss = 0.5675032579110375\n",
            "iter 999, loss = 0.5674145127492403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "0LiDfWkFUr_b",
        "outputId": "d440a6c0-64e2-414e-fee3-7aba0bdd7877"
      },
      "source": [
        "plot_curve(range(num_iterations), losses)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENonKGM0ZxM8"
      },
      "source": [
        "### Multilayer Perceptron\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH5qe_hiacGf"
      },
      "source": [
        "# Lets make some data\n",
        "num_classes = 2 \n",
        "num_units = 50\n",
        "num_out = 1\n",
        "\n",
        "X_train = np.random.random((num_train_examples, num_features))\n",
        "y_train = np.random.random((num_train_examples, 1)) \n",
        "# y_train = np.random.randint(0, num_classes, num_train_examples)\n",
        "# y_train = y_train.reshape(y_train.shape[0], 1)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8r3jFilCzqS"
      },
      "source": [
        "# Initialize weights\n",
        "\n",
        "Wh = np.random.random((num_features, num_units)) * (2/num_features)\n",
        "Wo = np.random.random((num_units, num_out)) * (2/num_units)\n",
        "\n",
        "bh = np.ones((1, num_units)) * 0.1\n",
        "bo = np.ones((1, num_out)) * 0.1"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihaR_ZzqU-iW"
      },
      "source": [
        "# ReLU function and its gradient.\n",
        "\n",
        "def relu(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "def grad_relu(z):\n",
        "  z[z > 0] = 1 \n",
        "  z[z < 0] = 0\n",
        "  return z\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2dZgx7saZdS"
      },
      "source": [
        "\"\"\"\n",
        "1-layer Neural Network with ReLU activations.\n",
        "h = relu(Wh.x + bh) \n",
        "y_hat = relu(Wo.h + bo)\n",
        "\"\"\"\n",
        "def feed_forward_nn(X, Wh, Wo, bh, bo):\n",
        "\n",
        "  zh = np.dot(X, Wh) + bh\n",
        "  # print(\"Shape zh\", zh.shape)\n",
        "\n",
        "  h = relu(zh)\n",
        "  # print(\"Shape h\", h.shape)\n",
        "\n",
        "  zo = np.dot(h, Wo) + bo\n",
        "  # print(\"Shape zo\", zo.shape)\n",
        "\n",
        "  y_hat = relu(zo)\n",
        "  # print(\"Shape y_hat\", y_hat.shape)\n",
        "\n",
        "  return y_hat, zo, h, zh\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJuzHCyJaZlT"
      },
      "source": [
        "y_hat = feed_forward_nn(X_train, Wh, Wo, bh, bo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqhWKj5eaZr_"
      },
      "source": [
        "\"\"\"\n",
        "Derivatives of the above network: \n",
        "\n",
        "L = 1/2 * (y - y_hat)**2\n",
        "\n",
        "Eo = (y - relu(h.wo + bo)) = (y - y_hat) * grad_relu(h.wo + bo)\n",
        "Eh = (y - y_hat) * grad_relu(h.wo + bo) * wo * grad_relu(x.wh + bh) = Eo * wo * grad_relu(x.wh + bh)\n",
        "\n",
        "dWo = Eo * h \n",
        "dWh = Eh * x \n",
        "\n",
        "Explanation:\n",
        "\n",
        "Initially, L' = (y - y_hat).\n",
        "Expanding y_hat and taking y_hat' = grad_relu(h.wo + bo) * h. \n",
        "Since in grad_relu, we have Wh variable, the derivative of (h.wo + bo) = wo. \n",
        "Expanding h and taking h' = grad_relu(x.wh + bh) * x \n",
        "\n",
        "Chain Rule baby! \n",
        "\n",
        "Wo = Wo - lr * dWo \n",
        "Wh = Wh - lr * dWh\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def backpropagate(y_hat, X, y, Wh, Wo, bh, bo, zo, h, zh, lr):\n",
        "  N = len(y_hat)\n",
        "\n",
        "  error = (y_hat - y)\n",
        "  relu_zo = grad_relu(zo)\n",
        "  Eo = error * relu_zo\n",
        "  \n",
        "  sig_zh = grad_relu(zh)\n",
        "  Eo_wo = Eo * Wo.T\n",
        "  Eh = Eo_wo * sig_zh\n",
        "  \n",
        "  dWo = np.dot(h.T, Eo)\n",
        "  dWh = np.dot(X.T, Eh) \n",
        "\n",
        "  dbo = Eo \n",
        "  dbh = Eo * 1 * sig_zh * 1  # Since zo' wrt bo = 1. \n",
        "  \n",
        "  # So it took me a lot of trials before I got the matrix multiplications right.\n",
        "  # I worked out each variable shape and had to math it out to get it right.  \n",
        "\n",
        "  dWo = dWo/N\n",
        "  dWh = dWh/N\n",
        "  dbo = dbo/N\n",
        "  dWh = dWh/N\n",
        "\n",
        "  Wo = Wo - lr * dWo \n",
        "  Wh = Wh - lr * dWh\n",
        "\n",
        "  bo = bo - lr * dbo\n",
        "  bh = bh - lr * dbh \n",
        "\n",
        "  return Wh, Wo, bh, bo \n",
        "  "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIqQxKuWneP2",
        "outputId": "e91d9dc0-e130-41a8-b416-9a34e8bf5ffa"
      },
      "source": [
        "lr = 0.0001\n",
        "\n",
        "# Initialize weights\n",
        "Wh = np.random.random((num_features, num_units)) * np.sqrt(2/num_features)\n",
        "Wo = np.random.random((num_units, num_out)) * np.sqrt(2/num_units)\n",
        "bh = np.ones((1, num_units)) * 0.1\n",
        "bo = np.ones((1, num_out)) * 0.1\n",
        "\n",
        "best_loss = float(\"inf\")\n",
        "losses = []\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  y_hat, zo, h, zh = feed_forward_nn(X_train, Wh, Wo, bh, bo)\n",
        "  loss = l2_loss(y_train, y_hat)\n",
        "  Wh, Wo, bh, bo = backpropagate(y_hat, X_train, y_train, Wh, Wo, bh, bo, zo, h, zh, lr)\n",
        "\n",
        "  losses.append(loss)\n",
        "  if loss < best_loss:\n",
        "    best_loss = loss\n",
        "  print(\"iter %s, loss = %s\" % (i, loss))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss = 35.171911682919465\n",
            "iter 1, loss = 34.11864388608385\n",
            "iter 2, loss = 33.09838104278462\n",
            "iter 3, loss = 32.11002673401755\n",
            "iter 4, loss = 31.15252370909509\n",
            "iter 5, loss = 30.224852363581054\n",
            "iter 6, loss = 29.326029281969458\n",
            "iter 7, loss = 28.455105842090692\n",
            "iter 8, loss = 27.61116687838206\n",
            "iter 9, loss = 26.7933294013044\n",
            "iter 10, loss = 26.00074137032355\n",
            "iter 11, loss = 25.2325805180049\n",
            "iter 12, loss = 24.488053222891782\n",
            "iter 13, loss = 23.766393428954057\n",
            "iter 14, loss = 23.066861609503064\n",
            "iter 15, loss = 22.38874377357252\n",
            "iter 16, loss = 21.73135051286327\n",
            "iter 17, loss = 21.094016087442533\n",
            "iter 18, loss = 20.476097548476414\n",
            "iter 19, loss = 19.87697389635764\n",
            "iter 20, loss = 19.296045272669563\n",
            "iter 21, loss = 18.732732184502247\n",
            "iter 22, loss = 18.186474759707313\n",
            "iter 23, loss = 17.65673203174565\n",
            "iter 24, loss = 17.142981252845733\n",
            "iter 25, loss = 16.644717234250752\n",
            "iter 26, loss = 16.16145171239046\n",
            "iter 27, loss = 15.69271273986771\n",
            "iter 28, loss = 15.238044100201682\n",
            "iter 29, loss = 14.79700474531884\n",
            "iter 30, loss = 14.369168254829164\n",
            "iter 31, loss = 13.954122316169679\n",
            "iter 32, loss = 13.55146822473926\n",
            "iter 33, loss = 13.160820403188755\n",
            "iter 34, loss = 12.781805939068438\n",
            "iter 35, loss = 12.41406414007102\n",
            "iter 36, loss = 12.057246106142692\n",
            "iter 37, loss = 11.71101431776752\n",
            "iter 38, loss = 11.375042239761447\n",
            "iter 39, loss = 11.049013939941874\n",
            "iter 40, loss = 10.732623722066888\n",
            "iter 41, loss = 10.425575772465\n",
            "iter 42, loss = 10.127583819801838\n",
            "iter 43, loss = 9.838370807454462\n",
            "iter 44, loss = 9.557668577987188\n",
            "iter 45, loss = 9.285217569244798\n",
            "iter 46, loss = 9.020766521600091\n",
            "iter 47, loss = 8.764072195912659\n",
            "iter 48, loss = 8.514899101774965\n",
            "iter 49, loss = 8.273019235639916\n",
            "iter 50, loss = 8.038211828441506\n",
            "iter 51, loss = 7.810263102336664\n",
            "iter 52, loss = 7.588966036212183\n",
            "iter 53, loss = 7.374120139615727\n",
            "iter 54, loss = 7.165531234784228\n",
            "iter 55, loss = 6.9630112464567375\n",
            "iter 56, loss = 6.766377999171873\n",
            "iter 57, loss = 6.575455021762509\n",
            "iter 58, loss = 6.390071358772281\n",
            "iter 59, loss = 6.210061388529908\n",
            "iter 60, loss = 6.035264647628198\n",
            "iter 61, loss = 5.865525661564991\n",
            "iter 62, loss = 5.700693781313316\n",
            "iter 63, loss = 5.540623025597434\n",
            "iter 64, loss = 5.385171928660598\n",
            "iter 65, loss = 5.23420339331903\n",
            "iter 66, loss = 5.087584549104887\n",
            "iter 67, loss = 4.945186615308987\n",
            "iter 68, loss = 4.806884768741583\n",
            "iter 69, loss = 4.672558016036845\n",
            "iter 70, loss = 4.542089070333544\n",
            "iter 71, loss = 4.415364232171187\n",
            "iter 72, loss = 4.292273274447164\n",
            "iter 73, loss = 4.172709331286578\n",
            "iter 74, loss = 4.056568790682271\n",
            "iter 75, loss = 3.9437511907681415\n",
            "iter 76, loss = 3.83415911959421\n",
            "iter 77, loss = 3.7276981182770013\n",
            "iter 78, loss = 3.6242765874037337\n",
            "iter 79, loss = 3.5238056965735196\n",
            "iter 80, loss = 3.4261992969632646\n",
            "iter 81, loss = 3.3313738368103025\n",
            "iter 82, loss = 3.23924827970793\n",
            "iter 83, loss = 3.149744025613978\n",
            "iter 84, loss = 3.062784834476369\n",
            "iter 85, loss = 2.9782967523832555\n",
            "iter 86, loss = 2.8962080401488435\n",
            "iter 87, loss = 2.81644910424935\n",
            "iter 88, loss = 2.738952430026791\n",
            "iter 89, loss = 2.663652517081357\n",
            "iter 90, loss = 2.5904858167761264\n",
            "iter 91, loss = 2.519390671780698\n",
            "iter 92, loss = 2.4503072575830656\n",
            "iter 93, loss = 2.38317752590168\n",
            "iter 94, loss = 2.3179451499321586\n",
            "iter 95, loss = 2.2545554713655207\n",
            "iter 96, loss = 2.1929554491171594\n",
            "iter 97, loss = 2.133093609707987\n",
            "iter 98, loss = 2.0749199992413243\n",
            "iter 99, loss = 2.0183861369211744\n",
            "iter 100, loss = 1.9634449700595038\n",
            "iter 101, loss = 1.910050830522037\n",
            "iter 102, loss = 1.8581593925639157\n",
            "iter 103, loss = 1.8077276320083195\n",
            "iter 104, loss = 1.7587137867228373\n",
            "iter 105, loss = 1.7110773183499988\n",
            "iter 106, loss = 1.664778875249939\n",
            "iter 107, loss = 1.61978025661467\n",
            "iter 108, loss = 1.5760443777148667\n",
            "iter 109, loss = 1.5335352362414825\n",
            "iter 110, loss = 1.492217879705823\n",
            "iter 111, loss = 1.452058373863009\n",
            "iter 112, loss = 1.4130237721249825\n",
            "iter 113, loss = 1.3750820859304123\n",
            "iter 114, loss = 1.338202256039984\n",
            "iter 115, loss = 1.3023541247266812\n",
            "iter 116, loss = 1.267508408831708\n",
            "iter 117, loss = 1.2336366736577293\n",
            "iter 118, loss = 1.2007113076721\n",
            "iter 119, loss = 1.1687054979936784\n",
            "iter 120, loss = 1.1375932066377532\n",
            "iter 121, loss = 1.107349147494479\n",
            "iter 122, loss = 1.0779487640170606\n",
            "iter 123, loss = 1.0493682075967512\n",
            "iter 124, loss = 1.021584316602502\n",
            "iter 125, loss = 0.9945745960638659\n",
            "iter 126, loss = 0.9683171979764857\n",
            "iter 127, loss = 0.9427909022101926\n",
            "iter 128, loss = 0.9179750980004283\n",
            "iter 129, loss = 0.8938497660043476\n",
            "iter 130, loss = 0.8703954609035949\n",
            "iter 131, loss = 0.847593294536345\n",
            "iter 132, loss = 0.8254249195417955\n",
            "iter 133, loss = 0.8038725135008494\n",
            "iter 134, loss = 0.7829187635572792\n",
            "iter 135, loss = 0.7625468515041863\n",
            "iter 136, loss = 0.7427404393210706\n",
            "iter 137, loss = 0.7234836551473187\n",
            "iter 138, loss = 0.7047610796783876\n",
            "iter 139, loss = 0.6865577329714165\n",
            "iter 140, loss = 0.668859061647431\n",
            "iter 141, loss = 0.6516509264777384\n",
            "iter 142, loss = 0.6349195903425084\n",
            "iter 143, loss = 0.6186517065499386\n",
            "iter 144, loss = 0.6028343075047734\n",
            "iter 145, loss = 0.587454793715321\n",
            "iter 146, loss = 0.5725009231284637\n",
            "iter 147, loss = 0.5579608007824962\n",
            "iter 148, loss = 0.5438228687679627\n",
            "iter 149, loss = 0.5300758964869752\n",
            "iter 150, loss = 0.5167089712018126\n",
            "iter 151, loss = 0.5037114888638862\n",
            "iter 152, loss = 0.4910731452144555\n",
            "iter 153, loss = 0.47878392714874785\n",
            "iter 154, loss = 0.46683410433540734\n",
            "iter 155, loss = 0.4552142210834562\n",
            "iter 156, loss = 0.4439150884492022\n",
            "iter 157, loss = 0.4329277765757668\n",
            "iter 158, loss = 0.42224360725814386\n",
            "iter 159, loss = 0.41185414672692205\n",
            "iter 160, loss = 0.4017511986440258\n",
            "iter 161, loss = 0.39192679730403546\n",
            "iter 162, loss = 0.3823732010348573\n",
            "iter 163, loss = 0.3730828857917047\n",
            "iter 164, loss = 0.36404853893854794\n",
            "iter 165, loss = 0.35526305321136914\n",
            "iter 166, loss = 0.3467195208577424\n",
            "iter 167, loss = 0.33841122794742534\n",
            "iter 168, loss = 0.3303316488488196\n",
            "iter 169, loss = 0.32247444086631644\n",
            "iter 170, loss = 0.31483343903369987\n",
            "iter 171, loss = 0.30740265105892994\n",
            "iter 172, loss = 0.3001762524157761\n",
            "iter 173, loss = 0.2931485815779076\n",
            "iter 174, loss = 0.2863141353911901\n",
            "iter 175, loss = 0.27966756458006414\n",
            "iter 176, loss = 0.2732036693840109\n",
            "iter 177, loss = 0.2669173953202347\n",
            "iter 178, loss = 0.26080382906881033\n",
            "iter 179, loss = 0.25485819447665975\n",
            "iter 180, loss = 0.24907584867683158\n",
            "iter 181, loss = 0.24345227831967078\n",
            "iter 182, loss = 0.23798309591256467\n",
            "iter 183, loss = 0.23266403626505697\n",
            "iter 184, loss = 0.22749095303621775\n",
            "iter 185, loss = 0.22245981538125287\n",
            "iter 186, loss = 0.21756670469442926\n",
            "iter 187, loss = 0.21280781144547897\n",
            "iter 188, loss = 0.20817943210673498\n",
            "iter 189, loss = 0.2036779661683313\n",
            "iter 190, loss = 0.1992999132388845\n",
            "iter 191, loss = 0.1950418702291483\n",
            "iter 192, loss = 0.19090052861621276\n",
            "iter 193, loss = 0.1868726717858893\n",
            "iter 194, loss = 0.18295517245099702\n",
            "iter 195, loss = 0.17914499014333202\n",
            "iter 196, loss = 0.17543916877717064\n",
            "iter 197, loss = 0.1718348342822205\n",
            "iter 198, loss = 0.1683291923039963\n",
            "iter 199, loss = 0.16491952596965823\n",
            "iter 200, loss = 0.16160319371741\n",
            "iter 201, loss = 0.1583776271876089\n",
            "iter 202, loss = 0.15524032917379815\n",
            "iter 203, loss = 0.15218887163192177\n",
            "iter 204, loss = 0.14922089374603806\n",
            "iter 205, loss = 0.1463341000488944\n",
            "iter 206, loss = 0.14352625859577664\n",
            "iter 207, loss = 0.14079519919009348\n",
            "iter 208, loss = 0.13813881165920167\n",
            "iter 209, loss = 0.13555504417902128\n",
            "iter 210, loss = 0.1330419016460354\n",
            "iter 211, loss = 0.1305974440953075\n",
            "iter 212, loss = 0.12821978516319296\n",
            "iter 213, loss = 0.12590709059345775\n",
            "iter 214, loss = 0.12365757678555696\n",
            "iter 215, loss = 0.12146950938386204\n",
            "iter 216, loss = 0.1193412019066609\n",
            "iter 217, loss = 0.11727101441379066\n",
            "iter 218, loss = 0.11525735221179471\n",
            "iter 219, loss = 0.11329866459553029\n",
            "iter 220, loss = 0.11139344362518219\n",
            "iter 221, loss = 0.1095402229376705\n",
            "iter 222, loss = 0.10773757659146913\n",
            "iter 223, loss = 0.10598411794388016\n",
            "iter 224, loss = 0.10427849855983844\n",
            "iter 225, loss = 0.10261940715134608\n",
            "iter 226, loss = 0.101005568546664\n",
            "iter 227, loss = 0.09943574268841254\n",
            "iter 228, loss = 0.09790872365975849\n",
            "iter 229, loss = 0.09642333873788834\n",
            "iter 230, loss = 0.094978447473993\n",
            "iter 231, loss = 0.09357294079900941\n",
            "iter 232, loss = 0.09220574015438816\n",
            "iter 233, loss = 0.09087579664717667\n",
            "iter 234, loss = 0.08958209022872762\n",
            "iter 235, loss = 0.0883236288963633\n",
            "iter 236, loss = 0.08709944791734531\n",
            "iter 237, loss = 0.08590860907451747\n",
            "iter 238, loss = 0.08475019993300902\n",
            "iter 239, loss = 0.08362333312740207\n",
            "iter 240, loss = 0.0825271456687847\n",
            "iter 241, loss = 0.08146079827112766\n",
            "iter 242, loss = 0.08042347469643948\n",
            "iter 243, loss = 0.07941438111816888\n",
            "iter 244, loss = 0.07843274550234061\n",
            "iter 245, loss = 0.07747781700592422\n",
            "iter 246, loss = 0.07654886539194997\n",
            "iter 247, loss = 0.07564518046090048\n",
            "iter 248, loss = 0.07476607149791913\n",
            "iter 249, loss = 0.07391086673539103\n",
            "iter 250, loss = 0.07307891283046343\n",
            "iter 251, loss = 0.07226957435708546\n",
            "iter 252, loss = 0.07148223331215996\n",
            "iter 253, loss = 0.07071628863540995\n",
            "iter 254, loss = 0.06997115574257545\n",
            "iter 255, loss = 0.06924626607156628\n",
            "iter 256, loss = 0.0685410666412072\n",
            "iter 257, loss = 0.06785501962222268\n",
            "iter 258, loss = 0.06718760192011793\n",
            "iter 259, loss = 0.06653830476962312\n",
            "iter 260, loss = 0.06590663334037689\n",
            "iter 261, loss = 0.06529210635353454\n",
            "iter 262, loss = 0.06469425570899523\n",
            "iter 263, loss = 0.06411262612295131\n",
            "iter 264, loss = 0.06354677477547097\n",
            "iter 265, loss = 0.062996270967834\n",
            "iter 266, loss = 0.062460695789348174\n",
            "iter 267, loss = 0.0619396417933814\n",
            "iter 268, loss = 0.06143271268235261\n",
            "iter 269, loss = 0.060939523001431156\n",
            "iter 270, loss = 0.06045969784070202\n",
            "iter 271, loss = 0.05999287254556067\n",
            "iter 272, loss = 0.05953869243510823\n",
            "iter 273, loss = 0.0590968125283241\n",
            "iter 274, loss = 0.058666897277799154\n",
            "iter 275, loss = 0.0582486203108195\n",
            "iter 276, loss = 0.05784166417759576\n",
            "iter 277, loss = 0.05744572010643913\n",
            "iter 278, loss = 0.05706048776569134\n",
            "iter 279, loss = 0.05668567503222018\n",
            "iter 280, loss = 0.05632099776629875\n",
            "iter 281, loss = 0.05596617959269027\n",
            "iter 282, loss = 0.05562095168776689\n",
            "iter 283, loss = 0.05528505257249426\n",
            "iter 284, loss = 0.054958227911119525\n",
            "iter 285, loss = 0.05464023031540409\n",
            "iter 286, loss = 0.05433081915424757\n",
            "iter 287, loss = 0.05402976036855332\n",
            "iter 288, loss = 0.0537368262911902\n",
            "iter 289, loss = 0.053451795471909286\n",
            "iter 290, loss = 0.05317445250707832\n",
            "iter 291, loss = 0.05290458787410028\n",
            "iter 292, loss = 0.05264199777038662\n",
            "iter 293, loss = 0.052386483956758594\n",
            "iter 294, loss = 0.05213785360515474\n",
            "iter 295, loss = 0.05189591915052467\n",
            "iter 296, loss = 0.05166049814679399\n",
            "iter 297, loss = 0.05143141312678722\n",
            "iter 298, loss = 0.05120849146599964\n",
            "iter 299, loss = 0.05099156525011153\n",
            "iter 300, loss = 0.05078047114614128\n",
            "iter 301, loss = 0.05057505027713704\n",
            "iter 302, loss = 0.05037514810030904\n",
            "iter 303, loss = 0.050180614288507445\n",
            "iter 304, loss = 0.04999130261495369\n",
            "iter 305, loss = 0.04980707084113519\n",
            "iter 306, loss = 0.04962778060777632\n",
            "iter 307, loss = 0.04945329732880056\n",
            "iter 308, loss = 0.0492834900882017\n",
            "iter 309, loss = 0.04911823153974337\n",
            "iter 310, loss = 0.04895739780940936\n",
            "iter 311, loss = 0.04880086840052861\n",
            "iter 312, loss = 0.04864852610150133\n",
            "iter 313, loss = 0.048500256896054486\n",
            "iter 314, loss = 0.04835594987595696\n",
            "iter 315, loss = 0.04821549715612673\n",
            "iter 316, loss = 0.04807879379206415\n",
            "iter 317, loss = 0.04794573769954724\n",
            "iter 318, loss = 0.047816229576526856\n",
            "iter 319, loss = 0.04769017282716098\n",
            "iter 320, loss = 0.04756747348792958\n",
            "iter 321, loss = 0.047448040155772454\n",
            "iter 322, loss = 0.04733178391819477\n",
            "iter 323, loss = 0.04721861828528586\n",
            "iter 324, loss = 0.04710845912359888\n",
            "iter 325, loss = 0.04700122459184019\n",
            "iter 326, loss = 0.04689683507831851\n",
            "iter 327, loss = 0.046795213140105735\n",
            "iter 328, loss = 0.04669628344386225\n",
            "iter 329, loss = 0.04659997270828105\n",
            "iter 330, loss = 0.046506209648106166\n",
            "iter 331, loss = 0.04641492491968225\n",
            "iter 332, loss = 0.04632605106799321\n",
            "iter 333, loss = 0.04623952247514904\n",
            "iter 334, loss = 0.04615527531028104\n",
            "iter 335, loss = 0.04607324748080697\n",
            "iter 336, loss = 0.04599337858502821\n",
            "iter 337, loss = 0.0459156098660228\n",
            "iter 338, loss = 0.04583988416679837\n",
            "iter 339, loss = 0.04576614588667081\n",
            "iter 340, loss = 0.04569434093883479\n",
            "iter 341, loss = 0.04562441670909356\n",
            "iter 342, loss = 0.04555632201571627\n",
            "iter 343, loss = 0.04549000707039184\n",
            "iter 344, loss = 0.04542542344024938\n",
            "iter 345, loss = 0.04536252401091587\n",
            "iter 346, loss = 0.0453012629505828\n",
            "iter 347, loss = 0.04524159567505397\n",
            "iter 348, loss = 0.04518347881374769\n",
            "iter 349, loss = 0.04512687017662721\n",
            "iter 350, loss = 0.04507172872203397\n",
            "iter 351, loss = 0.04501801452539888\n",
            "iter 352, loss = 0.0449656887488077\n",
            "iter 353, loss = 0.044914713611397095\n",
            "iter 354, loss = 0.04486505236055857\n",
            "iter 355, loss = 0.04481666924392828\n",
            "iter 356, loss = 0.044769529482141066\n",
            "iter 357, loss = 0.04472359924232806\n",
            "iter 358, loss = 0.04467884561233718\n",
            "iter 359, loss = 0.044635236575657\n",
            "iter 360, loss = 0.04459274098702467\n",
            "iter 361, loss = 0.04455132854869914\n",
            "iter 362, loss = 0.044510969787381595\n",
            "iter 363, loss = 0.04447163603176527\n",
            "iter 364, loss = 0.044433299390697525\n",
            "iter 365, loss = 0.04439593273193751\n",
            "iter 366, loss = 0.0443595096614929\n",
            "iter 367, loss = 0.044324004503520184\n",
            "iter 368, loss = 0.04428939228077288\n",
            "iter 369, loss = 0.044255648695582746\n",
            "iter 370, loss = 0.04422275011135953\n",
            "iter 371, loss = 0.04419067353459494\n",
            "iter 372, loss = 0.04415939659735722\n",
            "iter 373, loss = 0.044128897540262745\n",
            "iter 374, loss = 0.04409915519591177\n",
            "iter 375, loss = 0.04407014897277561\n",
            "iter 376, loss = 0.04404185883952285\n",
            "iter 377, loss = 0.04401426530977269\n",
            "iter 378, loss = 0.04398734942726373\n",
            "iter 379, loss = 0.043961092751426796\n",
            "iter 380, loss = 0.043935477343350945\n",
            "iter 381, loss = 0.043910485752131655\n",
            "iter 382, loss = 0.04388610100159103\n",
            "iter 383, loss = 0.04386230657735971\n",
            "iter 384, loss = 0.04383908641431065\n",
            "iter 385, loss = 0.04381642488433519\n",
            "iter 386, loss = 0.043794306784452085\n",
            "iter 387, loss = 0.0437727173252403\n",
            "iter 388, loss = 0.04375164211958692\n",
            "iter 389, loss = 0.04373106717174144\n",
            "iter 390, loss = 0.04371097886666812\n",
            "iter 391, loss = 0.04369136395968825\n",
            "iter 392, loss = 0.04367220956640455\n",
            "iter 393, loss = 0.043653503152899745\n",
            "iter 394, loss = 0.04363523252620212\n",
            "iter 395, loss = 0.043617385825010596\n",
            "iter 396, loss = 0.04359995151067237\n",
            "iter 397, loss = 0.043582918358406046\n",
            "iter 398, loss = 0.04356627544876386\n",
            "iter 399, loss = 0.04355001215932613\n",
            "iter 400, loss = 0.04353411815662193\n",
            "iter 401, loss = 0.043518583388269624\n",
            "iter 402, loss = 0.04350339807533124\n",
            "iter 403, loss = 0.04348855270487508\n",
            "iter 404, loss = 0.04347403802274059\n",
            "iter 405, loss = 0.04345984502650025\n",
            "iter 406, loss = 0.04344596495861296\n",
            "iter 407, loss = 0.04343238929976371\n",
            "iter 408, loss = 0.04341910976238451\n",
            "iter 409, loss = 0.04340611828435164\n",
            "iter 410, loss = 0.043393407022854436\n",
            "iter 411, loss = 0.043380968348430735\n",
            "iter 412, loss = 0.04336879483916487\n",
            "iter 413, loss = 0.04335687927504323\n",
            "iter 414, loss = 0.04334521463246367\n",
            "iter 415, loss = 0.04333379407889405\n",
            "iter 416, loss = 0.043322610967676305\n",
            "iter 417, loss = 0.04331165883297166\n",
            "iter 418, loss = 0.043300931384843526\n",
            "iter 419, loss = 0.04329042250447408\n",
            "iter 420, loss = 0.04328012623951095\n",
            "iter 421, loss = 0.04327003679954051\n",
            "iter 422, loss = 0.0432601485516843\n",
            "iter 423, loss = 0.043250456016315206\n",
            "iter 424, loss = 0.04324095386289013\n",
            "iter 425, loss = 0.043231636905896174\n",
            "iter 426, loss = 0.043222500100906766\n",
            "iter 427, loss = 0.04321353854074538\n",
            "iter 428, loss = 0.04320474745175339\n",
            "iter 429, loss = 0.04319612219015947\n",
            "iter 430, loss = 0.04318765823854775\n",
            "iter 431, loss = 0.04317935120242202\n",
            "iter 432, loss = 0.04317119680686334\n",
            "iter 433, loss = 0.043163190893278594\n",
            "iter 434, loss = 0.04315532941623737\n",
            "iter 435, loss = 0.04314760844039501\n",
            "iter 436, loss = 0.0431400241374991\n",
            "iter 437, loss = 0.043132572783477666\n",
            "iter 438, loss = 0.04312525075560619\n",
            "iter 439, loss = 0.043118054529751995\n",
            "iter 440, loss = 0.0431109806776933\n",
            "iter 441, loss = 0.043104025864511335\n",
            "iter 442, loss = 0.04309718684605329\n",
            "iter 443, loss = 0.04309046046646421\n",
            "iter 444, loss = 0.04308384365578614\n",
            "iter 445, loss = 0.04307733342762232\n",
            "iter 446, loss = 0.043070926876865104\n",
            "iter 447, loss = 0.043064621177485586\n",
            "iter 448, loss = 0.04305841358038316\n",
            "iter 449, loss = 0.043052301411293854\n",
            "iter 450, loss = 0.04304628206875518\n",
            "iter 451, loss = 0.04304035302212666\n",
            "iter 452, loss = 0.04303451180966384\n",
            "iter 453, loss = 0.04302875603664493\n",
            "iter 454, loss = 0.0430230833735482\n",
            "iter 455, loss = 0.04301749155427899\n",
            "iter 456, loss = 0.04301197837444492\n",
            "iter 457, loss = 0.04300654168967799\n",
            "iter 458, loss = 0.04300117941400227\n",
            "iter 459, loss = 0.04299588951824597\n",
            "iter 460, loss = 0.04299067002849671\n",
            "iter 461, loss = 0.04298551902459869\n",
            "iter 462, loss = 0.042980434638690876\n",
            "iter 463, loss = 0.04297541505378474\n",
            "iter 464, loss = 0.04297045850238075\n",
            "iter 465, loss = 0.04296556326512254\n",
            "iter 466, loss = 0.04296072766948749\n",
            "iter 467, loss = 0.042955950088513065\n",
            "iter 468, loss = 0.04295122893955768\n",
            "iter 469, loss = 0.04294656268309526\n",
            "iter 470, loss = 0.04294194982154263\n",
            "iter 471, loss = 0.0429373888981187\n",
            "iter 472, loss = 0.042932878495734676\n",
            "iter 473, loss = 0.04292841723591451\n",
            "iter 474, loss = 0.04292400377774456\n",
            "iter 475, loss = 0.042919636816851965\n",
            "iter 476, loss = 0.04291531508441064\n",
            "iter 477, loss = 0.04291103734617437\n",
            "iter 478, loss = 0.042906802401536215\n",
            "iter 479, loss = 0.04290260908261334\n",
            "iter 480, loss = 0.042898456253356934\n",
            "iter 481, loss = 0.04289434280868614\n",
            "iter 482, loss = 0.04289026767364561\n",
            "iter 483, loss = 0.04288622980258596\n",
            "iter 484, loss = 0.042882228178366434\n",
            "iter 485, loss = 0.042878261811579395\n",
            "iter 486, loss = 0.04287432973979571\n",
            "iter 487, loss = 0.04287043102683075\n",
            "iter 488, loss = 0.04286656476203038\n",
            "iter 489, loss = 0.04286273005957622\n",
            "iter 490, loss = 0.042858926057809936\n",
            "iter 491, loss = 0.04285515191857584\n",
            "iter 492, loss = 0.042851406826581284\n",
            "iter 493, loss = 0.04284768998877465\n",
            "iter 494, loss = 0.042844000633739936\n",
            "iter 495, loss = 0.04284033801110818\n",
            "iter 496, loss = 0.04283670139098448\n",
            "iter 497, loss = 0.04283309006339093\n",
            "iter 498, loss = 0.04282950333772452\n",
            "iter 499, loss = 0.04282594054222978\n",
            "iter 500, loss = 0.04282240102348585\n",
            "iter 501, loss = 0.042818884145907385\n",
            "iter 502, loss = 0.0428153892912591\n",
            "iter 503, loss = 0.04281191585818353\n",
            "iter 504, loss = 0.04280846326174151\n",
            "iter 505, loss = 0.042805030932965296\n",
            "iter 506, loss = 0.042801618318423745\n",
            "iter 507, loss = 0.04279822487979933\n",
            "iter 508, loss = 0.0427948500934767\n",
            "iter 509, loss = 0.04279149345014231\n",
            "iter 510, loss = 0.04278815445439512\n",
            "iter 511, loss = 0.042784832624367684\n",
            "iter 512, loss = 0.04278152749135769\n",
            "iter 513, loss = 0.04277823859946944\n",
            "iter 514, loss = 0.042774965505265085\n",
            "iter 515, loss = 0.04277170777742539\n",
            "iter 516, loss = 0.042768464996419626\n",
            "iter 517, loss = 0.04276523675418457\n",
            "iter 518, loss = 0.042762022653812105\n",
            "iter 519, loss = 0.04275882230924544\n",
            "iter 520, loss = 0.04275563534498345\n",
            "iter 521, loss = 0.042752461395793175\n",
            "iter 522, loss = 0.04274930010643009\n",
            "iter 523, loss = 0.042746151131365935\n",
            "iter 524, loss = 0.042743014134524096\n",
            "iter 525, loss = 0.042739888789021956\n",
            "iter 526, loss = 0.042736774776920484\n",
            "iter 527, loss = 0.04273367178898046\n",
            "iter 528, loss = 0.04273057952442542\n",
            "iter 529, loss = 0.042727497690711054\n",
            "iter 530, loss = 0.04272442600330079\n",
            "iter 531, loss = 0.0427213641854476\n",
            "iter 532, loss = 0.04271831196798163\n",
            "iter 533, loss = 0.0427152690891037\n",
            "iter 534, loss = 0.04271223529418434\n",
            "iter 535, loss = 0.042709210335568355\n",
            "iter 536, loss = 0.04270619397238467\n",
            "iter 537, loss = 0.04270318597036139\n",
            "iter 538, loss = 0.042700186101645776\n",
            "iter 539, loss = 0.04269719414462931\n",
            "iter 540, loss = 0.042694209883777354\n",
            "iter 541, loss = 0.042691233109463475\n",
            "iter 542, loss = 0.04268826361780838\n",
            "iter 543, loss = 0.04268530121052307\n",
            "iter 544, loss = 0.042682345694756436\n",
            "iter 545, loss = 0.04267939688294683\n",
            "iter 546, loss = 0.0426764545926778\n",
            "iter 547, loss = 0.04267351864653772\n",
            "iter 548, loss = 0.0426705888719832\n",
            "iter 549, loss = 0.04266766510120628\n",
            "iter 550, loss = 0.042664747171005137\n",
            "iter 551, loss = 0.04266183492265843\n",
            "iter 552, loss = 0.042658928201802976\n",
            "iter 553, loss = 0.04265602685831482\n",
            "iter 554, loss = 0.04265313074619347\n",
            "iter 555, loss = 0.04265023972344936\n",
            "iter 556, loss = 0.04264735365199427\n",
            "iter 557, loss = 0.04264447239753491\n",
            "iter 558, loss = 0.04264159582946916\n",
            "iter 559, loss = 0.04263872382078538\n",
            "iter 560, loss = 0.042635856247964246\n",
            "iter 561, loss = 0.04263299299088342\n",
            "iter 562, loss = 0.04263013393272473\n",
            "iter 563, loss = 0.042627278959883885\n",
            "iter 564, loss = 0.04262442796188268\n",
            "iter 565, loss = 0.04262158083128358\n",
            "iter 566, loss = 0.042618737463606594\n",
            "iter 567, loss = 0.04261589775724848\n",
            "iter 568, loss = 0.042613061613404064\n",
            "iter 569, loss = 0.04261022893598983\n",
            "iter 570, loss = 0.04260739963156938\n",
            "iter 571, loss = 0.042604573609281236\n",
            "iter 572, loss = 0.04260175078076825\n",
            "iter 573, loss = 0.04259893106010921\n",
            "iter 574, loss = 0.04259611436375222\n",
            "iter 575, loss = 0.04259330061044985\n",
            "iter 576, loss = 0.042590489721196126\n",
            "iter 577, loss = 0.04258768161916515\n",
            "iter 578, loss = 0.042584876229651535\n",
            "iter 579, loss = 0.04258207348001227\n",
            "iter 580, loss = 0.04257927329961033\n",
            "iter 581, loss = 0.04257647561975979\n",
            "iter 582, loss = 0.04257368037367233\n",
            "iter 583, loss = 0.04257088749640536\n",
            "iter 584, loss = 0.04256809692481142\n",
            "iter 585, loss = 0.04256530859748905\n",
            "iter 586, loss = 0.04256252245473495\n",
            "iter 587, loss = 0.042559738438497446\n",
            "iter 588, loss = 0.042556956492331294\n",
            "iter 589, loss = 0.04255417656135358\n",
            "iter 590, loss = 0.04255139859220097\n",
            "iter 591, loss = 0.04254862253298801\n",
            "iter 592, loss = 0.04254584833326668\n",
            "iter 593, loss = 0.04254307594398688\n",
            "iter 594, loss = 0.042540305317458206\n",
            "iter 595, loss = 0.04253753640731258\n",
            "iter 596, loss = 0.04253476916846801\n",
            "iter 597, loss = 0.04253200355709331\n",
            "iter 598, loss = 0.04252923953057379\n",
            "iter 599, loss = 0.04252647704747785\n",
            "iter 600, loss = 0.04252371606752447\n",
            "iter 601, loss = 0.042520956551551733\n",
            "iter 602, loss = 0.042518198461486\n",
            "iter 603, loss = 0.04251544176031209\n",
            "iter 604, loss = 0.04251268641204414\n",
            "iter 605, loss = 0.04250993238169741\n",
            "iter 606, loss = 0.0425071796352607\n",
            "iter 607, loss = 0.042504428139669655\n",
            "iter 608, loss = 0.04250167786278073\n",
            "iter 609, loss = 0.04249892877334583\n",
            "iter 610, loss = 0.04249618084098778\n",
            "iter 611, loss = 0.04249343403617628\n",
            "iter 612, loss = 0.04249068833020467\n",
            "iter 613, loss = 0.042487943695167237\n",
            "iter 614, loss = 0.04248520010393725\n",
            "iter 615, loss = 0.04248245753014537\n",
            "iter 616, loss = 0.042479715948159004\n",
            "iter 617, loss = 0.04247697533306183\n",
            "iter 618, loss = 0.042474235660634185\n",
            "iter 619, loss = 0.042471496907333865\n",
            "iter 620, loss = 0.042468759050277416\n",
            "iter 621, loss = 0.042466022067222005\n",
            "iter 622, loss = 0.04246328593654779\n",
            "iter 623, loss = 0.042460550637240704\n",
            "iter 624, loss = 0.042457816148875775\n",
            "iter 625, loss = 0.042455082451600845\n",
            "iter 626, loss = 0.042452349526120856\n",
            "iter 627, loss = 0.04244961735368232\n",
            "iter 628, loss = 0.04244688591605858\n",
            "iter 629, loss = 0.042444155195535044\n",
            "iter 630, loss = 0.04244142517489523\n",
            "iter 631, loss = 0.04243869583740689\n",
            "iter 632, loss = 0.042435967166808676\n",
            "iter 633, loss = 0.04243323914729716\n",
            "iter 634, loss = 0.04243051176351407\n",
            "iter 635, loss = 0.042427785000534124\n",
            "iter 636, loss = 0.042425058843852906\n",
            "iter 637, loss = 0.04242233327937536\n",
            "iter 638, loss = 0.04241960829340437\n",
            "iter 639, loss = 0.042416883872629776\n",
            "iter 640, loss = 0.04241416000411767\n",
            "iter 641, loss = 0.04241143667529994\n",
            "iter 642, loss = 0.042408713873964166\n",
            "iter 643, loss = 0.04240599158824376\n",
            "iter 644, loss = 0.042403269806608405\n",
            "iter 645, loss = 0.04240054851785463\n",
            "iter 646, loss = 0.04239782771109687\n",
            "iter 647, loss = 0.04239510737575856\n",
            "iter 648, loss = 0.04239238750156361\n",
            "iter 649, loss = 0.042389668078528016\n",
            "iter 650, loss = 0.042386949096951766\n",
            "iter 651, loss = 0.04238423054741099\n",
            "iter 652, loss = 0.042381512420750216\n",
            "iter 653, loss = 0.04237879470807494\n",
            "iter 654, loss = 0.042376077400744394\n",
            "iter 655, loss = 0.042373360490364415\n",
            "iter 656, loss = 0.04237064396878068\n",
            "iter 657, loss = 0.04236792782807194\n",
            "iter 658, loss = 0.042365212060543556\n",
            "iter 659, loss = 0.04236249665872123\n",
            "iter 660, loss = 0.04235978161534478\n",
            "iter 661, loss = 0.04235706692336224\n",
            "iter 662, loss = 0.04235435257592403\n",
            "iter 663, loss = 0.042351638566377275\n",
            "iter 664, loss = 0.04234892488826037\n",
            "iter 665, loss = 0.042346211535297586\n",
            "iter 666, loss = 0.0423434985013939\n",
            "iter 667, loss = 0.04234078578062996\n",
            "iter 668, loss = 0.04233807336725713\n",
            "iter 669, loss = 0.042335361255692774\n",
            "iter 670, loss = 0.04233264944051551\n",
            "iter 671, loss = 0.04232993791646083\n",
            "iter 672, loss = 0.04232722667841658\n",
            "iter 673, loss = 0.042324515721418765\n",
            "iter 674, loss = 0.04232180504064734\n",
            "iter 675, loss = 0.042319094631422256\n",
            "iter 676, loss = 0.04231638448919941\n",
            "iter 677, loss = 0.04231367460956695\n",
            "iter 678, loss = 0.04231096498824147\n",
            "iter 679, loss = 0.04230825562106444\n",
            "iter 680, loss = 0.042305546503998655\n",
            "iter 681, loss = 0.0423028376331249\n",
            "iter 682, loss = 0.04230012900463854\n",
            "iter 683, loss = 0.042297420614846336\n",
            "iter 684, loss = 0.04229471246016331\n",
            "iter 685, loss = 0.04229200453710967\n",
            "iter 686, loss = 0.04228929684230784\n",
            "iter 687, loss = 0.042286589372479595\n",
            "iter 688, loss = 0.04228388212444325\n",
            "iter 689, loss = 0.04228117509511091\n",
            "iter 690, loss = 0.042278468281485834\n",
            "iter 691, loss = 0.04227576168065987\n",
            "iter 692, loss = 0.042273055289810894\n",
            "iter 693, loss = 0.042270349106200436\n",
            "iter 694, loss = 0.04226764312717127\n",
            "iter 695, loss = 0.04226493735014508\n",
            "iter 696, loss = 0.04226223177262033\n",
            "iter 697, loss = 0.04225952639216994\n",
            "iter 698, loss = 0.042256821206439255\n",
            "iter 699, loss = 0.04225411621314397\n",
            "iter 700, loss = 0.04225141141006813\n",
            "iter 701, loss = 0.04224870679506215\n",
            "iter 702, loss = 0.04224600236604098\n",
            "iter 703, loss = 0.042243298120982195\n",
            "iter 704, loss = 0.042240594057924276\n",
            "iter 705, loss = 0.04223789017496479\n",
            "iter 706, loss = 0.04223518647025879\n",
            "iter 707, loss = 0.0422324829420171\n",
            "iter 708, loss = 0.04222977958850473\n",
            "iter 709, loss = 0.042227076408039375\n",
            "iter 710, loss = 0.04222437339898982\n",
            "iter 711, loss = 0.04222167055977454\n",
            "iter 712, loss = 0.04221896788886023\n",
            "iter 713, loss = 0.04221626538476044\n",
            "iter 714, loss = 0.04221356304603421\n",
            "iter 715, loss = 0.04221086087128479\n",
            "iter 716, loss = 0.04220815885915832\n",
            "iter 717, loss = 0.04220545700834265\n",
            "iter 718, loss = 0.04220275531756605\n",
            "iter 719, loss = 0.04220005378559612\n",
            "iter 720, loss = 0.0421973524112386\n",
            "iter 721, loss = 0.04219465119333634\n",
            "iter 722, loss = 0.0421919501307681\n",
            "iter 723, loss = 0.0421892492224476\n",
            "iter 724, loss = 0.042186548467322504\n",
            "iter 725, loss = 0.042183847864373375\n",
            "iter 726, loss = 0.04218114741261276\n",
            "iter 727, loss = 0.042178447111084254\n",
            "iter 728, loss = 0.04217574695886156\n",
            "iter 729, loss = 0.04217304695504767\n",
            "iter 730, loss = 0.042170347098773944\n",
            "iter 731, loss = 0.042167647389199314\n",
            "iter 732, loss = 0.04216494782550947\n",
            "iter 733, loss = 0.04216224840691606\n",
            "iter 734, loss = 0.04215954913265596\n",
            "iter 735, loss = 0.04215685000199048\n",
            "iter 736, loss = 0.0421541510142047\n",
            "iter 737, loss = 0.04215145216860672\n",
            "iter 738, loss = 0.04214875346452702\n",
            "iter 739, loss = 0.042146054901317766\n",
            "iter 740, loss = 0.04214335647835216\n",
            "iter 741, loss = 0.04214065819502388\n",
            "iter 742, loss = 0.04213796005074638\n",
            "iter 743, loss = 0.04213526204495238\n",
            "iter 744, loss = 0.042132564177093254\n",
            "iter 745, loss = 0.04212986644663844\n",
            "iter 746, loss = 0.042127168853075\n",
            "iter 747, loss = 0.042124471395906984\n",
            "iter 748, loss = 0.042121774074654984\n",
            "iter 749, loss = 0.042119076888855606\n",
            "iter 750, loss = 0.04211637983806103\n",
            "iter 751, loss = 0.04211368292183849\n",
            "iter 752, loss = 0.04211098613976983\n",
            "iter 753, loss = 0.0421082894914511\n",
            "iter 754, loss = 0.0421055929764921\n",
            "iter 755, loss = 0.042102896594515944\n",
            "iter 756, loss = 0.042100200345158656\n",
            "iter 757, loss = 0.04209750422806886\n",
            "iter 758, loss = 0.04209480824290727\n",
            "iter 759, loss = 0.042092112389346376\n",
            "iter 760, loss = 0.04208941666707015\n",
            "iter 761, loss = 0.042086721075773555\n",
            "iter 762, loss = 0.04208402561516233\n",
            "iter 763, loss = 0.0420813302849526\n",
            "iter 764, loss = 0.04207863508487056\n",
            "iter 765, loss = 0.042075940014652154\n",
            "iter 766, loss = 0.042073245074042816\n",
            "iter 767, loss = 0.04207055026279715\n",
            "iter 768, loss = 0.04206785558067859\n",
            "iter 769, loss = 0.04206516102745923\n",
            "iter 770, loss = 0.04206246660291947\n",
            "iter 771, loss = 0.04205977230684778\n",
            "iter 772, loss = 0.042057078139040464\n",
            "iter 773, loss = 0.04205438409930138\n",
            "iter 774, loss = 0.04205169018744173\n",
            "iter 775, loss = 0.04204899640327982\n",
            "iter 776, loss = 0.0420463027466408\n",
            "iter 777, loss = 0.04204360921735653\n",
            "iter 778, loss = 0.04204091581526525\n",
            "iter 779, loss = 0.04203822254021148\n",
            "iter 780, loss = 0.042035529392045745\n",
            "iter 781, loss = 0.04203283637062443\n",
            "iter 782, loss = 0.04203014347580958\n",
            "iter 783, loss = 0.042027450707468675\n",
            "iter 784, loss = 0.04202475806547451\n",
            "iter 785, loss = 0.04202206554970499\n",
            "iter 786, loss = 0.042019373160042964\n",
            "iter 787, loss = 0.04201668089637609\n",
            "iter 788, loss = 0.04201398875859661\n",
            "iter 789, loss = 0.0420112967466013\n",
            "iter 790, loss = 0.04200860486029121\n",
            "iter 791, loss = 0.04200591309957161\n",
            "iter 792, loss = 0.042003221464351786\n",
            "iter 793, loss = 0.04200052995454496\n",
            "iter 794, loss = 0.041997838570068086\n",
            "iter 795, loss = 0.0419951473108418\n",
            "iter 796, loss = 0.041992456176790274\n",
            "iter 797, loss = 0.041989765167841024\n",
            "iter 798, loss = 0.04198707428392492\n",
            "iter 799, loss = 0.041984383524975945\n",
            "iter 800, loss = 0.04198169289093119\n",
            "iter 801, loss = 0.041979002381730676\n",
            "iter 802, loss = 0.041976311997317294\n",
            "iter 803, loss = 0.04197362173763667\n",
            "iter 804, loss = 0.041970931602637096\n",
            "iter 805, loss = 0.041968241592269445\n",
            "iter 806, loss = 0.041965551706487\n",
            "iter 807, loss = 0.0419628619452455\n",
            "iter 808, loss = 0.04196017230850293\n",
            "iter 809, loss = 0.041957482796219504\n",
            "iter 810, loss = 0.04195479340835756\n",
            "iter 811, loss = 0.04195210414488151\n",
            "iter 812, loss = 0.04194941500575772\n",
            "iter 813, loss = 0.04194672599095446\n",
            "iter 814, loss = 0.04194403710044186\n",
            "iter 815, loss = 0.04194134833419178\n",
            "iter 816, loss = 0.041938659692177804\n",
            "iter 817, loss = 0.041935971174375135\n",
            "iter 818, loss = 0.041933282780760535\n",
            "iter 819, loss = 0.041930594511312314\n",
            "iter 820, loss = 0.041927906366010174\n",
            "iter 821, loss = 0.04192521834483527\n",
            "iter 822, loss = 0.04192253044777004\n",
            "iter 823, loss = 0.041919842674798265\n",
            "iter 824, loss = 0.04191715502590488\n",
            "iter 825, loss = 0.041914467501076066\n",
            "iter 826, loss = 0.04191178010029913\n",
            "iter 827, loss = 0.04190909282356243\n",
            "iter 828, loss = 0.04190640567085539\n",
            "iter 829, loss = 0.04190371864216845\n",
            "iter 830, loss = 0.04190103173749296\n",
            "iter 831, loss = 0.041898344956821196\n",
            "iter 832, loss = 0.041895658300146334\n",
            "iter 833, loss = 0.041892971767462345\n",
            "iter 834, loss = 0.04189028535876402\n",
            "iter 835, loss = 0.041887599074046905\n",
            "iter 836, loss = 0.04188491291330728\n",
            "iter 837, loss = 0.0418822268765421\n",
            "iter 838, loss = 0.04187954096374897\n",
            "iter 839, loss = 0.041876855174926154\n",
            "iter 840, loss = 0.0418741695100725\n",
            "iter 841, loss = 0.0418714839691874\n",
            "iter 842, loss = 0.04186879855227081\n",
            "iter 843, loss = 0.041866113259323184\n",
            "iter 844, loss = 0.04186342809034547\n",
            "iter 845, loss = 0.04186074304533907\n",
            "iter 846, loss = 0.041858058124305805\n",
            "iter 847, loss = 0.04185537332724792\n",
            "iter 848, loss = 0.04185268865416804\n",
            "iter 849, loss = 0.041850004105069144\n",
            "iter 850, loss = 0.04184731967995457\n",
            "iter 851, loss = 0.04184463537882798\n",
            "iter 852, loss = 0.0418419512016933\n",
            "iter 853, loss = 0.041839267148554786\n",
            "iter 854, loss = 0.04183658321941691\n",
            "iter 855, loss = 0.04183389941428442\n",
            "iter 856, loss = 0.04183121573316228\n",
            "iter 857, loss = 0.041828532176055644\n",
            "iter 858, loss = 0.04182584874296988\n",
            "iter 859, loss = 0.041823165433910556\n",
            "iter 860, loss = 0.041820482248883344\n",
            "iter 861, loss = 0.04181779918789411\n",
            "iter 862, loss = 0.04181511625094883\n",
            "iter 863, loss = 0.041812433438053624\n",
            "iter 864, loss = 0.04180975074921469\n",
            "iter 865, loss = 0.04180706818443834\n",
            "iter 866, loss = 0.04180438574373095\n",
            "iter 867, loss = 0.041801703427099\n",
            "iter 868, loss = 0.04179902123454899\n",
            "iter 869, loss = 0.041796339166087514\n",
            "iter 870, loss = 0.04179365722172115\n",
            "iter 871, loss = 0.04179097540145656\n",
            "iter 872, loss = 0.041788293705300385\n",
            "iter 873, loss = 0.04178561213325928\n",
            "iter 874, loss = 0.04178293068533994\n",
            "iter 875, loss = 0.041780249361549004\n",
            "iter 876, loss = 0.041777568161893125\n",
            "iter 877, loss = 0.04177488708637892\n",
            "iter 878, loss = 0.041772206135012986\n",
            "iter 879, loss = 0.0417695253078019\n",
            "iter 880, loss = 0.04176684460475213\n",
            "iter 881, loss = 0.04176416402587017\n",
            "iter 882, loss = 0.04176148357116241\n",
            "iter 883, loss = 0.04175880324063521\n",
            "iter 884, loss = 0.04175612303429482\n",
            "iter 885, loss = 0.04175344295214746\n",
            "iter 886, loss = 0.04175076299419925\n",
            "iter 887, loss = 0.04174808316045621\n",
            "iter 888, loss = 0.041745403450924305\n",
            "iter 889, loss = 0.041742723865609406\n",
            "iter 890, loss = 0.041740044404517236\n",
            "iter 891, loss = 0.04173736506765348\n",
            "iter 892, loss = 0.041734685855023684\n",
            "iter 893, loss = 0.0417320067666333\n",
            "iter 894, loss = 0.041729327802487656\n",
            "iter 895, loss = 0.04172664896259197\n",
            "iter 896, loss = 0.04172397024695134\n",
            "iter 897, loss = 0.041721291655570744\n",
            "iter 898, loss = 0.04171861318845504\n",
            "iter 899, loss = 0.04171593484560896\n",
            "iter 900, loss = 0.041713256627037086\n",
            "iter 901, loss = 0.041710578532743896\n",
            "iter 902, loss = 0.04170790056273373\n",
            "iter 903, loss = 0.04170522271701077\n",
            "iter 904, loss = 0.041702544995579086\n",
            "iter 905, loss = 0.04169986739844261\n",
            "iter 906, loss = 0.041697189925605095\n",
            "iter 907, loss = 0.04169451257707019\n",
            "iter 908, loss = 0.0416918353528414\n",
            "iter 909, loss = 0.041689158252922076\n",
            "iter 910, loss = 0.041686481277315404\n",
            "iter 911, loss = 0.04168380442602446\n",
            "iter 912, loss = 0.041681127699052134\n",
            "iter 913, loss = 0.04167845109640119\n",
            "iter 914, loss = 0.04167577461807424\n",
            "iter 915, loss = 0.041673098264073755\n",
            "iter 916, loss = 0.04167042203440202\n",
            "iter 917, loss = 0.0416677459290612\n",
            "iter 918, loss = 0.0416650699480533\n",
            "iter 919, loss = 0.04166239409138017\n",
            "iter 920, loss = 0.041659718359043516\n",
            "iter 921, loss = 0.04165704275104487\n",
            "iter 922, loss = 0.04165436726738563\n",
            "iter 923, loss = 0.04165169190806705\n",
            "iter 924, loss = 0.0416490166730902\n",
            "iter 925, loss = 0.041646341562456\n",
            "iter 926, loss = 0.04164366657616526\n",
            "iter 927, loss = 0.04164099171421859\n",
            "iter 928, loss = 0.04163831697661646\n",
            "iter 929, loss = 0.04163564236335922\n",
            "iter 930, loss = 0.041632967874446994\n",
            "iter 931, loss = 0.04163029350987983\n",
            "iter 932, loss = 0.041627619269657606\n",
            "iter 933, loss = 0.04162494515378\n",
            "iter 934, loss = 0.0416222711622466\n",
            "iter 935, loss = 0.04161959729505682\n",
            "iter 936, loss = 0.04161692355220992\n",
            "iter 937, loss = 0.041614249933705015\n",
            "iter 938, loss = 0.041611576439541074\n",
            "iter 939, loss = 0.04160890306971693\n",
            "iter 940, loss = 0.04160622982423122\n",
            "iter 941, loss = 0.041603556703082516\n",
            "iter 942, loss = 0.041600883706269166\n",
            "iter 943, loss = 0.04159821083378942\n",
            "iter 944, loss = 0.04159553808564138\n",
            "iter 945, loss = 0.04159286546182297\n",
            "iter 946, loss = 0.04159019296233202\n",
            "iter 947, loss = 0.04158752058716619\n",
            "iter 948, loss = 0.04158484833632299\n",
            "iter 949, loss = 0.041582176209799815\n",
            "iter 950, loss = 0.04157950420759391\n",
            "iter 951, loss = 0.0415768323297024\n",
            "iter 952, loss = 0.041574160576122225\n",
            "iter 953, loss = 0.04157148894685024\n",
            "iter 954, loss = 0.04156881744188314\n",
            "iter 955, loss = 0.041566146061217495\n",
            "iter 956, loss = 0.04156347480484974\n",
            "iter 957, loss = 0.041560803672776175\n",
            "iter 958, loss = 0.04155813266499296\n",
            "iter 959, loss = 0.04155546178149615\n",
            "iter 960, loss = 0.04155279102228164\n",
            "iter 961, loss = 0.04155012038734522\n",
            "iter 962, loss = 0.041547449876682546\n",
            "iter 963, loss = 0.04154477949028915\n",
            "iter 964, loss = 0.041542109228160436\n",
            "iter 965, loss = 0.04153943909029168\n",
            "iter 966, loss = 0.04153676907667806\n",
            "iter 967, loss = 0.0415340991873146\n",
            "iter 968, loss = 0.04153142942219621\n",
            "iter 969, loss = 0.041528759781317696\n",
            "iter 970, loss = 0.04152609026467374\n",
            "iter 971, loss = 0.041523420872258895\n",
            "iter 972, loss = 0.041520751604067624\n",
            "iter 973, loss = 0.04151808246009423\n",
            "iter 974, loss = 0.04151541344033296\n",
            "iter 975, loss = 0.041512744544777894\n",
            "iter 976, loss = 0.04151007577342305\n",
            "iter 977, loss = 0.04150740712626228\n",
            "iter 978, loss = 0.041504738603289376\n",
            "iter 979, loss = 0.04150207020449798\n",
            "iter 980, loss = 0.04149940192988168\n",
            "iter 981, loss = 0.0414967337794339\n",
            "iter 982, loss = 0.04149406575314799\n",
            "iter 983, loss = 0.041491397851017194\n",
            "iter 984, loss = 0.041488730073034656\n",
            "iter 985, loss = 0.041486062419193385\n",
            "iter 986, loss = 0.041483394889486346\n",
            "iter 987, loss = 0.041480727483906354\n",
            "iter 988, loss = 0.04147806020244613\n",
            "iter 989, loss = 0.04147539304509834\n",
            "iter 990, loss = 0.041472726011855525\n",
            "iter 991, loss = 0.04147005910271011\n",
            "iter 992, loss = 0.04146739231765445\n",
            "iter 993, loss = 0.041464725656680794\n",
            "iter 994, loss = 0.041462059119781326\n",
            "iter 995, loss = 0.0414593927069481\n",
            "iter 996, loss = 0.0414567264181731\n",
            "iter 997, loss = 0.04145406025344823\n",
            "iter 998, loss = 0.04145139421276528\n",
            "iter 999, loss = 0.04144872829611597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "8gXUV1CYneSF",
        "outputId": "67039d24-0511-400b-9413-52169c8096d9"
      },
      "source": [
        "plot_curve(range(num_iterations), losses)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGXW5skFlyZ"
      },
      "source": [
        "### Logistic Regression Classifier\n",
        "\n",
        "So far, we have seen regression algorithms. We will now see a classification algorithm. Logistic Regression uses the `sigmoid` as the output function. sigmoid function transforms the predictions into a range [0, 1] which can be nicely used as a probability score for a class prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyQWhzBuFlEZ"
      },
      "source": [
        "# Sigmoid function and its gradient.\n",
        "\n",
        "\"\"\"\n",
        "sig = 1 / (1 + e^(-x)) \n",
        "\"\"\"\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\"\"\"\n",
        "Derivative of sigmoid:\n",
        "dsig = (-1 / (1 + e^(-x))^2) * e^(-x) * (-1)\n",
        "dsig =  e^(-x) / (1 + e^(-x))^2\n",
        "\n",
        "Adding +1 and -1 to the numerator:\n",
        "dsig = (1 + e^(-x) - 1) / (1 + e^(-x))^2\n",
        "\n",
        "Let's replace deno = 1 + e^(-x) in the equation so its easier to operate: \n",
        "dsig = (deno - 1) / deno^2\n",
        "\n",
        "Splitting the denominator into two deno:\n",
        "dsig = (deno/deno - 1/deno ) * (1/deno)\n",
        "\n",
        "Replace sig = 1/deno: \n",
        "dsig = (1 - sig) * sig\n",
        "\"\"\"\n",
        "\n",
        "def grad_sigmoid(z):\n",
        "  sig = sigmoid(z)\n",
        "  return (1 - sig) * sig\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJOiwhzJFlGv",
        "outputId": "45ff1834-fb9a-4670-d408-add7dbbb7a85"
      },
      "source": [
        "# Try out some random values and see the sigmoid transformation on the values. All the values should be squished between [0, 1]\n",
        "\n",
        "sigmoid(np.array([1.083579, -0.446771, -0.123665, 0.899643]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.74717068, 0.39012876, 0.46912309, 0.71087613])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkjAQ9_wNLC6"
      },
      "source": [
        "### Let's make some classification data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wayJ-PioFlIy"
      },
      "source": [
        "num_train_examples = 900\n",
        "num_features = 20 \n",
        "num_classes = 2 \n",
        "\n",
        "# let's make some data \n",
        "X_train = np.random.random((num_train_examples, num_features))\n",
        "y_train = np.random.randint(0, 2, num_train_examples)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFiMp5_1Nh-d",
        "outputId": "272f3008-3b04-4905-b277-223d1f976e41"
      },
      "source": [
        "print(X_train[:5], y_train[:5])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.31294739 0.93171592 0.75895225 0.16212908 0.56493403 0.13445388\n",
            "  0.58639983 0.92582337 0.60864097 0.49944246 0.74619062 0.5634484\n",
            "  0.62345999 0.24219214 0.89931828 0.43328129 0.58817594 0.43594389\n",
            "  0.67130117 0.94051155]\n",
            " [0.17769229 0.77488881 0.7821518  0.76317354 0.71561888 0.70656117\n",
            "  0.64444283 0.89239545 0.45000869 0.79368506 0.44461913 0.17038028\n",
            "  0.33162837 0.07356898 0.12816893 0.70455349 0.5150542  0.7316682\n",
            "  0.17996505 0.24687717]\n",
            " [0.84466303 0.47506014 0.10630105 0.47119152 0.35862454 0.64153631\n",
            "  0.42304303 0.95178009 0.95675142 0.60883918 0.67516046 0.09379607\n",
            "  0.13450048 0.12385304 0.81255077 0.56839223 0.62650793 0.23614535\n",
            "  0.5199379  0.06343868]\n",
            " [0.80746893 0.51279563 0.14671142 0.94925631 0.61364    0.70238355\n",
            "  0.55791921 0.48057762 0.881722   0.91611089 0.95986015 0.11411736\n",
            "  0.4009185  0.37218945 0.77225581 0.50521593 0.63781296 0.01456454\n",
            "  0.3987808  0.03697269]\n",
            " [0.73384056 0.50944631 0.1790313  0.13923337 0.71027708 0.53548879\n",
            "  0.99336658 0.57914356 0.02043959 0.06661141 0.96847315 0.51125074\n",
            "  0.93480126 0.1490121  0.98939828 0.88787552 0.705822   0.61942368\n",
            "  0.48559827 0.06023349]] [1 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1kYRRlTHujp"
      },
      "source": [
        "### Cross-Entropy loss or log loss.\n",
        "\n",
        "Cross Entropy is the measure of dissimilarity between two probability distributions. We measure the entropy of the distribution q relative to the distribution p using the formula (its derivation stems from KL divergence): \n",
        "\n",
        "H(p, q) = - p log (q) \n",
        "\n",
        "Let's say we have p as true label and q as predicted value. Let's assume we have two class labels 0 and 1. For a logistic regression, the value of \n",
        "\n",
        "q (y=1) = y_hat = sigmoid(x*w) \n",
        "\n",
        "q (y=0) = 1 - y_hat. \n",
        "\n",
        "For our binary outcome, we will have the following function for entropy: \n",
        "\n",
        "H(p, q) = - y log (y_hat) - (1 - y) log (1 - y_hat) \n",
        "\n",
        "The above can be derived from the bernoulli outcome too. f(p,q) = (q^p) (1-q)^(1-p)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8_TL2jJFlMm"
      },
      "source": [
        "def cross_entropy_loss(y, y_hat):\n",
        "  loss = - y * np.log(y_hat) - (1- y) * np.log(1 - y_hat)\n",
        "  mean_loss = np.sum(loss) / len(y)\n",
        "  return mean_loss"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeGjMoqsQTVc",
        "outputId": "f9ba2a68-5183-47a0-f6c6-c3bc70eec903"
      },
      "source": [
        "cross_entropy_loss(np.array([0, 1, 0]), np.array([0.12, 0.98, 0.001]))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.049678859720329295"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWV_1bVbgBI1"
      },
      "source": [
        "### Gradient Descent for Cross Entropy Loss and Logistic Regression.\n",
        "\n",
        "Above we derived the sig' = (1 - sig)sig \n",
        "\n",
        "It can be proven that the derivative of the cross entropy loss function is: \n",
        "\n",
        "grad = x * (sig - y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_z7pZQ9aq5z"
      },
      "source": [
        "def backprop_cross_entropy(y_hat, X, y, W, b, lr):\n",
        "  error = y_hat - y\n",
        "  dW = np.dot(X.T, error)\n",
        "  db = np.sum(error)\n",
        "\n",
        "  dW = dW/len(y)\n",
        "  db = db/len(y)\n",
        "\n",
        "  W = W - lr * dW\n",
        "  b = b - lr * db\n",
        "\n",
        "  return W, b\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJCRp70aaq73",
        "outputId": "cc3d1238-ee7c-4aae-92a3-301349776acb"
      },
      "source": [
        "lr = 0.0001\n",
        "num_iterations = 1000 \n",
        "losses = []\n",
        "num_features = 20 \n",
        "num_coefficients = 1 \n",
        "\n",
        "# Initialize the weights and biases\n",
        "W = np.random.random((num_features, num_coefficients))\n",
        "b = np.ones((num_coefficients))\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  z = np.dot(X_train, W) + b\n",
        "  y_hat = sigmoid(z)\n",
        "\n",
        "  loss = cross_entropy_loss(y_train, y_hat)\n",
        "  \n",
        "  W, b = backprop_cross_entropy(y_hat, X_train, y_train, W, b, lr)\n",
        "\n",
        "  losses.append(loss)\n",
        "\n",
        "  print(\"iter %s, loss = %s\" % (i, loss))\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss = 2266.6045423440564\n",
            "iter 1, loss = 2266.3878417137826\n",
            "iter 2, loss = 2266.171142461604\n",
            "iter 3, loss = 2265.954444588157\n",
            "iter 4, loss = 2265.737748094084\n",
            "iter 5, loss = 2265.521052980025\n",
            "iter 6, loss = 2265.30435924662\n",
            "iter 7, loss = 2265.0876668945134\n",
            "iter 8, loss = 2264.870975924345\n",
            "iter 9, loss = 2264.654286336756\n",
            "iter 10, loss = 2264.437598132391\n",
            "iter 11, loss = 2264.220911311892\n",
            "iter 12, loss = 2264.0042258759013\n",
            "iter 13, loss = 2263.7875418250605\n",
            "iter 14, loss = 2263.570859160015\n",
            "iter 15, loss = 2263.3541778814083\n",
            "iter 16, loss = 2263.1374979898837\n",
            "iter 17, loss = 2262.920819486087\n",
            "iter 18, loss = 2262.704142370662\n",
            "iter 19, loss = 2262.487466644252\n",
            "iter 20, loss = 2262.2707923075054\n",
            "iter 21, loss = 2262.0541193610657\n",
            "iter 22, loss = 2261.8374478055775\n",
            "iter 23, loss = 2261.6207776416904\n",
            "iter 24, loss = 2261.4041088700455\n",
            "iter 25, loss = 2261.1874414912972\n",
            "iter 26, loss = 2260.9707755060863\n",
            "iter 27, loss = 2260.7541109150616\n",
            "iter 28, loss = 2260.53744771887\n",
            "iter 29, loss = 2260.320785918162\n",
            "iter 30, loss = 2260.1041255135838\n",
            "iter 31, loss = 2259.887466505784\n",
            "iter 32, loss = 2259.670808895411\n",
            "iter 33, loss = 2259.454152683115\n",
            "iter 34, loss = 2259.237497869544\n",
            "iter 35, loss = 2259.0208444553473\n",
            "iter 36, loss = 2258.804192441178\n",
            "iter 37, loss = 2258.587541827684\n",
            "iter 38, loss = 2258.3708926155145\n",
            "iter 39, loss = 2258.154244805323\n",
            "iter 40, loss = 2257.937598397758\n",
            "iter 41, loss = 2257.7209533934724\n",
            "iter 42, loss = 2257.50430979312\n",
            "iter 43, loss = 2257.287667597349\n",
            "iter 44, loss = 2257.071026806813\n",
            "iter 45, loss = 2256.854387422166\n",
            "iter 46, loss = 2256.637749444058\n",
            "iter 47, loss = 2256.4211128731454\n",
            "iter 48, loss = 2256.2044777100787\n",
            "iter 49, loss = 2255.9878439555164\n",
            "iter 50, loss = 2255.7712116101075\n",
            "iter 51, loss = 2255.554580674508\n",
            "iter 52, loss = 2255.337951149373\n",
            "iter 53, loss = 2255.1213230353596\n",
            "iter 54, loss = 2254.904696333118\n",
            "iter 55, loss = 2254.688071043308\n",
            "iter 56, loss = 2254.4714471665834\n",
            "iter 57, loss = 2254.254824703601\n",
            "iter 58, loss = 2254.038203655019\n",
            "iter 59, loss = 2253.8215840214907\n",
            "iter 60, loss = 2253.604965803675\n",
            "iter 61, loss = 2253.3883490022304\n",
            "iter 62, loss = 2253.171733617813\n",
            "iter 63, loss = 2252.9551196510806\n",
            "iter 64, loss = 2252.738507102694\n",
            "iter 65, loss = 2252.5218959733065\n",
            "iter 66, loss = 2252.3052862635823\n",
            "iter 67, loss = 2252.08867797418\n",
            "iter 68, loss = 2251.872071105754\n",
            "iter 69, loss = 2251.6554656589688\n",
            "iter 70, loss = 2251.438861634486\n",
            "iter 71, loss = 2251.222259032961\n",
            "iter 72, loss = 2251.0056578550557\n",
            "iter 73, loss = 2250.7890581014317\n",
            "iter 74, loss = 2250.572459772755\n",
            "iter 75, loss = 2250.3558628696783\n",
            "iter 76, loss = 2250.1392673928694\n",
            "iter 77, loss = 2249.9226733429887\n",
            "iter 78, loss = 2249.706080720699\n",
            "iter 79, loss = 2249.4894895266634\n",
            "iter 80, loss = 2249.272899761545\n",
            "iter 81, loss = 2249.0563114260053\n",
            "iter 82, loss = 2248.839724520711\n",
            "iter 83, loss = 2248.6231390463226\n",
            "iter 84, loss = 2248.4065550035093\n",
            "iter 85, loss = 2248.1899723929305\n",
            "iter 86, loss = 2247.973391215254\n",
            "iter 87, loss = 2247.7568114711435\n",
            "iter 88, loss = 2247.540233161266\n",
            "iter 89, loss = 2247.3236562862853\n",
            "iter 90, loss = 2247.1070808468685\n",
            "iter 91, loss = 2246.890506843684\n",
            "iter 92, loss = 2246.673934277394\n",
            "iter 93, loss = 2246.45736314867\n",
            "iter 94, loss = 2246.2407934581793\n",
            "iter 95, loss = 2246.024225206584\n",
            "iter 96, loss = 2245.8076583945567\n",
            "iter 97, loss = 2245.591093022764\n",
            "iter 98, loss = 2245.3745290918755\n",
            "iter 99, loss = 2245.1579666025605\n",
            "iter 100, loss = 2244.9414055554857\n",
            "iter 101, loss = 2244.7248459513225\n",
            "iter 102, loss = 2244.50828779074\n",
            "iter 103, loss = 2244.2917310744087\n",
            "iter 104, loss = 2244.075175802997\n",
            "iter 105, loss = 2243.8586219771782\n",
            "iter 106, loss = 2243.642069597622\n",
            "iter 107, loss = 2243.425518665\n",
            "iter 108, loss = 2243.208969179983\n",
            "iter 109, loss = 2242.9924211432426\n",
            "iter 110, loss = 2242.775874555452\n",
            "iter 111, loss = 2242.5593294172836\n",
            "iter 112, loss = 2242.3427857294096\n",
            "iter 113, loss = 2242.1262434925047\n",
            "iter 114, loss = 2241.9097027072394\n",
            "iter 115, loss = 2241.6931633742884\n",
            "iter 116, loss = 2241.4766254943293\n",
            "iter 117, loss = 2241.26008906803\n",
            "iter 118, loss = 2241.0435540960707\n",
            "iter 119, loss = 2240.827020579123\n",
            "iter 120, loss = 2240.610488517865\n",
            "iter 121, loss = 2240.3939579129683\n",
            "iter 122, loss = 2240.1774287651115\n",
            "iter 123, loss = 2239.960901074972\n",
            "iter 124, loss = 2239.7443748432215\n",
            "iter 125, loss = 2239.527850070541\n",
            "iter 126, loss = 2239.3113267576064\n",
            "iter 127, loss = 2239.0948049050935\n",
            "iter 128, loss = 2238.8782845136843\n",
            "iter 129, loss = 2238.661765584051\n",
            "iter 130, loss = 2238.445248116874\n",
            "iter 131, loss = 2238.2287321128356\n",
            "iter 132, loss = 2238.01221757261\n",
            "iter 133, loss = 2237.7957044968775\n",
            "iter 134, loss = 2237.57919288632\n",
            "iter 135, loss = 2237.3626827416147\n",
            "iter 136, loss = 2237.146174063443\n",
            "iter 137, loss = 2236.9296668524835\n",
            "iter 138, loss = 2236.7131611094205\n",
            "iter 139, loss = 2236.4966568349337\n",
            "iter 140, loss = 2236.2801540297014\n",
            "iter 141, loss = 2236.0636526944095\n",
            "iter 142, loss = 2235.8471528297373\n",
            "iter 143, loss = 2235.630654436368\n",
            "iter 144, loss = 2235.414157514984\n",
            "iter 145, loss = 2235.197662066269\n",
            "iter 146, loss = 2234.9811680909047\n",
            "iter 147, loss = 2234.7646755895757\n",
            "iter 148, loss = 2234.5481845629683\n",
            "iter 149, loss = 2234.3316950117623\n",
            "iter 150, loss = 2234.115206936644\n",
            "iter 151, loss = 2233.8987203382976\n",
            "iter 152, loss = 2233.6822352174095\n",
            "iter 153, loss = 2233.465751574664\n",
            "iter 154, loss = 2233.2492694107477\n",
            "iter 155, loss = 2233.0327887263475\n",
            "iter 156, loss = 2232.8163095221457\n",
            "iter 157, loss = 2232.5998317988324\n",
            "iter 158, loss = 2232.3833555570945\n",
            "iter 159, loss = 2232.166880797618\n",
            "iter 160, loss = 2231.9504075210925\n",
            "iter 161, loss = 2231.733935728202\n",
            "iter 162, loss = 2231.5174654196403\n",
            "iter 163, loss = 2231.300996596089\n",
            "iter 164, loss = 2231.084529258245\n",
            "iter 165, loss = 2230.8680634067896\n",
            "iter 166, loss = 2230.6515990424173\n",
            "iter 167, loss = 2230.4351361658178\n",
            "iter 168, loss = 2230.2186747776777\n",
            "iter 169, loss = 2230.00221487869\n",
            "iter 170, loss = 2229.7857564695464\n",
            "iter 171, loss = 2229.569299550934\n",
            "iter 172, loss = 2229.3528441235485\n",
            "iter 173, loss = 2229.1363901880777\n",
            "iter 174, loss = 2228.919937745217\n",
            "iter 175, loss = 2228.703486795657\n",
            "iter 176, loss = 2228.4870373400922\n",
            "iter 177, loss = 2228.2705893792095\n",
            "iter 178, loss = 2228.054142913711\n",
            "iter 179, loss = 2227.8376979442837\n",
            "iter 180, loss = 2227.621254471622\n",
            "iter 181, loss = 2227.404812496423\n",
            "iter 182, loss = 2227.188372019379\n",
            "iter 183, loss = 2226.9719330411854\n",
            "iter 184, loss = 2226.7554955625383\n",
            "iter 185, loss = 2226.539059584131\n",
            "iter 186, loss = 2226.3226251066603\n",
            "iter 187, loss = 2226.1061921308233\n",
            "iter 188, loss = 2225.889760657314\n",
            "iter 189, loss = 2225.6733306868314\n",
            "iter 190, loss = 2225.4569022200717\n",
            "iter 191, loss = 2225.240475257733\n",
            "iter 192, loss = 2225.0240498005096\n",
            "iter 193, loss = 2224.807625849104\n",
            "iter 194, loss = 2224.5912034042103\n",
            "iter 195, loss = 2224.374782466531\n",
            "iter 196, loss = 2224.1583630367622\n",
            "iter 197, loss = 2223.941945115605\n",
            "iter 198, loss = 2223.7255287037565\n",
            "iter 199, loss = 2223.5091138019197\n",
            "iter 200, loss = 2223.292700410793\n",
            "iter 201, loss = 2223.076288531075\n",
            "iter 202, loss = 2222.8598781634696\n",
            "iter 203, loss = 2222.6434693086762\n",
            "iter 204, loss = 2222.427061967398\n",
            "iter 205, loss = 2222.2106561403352\n",
            "iter 206, loss = 2221.9942518281896\n",
            "iter 207, loss = 2221.777849031664\n",
            "iter 208, loss = 2221.5614477514623\n",
            "iter 209, loss = 2221.3450479882854\n",
            "iter 210, loss = 2221.1286497428387\n",
            "iter 211, loss = 2220.9122530158243\n",
            "iter 212, loss = 2220.6958578079475\n",
            "iter 213, loss = 2220.4794641199105\n",
            "iter 214, loss = 2220.263071952421\n",
            "iter 215, loss = 2220.0466813061807\n",
            "iter 216, loss = 2219.830292181897\n",
            "iter 217, loss = 2219.613904580275\n",
            "iter 218, loss = 2219.3975185020204\n",
            "iter 219, loss = 2219.1811339478413\n",
            "iter 220, loss = 2218.96475091844\n",
            "iter 221, loss = 2218.7483694145267\n",
            "iter 222, loss = 2218.531989436807\n",
            "iter 223, loss = 2218.315610985989\n",
            "iter 224, loss = 2218.0992340627818\n",
            "iter 225, loss = 2217.8828586678924\n",
            "iter 226, loss = 2217.6664848020264\n",
            "iter 227, loss = 2217.450112465897\n",
            "iter 228, loss = 2217.233741660213\n",
            "iter 229, loss = 2217.0173723856806\n",
            "iter 230, loss = 2216.8010046430118\n",
            "iter 231, loss = 2216.584638432916\n",
            "iter 232, loss = 2216.3682737561035\n",
            "iter 233, loss = 2216.151910613287\n",
            "iter 234, loss = 2215.935549005175\n",
            "iter 235, loss = 2215.7191889324786\n",
            "iter 236, loss = 2215.5028303959116\n",
            "iter 237, loss = 2215.286473396185\n",
            "iter 238, loss = 2215.0701179340103\n",
            "iter 239, loss = 2214.8537640101013\n",
            "iter 240, loss = 2214.63741162517\n",
            "iter 241, loss = 2214.421060779931\n",
            "iter 242, loss = 2214.2047114750953\n",
            "iter 243, loss = 2213.9883637113803\n",
            "iter 244, loss = 2213.7720174894985\n",
            "iter 245, loss = 2213.5556728101637\n",
            "iter 246, loss = 2213.339329674093\n",
            "iter 247, loss = 2213.1229880819983\n",
            "iter 248, loss = 2212.9066480345987\n",
            "iter 249, loss = 2212.690309532608\n",
            "iter 250, loss = 2212.4739725767417\n",
            "iter 251, loss = 2212.257637167718\n",
            "iter 252, loss = 2212.0413033062537\n",
            "iter 253, loss = 2211.824970993067\n",
            "iter 254, loss = 2211.60864022887\n",
            "iter 255, loss = 2211.392311014386\n",
            "iter 256, loss = 2211.1759833503306\n",
            "iter 257, loss = 2210.9596572374257\n",
            "iter 258, loss = 2210.7433326763858\n",
            "iter 259, loss = 2210.5270096679305\n",
            "iter 260, loss = 2210.3106882127827\n",
            "iter 261, loss = 2210.094368311659\n",
            "iter 262, loss = 2209.87804996528\n",
            "iter 263, loss = 2209.661733174366\n",
            "iter 264, loss = 2209.4454179396394\n",
            "iter 265, loss = 2209.2291042618194\n",
            "iter 266, loss = 2209.0127921416283\n",
            "iter 267, loss = 2208.796481579787\n",
            "iter 268, loss = 2208.5801725770184\n",
            "iter 269, loss = 2208.3638651340448\n",
            "iter 270, loss = 2208.1475592515885\n",
            "iter 271, loss = 2207.9312549303727\n",
            "iter 272, loss = 2207.71495217112\n",
            "iter 273, loss = 2207.4986509745554\n",
            "iter 274, loss = 2207.2823513414023\n",
            "iter 275, loss = 2207.066053272386\n",
            "iter 276, loss = 2206.849756768227\n",
            "iter 277, loss = 2206.633461829655\n",
            "iter 278, loss = 2206.417168457395\n",
            "iter 279, loss = 2206.200876652172\n",
            "iter 280, loss = 2205.984586414708\n",
            "iter 281, loss = 2205.7682977457353\n",
            "iter 282, loss = 2205.5520106459753\n",
            "iter 283, loss = 2205.33572511616\n",
            "iter 284, loss = 2205.119441157014\n",
            "iter 285, loss = 2204.903158769264\n",
            "iter 286, loss = 2204.68687795364\n",
            "iter 287, loss = 2204.470598710868\n",
            "iter 288, loss = 2204.2543210416793\n",
            "iter 289, loss = 2204.0380449468\n",
            "iter 290, loss = 2203.8217704269614\n",
            "iter 291, loss = 2203.6054974828926\n",
            "iter 292, loss = 2203.3892261153223\n",
            "iter 293, loss = 2203.1729563249837\n",
            "iter 294, loss = 2202.9566881126043\n",
            "iter 295, loss = 2202.740421478915\n",
            "iter 296, loss = 2202.5241564246494\n",
            "iter 297, loss = 2202.307892950537\n",
            "iter 298, loss = 2202.0916310573107\n",
            "iter 299, loss = 2201.875370745702\n",
            "iter 300, loss = 2201.6591120164458\n",
            "iter 301, loss = 2201.442854870271\n",
            "iter 302, loss = 2201.226599307915\n",
            "iter 303, loss = 2201.0103453301094\n",
            "iter 304, loss = 2200.794092937585\n",
            "iter 305, loss = 2200.5778421310815\n",
            "iter 306, loss = 2200.3615929113307\n",
            "iter 307, loss = 2200.145345279068\n",
            "iter 308, loss = 2199.929099235027\n",
            "iter 309, loss = 2199.7128547799443\n",
            "iter 310, loss = 2199.496611914556\n",
            "iter 311, loss = 2199.2803706396\n",
            "iter 312, loss = 2199.064130955809\n",
            "iter 313, loss = 2198.847892863925\n",
            "iter 314, loss = 2198.631656364678\n",
            "iter 315, loss = 2198.4154214588134\n",
            "iter 316, loss = 2198.199188147063\n",
            "iter 317, loss = 2197.982956430169\n",
            "iter 318, loss = 2197.766726308867\n",
            "iter 319, loss = 2197.5504977838978\n",
            "iter 320, loss = 2197.3342708559985\n",
            "iter 321, loss = 2197.1180455259114\n",
            "iter 322, loss = 2196.901821794374\n",
            "iter 323, loss = 2196.6855996621284\n",
            "iter 324, loss = 2196.4693791299132\n",
            "iter 325, loss = 2196.2531601984715\n",
            "iter 326, loss = 2196.036942868542\n",
            "iter 327, loss = 2195.8207271408687\n",
            "iter 328, loss = 2195.6045130161915\n",
            "iter 329, loss = 2195.3883004952536\n",
            "iter 330, loss = 2195.1720895787985\n",
            "iter 331, loss = 2194.955880267567\n",
            "iter 332, loss = 2194.7396725623016\n",
            "iter 333, loss = 2194.5234664637483\n",
            "iter 334, loss = 2194.3072619726518\n",
            "iter 335, loss = 2194.0910590897524\n",
            "iter 336, loss = 2193.8748578157965\n",
            "iter 337, loss = 2193.6586581515317\n",
            "iter 338, loss = 2193.4424600977\n",
            "iter 339, loss = 2193.226263655048\n",
            "iter 340, loss = 2193.010068824319\n",
            "iter 341, loss = 2192.7938756062636\n",
            "iter 342, loss = 2192.577684001627\n",
            "iter 343, loss = 2192.361494011154\n",
            "iter 344, loss = 2192.1453056355945\n",
            "iter 345, loss = 2191.9291188756933\n",
            "iter 346, loss = 2191.7129337322012\n",
            "iter 347, loss = 2191.496750205866\n",
            "iter 348, loss = 2191.280568297433\n",
            "iter 349, loss = 2191.0643880076555\n",
            "iter 350, loss = 2190.848209337279\n",
            "iter 351, loss = 2190.6320322870574\n",
            "iter 352, loss = 2190.4158568577345\n",
            "iter 353, loss = 2190.199683050068\n",
            "iter 354, loss = 2189.9835108648012\n",
            "iter 355, loss = 2189.7673403026915\n",
            "iter 356, loss = 2189.5511713644864\n",
            "iter 357, loss = 2189.3350040509386\n",
            "iter 358, loss = 2189.118838362799\n",
            "iter 359, loss = 2188.9026743008217\n",
            "iter 360, loss = 2188.6865118657565\n",
            "iter 361, loss = 2188.4703510583595\n",
            "iter 362, loss = 2188.2541918793836\n",
            "iter 363, loss = 2188.038034329581\n",
            "iter 364, loss = 2187.8218784097066\n",
            "iter 365, loss = 2187.605724120514\n",
            "iter 366, loss = 2187.3895714627592\n",
            "iter 367, loss = 2187.173420437196\n",
            "iter 368, loss = 2186.9572710445796\n",
            "iter 369, loss = 2186.741123285668\n",
            "iter 370, loss = 2186.524977161214\n",
            "iter 371, loss = 2186.3088326719744\n",
            "iter 372, loss = 2186.0926898187076\n",
            "iter 373, loss = 2185.8765486021716\n",
            "iter 374, loss = 2185.660409023122\n",
            "iter 375, loss = 2185.444271082317\n",
            "iter 376, loss = 2185.228134780514\n",
            "iter 377, loss = 2185.012000118473\n",
            "iter 378, loss = 2184.79586709695\n",
            "iter 379, loss = 2184.579735716706\n",
            "iter 380, loss = 2184.363605978502\n",
            "iter 381, loss = 2184.147477883095\n",
            "iter 382, loss = 2183.9313514312457\n",
            "iter 383, loss = 2183.7152266237163\n",
            "iter 384, loss = 2183.499103461266\n",
            "iter 385, loss = 2183.282981944656\n",
            "iter 386, loss = 2183.066862074648\n",
            "iter 387, loss = 2182.8507438520032\n",
            "iter 388, loss = 2182.6346272774877\n",
            "iter 389, loss = 2182.4185123518587\n",
            "iter 390, loss = 2182.202399075882\n",
            "iter 391, loss = 2181.986287450321\n",
            "iter 392, loss = 2181.7701774759375\n",
            "iter 393, loss = 2181.5540691534966\n",
            "iter 394, loss = 2181.337962483761\n",
            "iter 395, loss = 2181.1218574674986\n",
            "iter 396, loss = 2180.905754105471\n",
            "iter 397, loss = 2180.6896523984465\n",
            "iter 398, loss = 2180.4735523471895\n",
            "iter 399, loss = 2180.2574539524644\n",
            "iter 400, loss = 2180.041357215038\n",
            "iter 401, loss = 2179.8252621356787\n",
            "iter 402, loss = 2179.609168715153\n",
            "iter 403, loss = 2179.3930769542258\n",
            "iter 404, loss = 2179.1769868536694\n",
            "iter 405, loss = 2178.9608984142474\n",
            "iter 406, loss = 2178.7448116367304\n",
            "iter 407, loss = 2178.5287265218867\n",
            "iter 408, loss = 2178.3126430704865\n",
            "iter 409, loss = 2178.0965612832965\n",
            "iter 410, loss = 2177.88048116109\n",
            "iter 411, loss = 2177.664402704634\n",
            "iter 412, loss = 2177.4483259146996\n",
            "iter 413, loss = 2177.2322507920585\n",
            "iter 414, loss = 2177.016177337484\n",
            "iter 415, loss = 2176.8001055517425\n",
            "iter 416, loss = 2176.58403543561\n",
            "iter 417, loss = 2176.3679669898575\n",
            "iter 418, loss = 2176.151900215256\n",
            "iter 419, loss = 2175.93583511258\n",
            "iter 420, loss = 2175.7197716826026\n",
            "iter 421, loss = 2175.503709926099\n",
            "iter 422, loss = 2175.287649843839\n",
            "iter 423, loss = 2175.0715914366015\n",
            "iter 424, loss = 2174.855534705159\n",
            "iter 425, loss = 2174.6394796502855\n",
            "iter 426, loss = 2174.423426272756\n",
            "iter 427, loss = 2174.207374573349\n",
            "iter 428, loss = 2173.9913245528396\n",
            "iter 429, loss = 2173.775276212001\n",
            "iter 430, loss = 2173.5592295516158\n",
            "iter 431, loss = 2173.3431845724554\n",
            "iter 432, loss = 2173.127141275303\n",
            "iter 433, loss = 2172.9110996609297\n",
            "iter 434, loss = 2172.6950597301184\n",
            "iter 435, loss = 2172.4790214836457\n",
            "iter 436, loss = 2172.26298492229\n",
            "iter 437, loss = 2172.046950046833\n",
            "iter 438, loss = 2171.8309168580518\n",
            "iter 439, loss = 2171.6148853567265\n",
            "iter 440, loss = 2171.3988555436395\n",
            "iter 441, loss = 2171.1828274195677\n",
            "iter 442, loss = 2170.9668009852953\n",
            "iter 443, loss = 2170.750776241604\n",
            "iter 444, loss = 2170.534753189271\n",
            "iter 445, loss = 2170.3187318290825\n",
            "iter 446, loss = 2170.102712161819\n",
            "iter 447, loss = 2169.8866941882643\n",
            "iter 448, loss = 2169.670677909201\n",
            "iter 449, loss = 2169.45466332541\n",
            "iter 450, loss = 2169.2386504376805\n",
            "iter 451, loss = 2169.022639246792\n",
            "iter 452, loss = 2168.8066297535306\n",
            "iter 453, loss = 2168.5906219586795\n",
            "iter 454, loss = 2168.374615863027\n",
            "iter 455, loss = 2168.1586114673555\n",
            "iter 456, loss = 2167.9426087724532\n",
            "iter 457, loss = 2167.726607779105\n",
            "iter 458, loss = 2167.510608488099\n",
            "iter 459, loss = 2167.2946109002187\n",
            "iter 460, loss = 2167.078615016254\n",
            "iter 461, loss = 2166.8626208369924\n",
            "iter 462, loss = 2166.646628363223\n",
            "iter 463, loss = 2166.4306375957317\n",
            "iter 464, loss = 2166.2146485353082\n",
            "iter 465, loss = 2165.99866118274\n",
            "iter 466, loss = 2165.7826755388196\n",
            "iter 467, loss = 2165.566691604335\n",
            "iter 468, loss = 2165.3507093800768\n",
            "iter 469, loss = 2165.1347288668344\n",
            "iter 470, loss = 2164.9187500653993\n",
            "iter 471, loss = 2164.702772976563\n",
            "iter 472, loss = 2164.4867976011183\n",
            "iter 473, loss = 2164.270823939854\n",
            "iter 474, loss = 2164.0548519935633\n",
            "iter 475, loss = 2163.838881763041\n",
            "iter 476, loss = 2163.622913249078\n",
            "iter 477, loss = 2163.406946452466\n",
            "iter 478, loss = 2163.1909813740026\n",
            "iter 479, loss = 2162.97501801448\n",
            "iter 480, loss = 2162.759056374691\n",
            "iter 481, loss = 2162.543096455434\n",
            "iter 482, loss = 2162.3271382575\n",
            "iter 483, loss = 2162.1111817816854\n",
            "iter 484, loss = 2161.89522702879\n",
            "iter 485, loss = 2161.6792739996044\n",
            "iter 486, loss = 2161.4633226949295\n",
            "iter 487, loss = 2161.247373115558\n",
            "iter 488, loss = 2161.031425262292\n",
            "iter 489, loss = 2160.8154791359234\n",
            "iter 490, loss = 2160.5995347372555\n",
            "iter 491, loss = 2160.3835920670845\n",
            "iter 492, loss = 2160.1676511262067\n",
            "iter 493, loss = 2159.9517119154257\n",
            "iter 494, loss = 2159.7357744355363\n",
            "iter 495, loss = 2159.519838687342\n",
            "iter 496, loss = 2159.3039046716426\n",
            "iter 497, loss = 2159.087972389236\n",
            "iter 498, loss = 2158.8720418409225\n",
            "iter 499, loss = 2158.6561130275063\n",
            "iter 500, loss = 2158.4401859497875\n",
            "iter 501, loss = 2158.2242606085674\n",
            "iter 502, loss = 2158.008337004652\n",
            "iter 503, loss = 2157.792415138839\n",
            "iter 504, loss = 2157.576495011931\n",
            "iter 505, loss = 2157.360576624737\n",
            "iter 506, loss = 2157.144659978057\n",
            "iter 507, loss = 2156.9287450726947\n",
            "iter 508, loss = 2156.712831909454\n",
            "iter 509, loss = 2156.4969204891418\n",
            "iter 510, loss = 2156.2810108125623\n",
            "iter 511, loss = 2156.065102880522\n",
            "iter 512, loss = 2155.849196693824\n",
            "iter 513, loss = 2155.6332922532774\n",
            "iter 514, loss = 2155.417389559687\n",
            "iter 515, loss = 2155.20148861386\n",
            "iter 516, loss = 2154.9855894166053\n",
            "iter 517, loss = 2154.769691968728\n",
            "iter 518, loss = 2154.553796271036\n",
            "iter 519, loss = 2154.3379023243433\n",
            "iter 520, loss = 2154.122010129451\n",
            "iter 521, loss = 2153.9061196871735\n",
            "iter 522, loss = 2153.6902309983184\n",
            "iter 523, loss = 2153.474344063694\n",
            "iter 524, loss = 2153.2584588841128\n",
            "iter 525, loss = 2153.0425754603857\n",
            "iter 526, loss = 2152.8266937933217\n",
            "iter 527, loss = 2152.610813883733\n",
            "iter 528, loss = 2152.394935732432\n",
            "iter 529, loss = 2152.179059340228\n",
            "iter 530, loss = 2151.9631847079368\n",
            "iter 531, loss = 2151.747311836369\n",
            "iter 532, loss = 2151.531440726339\n",
            "iter 533, loss = 2151.3155713786596\n",
            "iter 534, loss = 2151.0997037941447\n",
            "iter 535, loss = 2150.883837973607\n",
            "iter 536, loss = 2150.667973917865\n",
            "iter 537, loss = 2150.4521116277306\n",
            "iter 538, loss = 2150.236251104018\n",
            "iter 539, loss = 2150.020392347546\n",
            "iter 540, loss = 2149.8045353591283\n",
            "iter 541, loss = 2149.5886801395814\n",
            "iter 542, loss = 2149.3728266897256\n",
            "iter 543, loss = 2149.1569750103736\n",
            "iter 544, loss = 2148.941125102344\n",
            "iter 545, loss = 2148.725276966454\n",
            "iter 546, loss = 2148.509430603525\n",
            "iter 547, loss = 2148.2935860143734\n",
            "iter 548, loss = 2148.077743199818\n",
            "iter 549, loss = 2147.861902160679\n",
            "iter 550, loss = 2147.6460628977743\n",
            "iter 551, loss = 2147.4302254119266\n",
            "iter 552, loss = 2147.214389703954\n",
            "iter 553, loss = 2146.9985557746772\n",
            "iter 554, loss = 2146.78272362492\n",
            "iter 555, loss = 2146.566893255502\n",
            "iter 556, loss = 2146.351064667245\n",
            "iter 557, loss = 2146.135237860972\n",
            "iter 558, loss = 2145.9194128375048\n",
            "iter 559, loss = 2145.7035895976665\n",
            "iter 560, loss = 2145.4877681422827\n",
            "iter 561, loss = 2145.2719484721733\n",
            "iter 562, loss = 2145.0561305881643\n",
            "iter 563, loss = 2144.840314491081\n",
            "iter 564, loss = 2144.624500181747\n",
            "iter 565, loss = 2144.408687660988\n",
            "iter 566, loss = 2144.19287692963\n",
            "iter 567, loss = 2143.9770679884987\n",
            "iter 568, loss = 2143.7612608384193\n",
            "iter 569, loss = 2143.545455480219\n",
            "iter 570, loss = 2143.329651914725\n",
            "iter 571, loss = 2143.1138501427663\n",
            "iter 572, loss = 2142.898050165171\n",
            "iter 573, loss = 2142.6822519827633\n",
            "iter 574, loss = 2142.466455596374\n",
            "iter 575, loss = 2142.2506610068317\n",
            "iter 576, loss = 2142.0348682149656\n",
            "iter 577, loss = 2141.819077221606\n",
            "iter 578, loss = 2141.6032880275834\n",
            "iter 579, loss = 2141.387500633728\n",
            "iter 580, loss = 2141.1717150408704\n",
            "iter 581, loss = 2140.95593124984\n",
            "iter 582, loss = 2140.740149261471\n",
            "iter 583, loss = 2140.5243690765924\n",
            "iter 584, loss = 2140.308590696038\n",
            "iter 585, loss = 2140.0928141206414\n",
            "iter 586, loss = 2139.877039351233\n",
            "iter 587, loss = 2139.661266388651\n",
            "iter 588, loss = 2139.4454952337237\n",
            "iter 589, loss = 2139.2297258872877\n",
            "iter 590, loss = 2139.0139583501777\n",
            "iter 591, loss = 2138.7981926232264\n",
            "iter 592, loss = 2138.582428707273\n",
            "iter 593, loss = 2138.366666603149\n",
            "iter 594, loss = 2138.1509063116932\n",
            "iter 595, loss = 2137.9351478337408\n",
            "iter 596, loss = 2137.7193911701274\n",
            "iter 597, loss = 2137.5036363216936\n",
            "iter 598, loss = 2137.287883289271\n",
            "iter 599, loss = 2137.072132073704\n",
            "iter 600, loss = 2136.8563826758277\n",
            "iter 601, loss = 2136.640635096479\n",
            "iter 602, loss = 2136.4248893365007\n",
            "iter 603, loss = 2136.209145396729\n",
            "iter 604, loss = 2135.993403278005\n",
            "iter 605, loss = 2135.7776629811706\n",
            "iter 606, loss = 2135.561924507061\n",
            "iter 607, loss = 2135.346187856522\n",
            "iter 608, loss = 2135.1304530303933\n",
            "iter 609, loss = 2134.9147200295156\n",
            "iter 610, loss = 2134.698988854732\n",
            "iter 611, loss = 2134.4832595068838\n",
            "iter 612, loss = 2134.2675319868144\n",
            "iter 613, loss = 2134.0518062953665\n",
            "iter 614, loss = 2133.836082433387\n",
            "iter 615, loss = 2133.620360401713\n",
            "iter 616, loss = 2133.404640201195\n",
            "iter 617, loss = 2133.188921832673\n",
            "iter 618, loss = 2132.9732052969944\n",
            "iter 619, loss = 2132.757490595005\n",
            "iter 620, loss = 2132.541777727549\n",
            "iter 621, loss = 2132.3260666954734\n",
            "iter 622, loss = 2132.1103574996246\n",
            "iter 623, loss = 2131.894650140848\n",
            "iter 624, loss = 2131.678944619995\n",
            "iter 625, loss = 2131.4632409379083\n",
            "iter 626, loss = 2131.2475390954382\n",
            "iter 627, loss = 2131.0318390934344\n",
            "iter 628, loss = 2130.8161409327436\n",
            "iter 629, loss = 2130.600444614215\n",
            "iter 630, loss = 2130.3847501386995\n",
            "iter 631, loss = 2130.1690575070447\n",
            "iter 632, loss = 2129.9533667201035\n",
            "iter 633, loss = 2129.7376777787263\n",
            "iter 634, loss = 2129.5219906837615\n",
            "iter 635, loss = 2129.3063054360637\n",
            "iter 636, loss = 2129.090622036481\n",
            "iter 637, loss = 2128.8749404858704\n",
            "iter 638, loss = 2128.659260785081\n",
            "iter 639, loss = 2128.443582934967\n",
            "iter 640, loss = 2128.22790693638\n",
            "iter 641, loss = 2128.0122327901754\n",
            "iter 642, loss = 2127.796560497208\n",
            "iter 643, loss = 2127.5808900583315\n",
            "iter 644, loss = 2127.3652214743997\n",
            "iter 645, loss = 2127.1495547462678\n",
            "iter 646, loss = 2126.933889874793\n",
            "iter 647, loss = 2126.7182268608317\n",
            "iter 648, loss = 2126.502565705238\n",
            "iter 649, loss = 2126.2869064088695\n",
            "iter 650, loss = 2126.0712489725825\n",
            "iter 651, loss = 2125.855593397236\n",
            "iter 652, loss = 2125.6399396836887\n",
            "iter 653, loss = 2125.424287832796\n",
            "iter 654, loss = 2125.2086378454196\n",
            "iter 655, loss = 2124.992989722417\n",
            "iter 656, loss = 2124.777343464648\n",
            "iter 657, loss = 2124.5616990729704\n",
            "iter 658, loss = 2124.3460565482487\n",
            "iter 659, loss = 2124.13041589134\n",
            "iter 660, loss = 2123.914777103105\n",
            "iter 661, loss = 2123.699140184408\n",
            "iter 662, loss = 2123.4835051361074\n",
            "iter 663, loss = 2123.2678719590676\n",
            "iter 664, loss = 2123.05224065415\n",
            "iter 665, loss = 2122.8366112222193\n",
            "iter 666, loss = 2122.6209836641365\n",
            "iter 667, loss = 2122.405357980766\n",
            "iter 668, loss = 2122.1897341729714\n",
            "iter 669, loss = 2121.9741122416194\n",
            "iter 670, loss = 2121.7584921875714\n",
            "iter 671, loss = 2121.5428740116963\n",
            "iter 672, loss = 2121.327257714856\n",
            "iter 673, loss = 2121.1116432979175\n",
            "iter 674, loss = 2120.8960307617485\n",
            "iter 675, loss = 2120.680420107215\n",
            "iter 676, loss = 2120.464811335184\n",
            "iter 677, loss = 2120.2492044465234\n",
            "iter 678, loss = 2120.0335994421016\n",
            "iter 679, loss = 2119.8179963227853\n",
            "iter 680, loss = 2119.6023950894446\n",
            "iter 681, loss = 2119.3867957429466\n",
            "iter 682, loss = 2119.171198284162\n",
            "iter 683, loss = 2118.9556027139633\n",
            "iter 684, loss = 2118.7400090332153\n",
            "iter 685, loss = 2118.524417242792\n",
            "iter 686, loss = 2118.308827343565\n",
            "iter 687, loss = 2118.093239336403\n",
            "iter 688, loss = 2117.8776532221814\n",
            "iter 689, loss = 2117.6620690017694\n",
            "iter 690, loss = 2117.446486676038\n",
            "iter 691, loss = 2117.2309062458653\n",
            "iter 692, loss = 2117.015327712121\n",
            "iter 693, loss = 2116.799751075679\n",
            "iter 694, loss = 2116.5841763374156\n",
            "iter 695, loss = 2116.368603498202\n",
            "iter 696, loss = 2116.1530325589156\n",
            "iter 697, loss = 2115.937463520431\n",
            "iter 698, loss = 2115.721896383623\n",
            "iter 699, loss = 2115.506331149369\n",
            "iter 700, loss = 2115.2907678185447\n",
            "iter 701, loss = 2115.075206392027\n",
            "iter 702, loss = 2114.8596468706933\n",
            "iter 703, loss = 2114.6440892554197\n",
            "iter 704, loss = 2114.428533547087\n",
            "iter 705, loss = 2114.2129797465723\n",
            "iter 706, loss = 2113.997427854754\n",
            "iter 707, loss = 2113.781877872511\n",
            "iter 708, loss = 2113.566329800725\n",
            "iter 709, loss = 2113.350783640272\n",
            "iter 710, loss = 2113.135239392036\n",
            "iter 711, loss = 2112.919697056896\n",
            "iter 712, loss = 2112.7041566357325\n",
            "iter 713, loss = 2112.4886181294296\n",
            "iter 714, loss = 2112.2730815388672\n",
            "iter 715, loss = 2112.0575468649276\n",
            "iter 716, loss = 2111.842014108495\n",
            "iter 717, loss = 2111.6264832704505\n",
            "iter 718, loss = 2111.4109543516774\n",
            "iter 719, loss = 2111.1954273530628\n",
            "iter 720, loss = 2110.979902275488\n",
            "iter 721, loss = 2110.764379119839\n",
            "iter 722, loss = 2110.548857887\n",
            "iter 723, loss = 2110.333338577855\n",
            "iter 724, loss = 2110.1178211932934\n",
            "iter 725, loss = 2109.9023057342\n",
            "iter 726, loss = 2109.6867922014612\n",
            "iter 727, loss = 2109.4712805959625\n",
            "iter 728, loss = 2109.2557709185944\n",
            "iter 729, loss = 2109.0402631702423\n",
            "iter 730, loss = 2108.824757351796\n",
            "iter 731, loss = 2108.609253464142\n",
            "iter 732, loss = 2108.393751508172\n",
            "iter 733, loss = 2108.1782514847728\n",
            "iter 734, loss = 2107.9627533948355\n",
            "iter 735, loss = 2107.7472572392485\n",
            "iter 736, loss = 2107.531763018907\n",
            "iter 737, loss = 2107.3162707346983\n",
            "iter 738, loss = 2107.1007803875136\n",
            "iter 739, loss = 2106.885291978245\n",
            "iter 740, loss = 2106.6698055077864\n",
            "iter 741, loss = 2106.4543209770277\n",
            "iter 742, loss = 2106.238838386865\n",
            "iter 743, loss = 2106.02335773819\n",
            "iter 744, loss = 2105.807879031894\n",
            "iter 745, loss = 2105.5924022688764\n",
            "iter 746, loss = 2105.3769274500282\n",
            "iter 747, loss = 2105.161454576242\n",
            "iter 748, loss = 2104.9459836484198\n",
            "iter 749, loss = 2104.7305146674535\n",
            "iter 750, loss = 2104.5150476342387\n",
            "iter 751, loss = 2104.2995825496723\n",
            "iter 752, loss = 2104.084119414651\n",
            "iter 753, loss = 2103.8686582300734\n",
            "iter 754, loss = 2103.653198996836\n",
            "iter 755, loss = 2103.4377417158394\n",
            "iter 756, loss = 2103.222286387978\n",
            "iter 757, loss = 2103.006833014154\n",
            "iter 758, loss = 2102.7913815952643\n",
            "iter 759, loss = 2102.5759321322116\n",
            "iter 760, loss = 2102.3604846258922\n",
            "iter 761, loss = 2102.145039077211\n",
            "iter 762, loss = 2101.929595487066\n",
            "iter 763, loss = 2101.714153856359\n",
            "iter 764, loss = 2101.4987141859915\n",
            "iter 765, loss = 2101.283276476867\n",
            "iter 766, loss = 2101.067840729888\n",
            "iter 767, loss = 2100.852406945954\n",
            "iter 768, loss = 2100.6369751259717\n",
            "iter 769, loss = 2100.4215452708454\n",
            "iter 770, loss = 2100.2061173814764\n",
            "iter 771, loss = 2099.99069145877\n",
            "iter 772, loss = 2099.7752675036336\n",
            "iter 773, loss = 2099.5598455169697\n",
            "iter 774, loss = 2099.344425499684\n",
            "iter 775, loss = 2099.1290074526855\n",
            "iter 776, loss = 2098.913591376878\n",
            "iter 777, loss = 2098.6981772731683\n",
            "iter 778, loss = 2098.4827651424644\n",
            "iter 779, loss = 2098.267354985677\n",
            "iter 780, loss = 2098.051946803709\n",
            "iter 781, loss = 2097.836540597473\n",
            "iter 782, loss = 2097.621136367875\n",
            "iter 783, loss = 2097.405734115827\n",
            "iter 784, loss = 2097.190333842237\n",
            "iter 785, loss = 2096.9749355480167\n",
            "iter 786, loss = 2096.759539234076\n",
            "iter 787, loss = 2096.5441449013224\n",
            "iter 788, loss = 2096.3287525506735\n",
            "iter 789, loss = 2096.1133621830363\n",
            "iter 790, loss = 2095.8979737993272\n",
            "iter 791, loss = 2095.682587400453\n",
            "iter 792, loss = 2095.467202987333\n",
            "iter 793, loss = 2095.251820560876\n",
            "iter 794, loss = 2095.036440121997\n",
            "iter 795, loss = 2094.8210616716106\n",
            "iter 796, loss = 2094.605685210632\n",
            "iter 797, loss = 2094.3903107399738\n",
            "iter 798, loss = 2094.1749382605562\n",
            "iter 799, loss = 2093.9595677732905\n",
            "iter 800, loss = 2093.7441992790928\n",
            "iter 801, loss = 2093.5288327788835\n",
            "iter 802, loss = 2093.313468273577\n",
            "iter 803, loss = 2093.0981057640925\n",
            "iter 804, loss = 2092.8827452513447\n",
            "iter 805, loss = 2092.667386736254\n",
            "iter 806, loss = 2092.452030219739\n",
            "iter 807, loss = 2092.2366757027185\n",
            "iter 808, loss = 2092.0213231861135\n",
            "iter 809, loss = 2091.8059726708407\n",
            "iter 810, loss = 2091.5906241578223\n",
            "iter 811, loss = 2091.3752776479787\n",
            "iter 812, loss = 2091.159933142232\n",
            "iter 813, loss = 2090.944590641503\n",
            "iter 814, loss = 2090.7292501467136\n",
            "iter 815, loss = 2090.513911658786\n",
            "iter 816, loss = 2090.2985751786423\n",
            "iter 817, loss = 2090.083240707206\n",
            "iter 818, loss = 2089.8679082454023\n",
            "iter 819, loss = 2089.652577794154\n",
            "iter 820, loss = 2089.4372493543847\n",
            "iter 821, loss = 2089.221922927019\n",
            "iter 822, loss = 2089.0065985129845\n",
            "iter 823, loss = 2088.791276113204\n",
            "iter 824, loss = 2088.575955728606\n",
            "iter 825, loss = 2088.3606373601156\n",
            "iter 826, loss = 2088.145321008658\n",
            "iter 827, loss = 2087.9300066751657\n",
            "iter 828, loss = 2087.7146943605585\n",
            "iter 829, loss = 2087.499384065771\n",
            "iter 830, loss = 2087.284075791732\n",
            "iter 831, loss = 2087.0687695393644\n",
            "iter 832, loss = 2086.8534653096017\n",
            "iter 833, loss = 2086.6381631033737\n",
            "iter 834, loss = 2086.4228629216086\n",
            "iter 835, loss = 2086.207564765239\n",
            "iter 836, loss = 2085.9922686351947\n",
            "iter 837, loss = 2085.776974532407\n",
            "iter 838, loss = 2085.5616824578083\n",
            "iter 839, loss = 2085.3463924123316\n",
            "iter 840, loss = 2085.1311043969076\n",
            "iter 841, loss = 2084.91581841247\n",
            "iter 842, loss = 2084.700534459952\n",
            "iter 843, loss = 2084.4852525402885\n",
            "iter 844, loss = 2084.269972654411\n",
            "iter 845, loss = 2084.054694803257\n",
            "iter 846, loss = 2083.839418987761\n",
            "iter 847, loss = 2083.6241452088584\n",
            "iter 848, loss = 2083.408873467485\n",
            "iter 849, loss = 2083.1936037645764\n",
            "iter 850, loss = 2082.97833610107\n",
            "iter 851, loss = 2082.7630704779044\n",
            "iter 852, loss = 2082.5478068960133\n",
            "iter 853, loss = 2082.332545356339\n",
            "iter 854, loss = 2082.117285859817\n",
            "iter 855, loss = 2081.9020284073868\n",
            "iter 856, loss = 2081.6867729999876\n",
            "iter 857, loss = 2081.4715196385605\n",
            "iter 858, loss = 2081.2562683240435\n",
            "iter 859, loss = 2081.0410190573775\n",
            "iter 860, loss = 2080.825771839505\n",
            "iter 861, loss = 2080.610526671364\n",
            "iter 862, loss = 2080.395283553901\n",
            "iter 863, loss = 2080.1800424880535\n",
            "iter 864, loss = 2079.9648034747665\n",
            "iter 865, loss = 2079.7495665149813\n",
            "iter 866, loss = 2079.534331609643\n",
            "iter 867, loss = 2079.3190987596977\n",
            "iter 868, loss = 2079.1038679660815\n",
            "iter 869, loss = 2078.8886392297472\n",
            "iter 870, loss = 2078.673412551636\n",
            "iter 871, loss = 2078.4581879326947\n",
            "iter 872, loss = 2078.242965373869\n",
            "iter 873, loss = 2078.0277448761044\n",
            "iter 874, loss = 2077.812526440348\n",
            "iter 875, loss = 2077.5973100675474\n",
            "iter 876, loss = 2077.3820957586495\n",
            "iter 877, loss = 2077.166883514603\n",
            "iter 878, loss = 2076.9516733363557\n",
            "iter 879, loss = 2076.7364652248575\n",
            "iter 880, loss = 2076.5212591810555\n",
            "iter 881, loss = 2076.3060552058996\n",
            "iter 882, loss = 2076.0908533003417\n",
            "iter 883, loss = 2075.8756534653326\n",
            "iter 884, loss = 2075.6604557018204\n",
            "iter 885, loss = 2075.4452600107593\n",
            "iter 886, loss = 2075.2300663930982\n",
            "iter 887, loss = 2075.0148748497922\n",
            "iter 888, loss = 2074.7996853817913\n",
            "iter 889, loss = 2074.584497990051\n",
            "iter 890, loss = 2074.3693126755225\n",
            "iter 891, loss = 2074.154129439161\n",
            "iter 892, loss = 2073.9389482819215\n",
            "iter 893, loss = 2073.7237692047556\n",
            "iter 894, loss = 2073.5085922086214\n",
            "iter 895, loss = 2073.293417294473\n",
            "iter 896, loss = 2073.0782444632678\n",
            "iter 897, loss = 2072.8630737159597\n",
            "iter 898, loss = 2072.6479050535086\n",
            "iter 899, loss = 2072.4327384768694\n",
            "iter 900, loss = 2072.217573986999\n",
            "iter 901, loss = 2072.0024115848573\n",
            "iter 902, loss = 2071.787251271403\n",
            "iter 903, loss = 2071.5720930475954\n",
            "iter 904, loss = 2071.356936914392\n",
            "iter 905, loss = 2071.141782872753\n",
            "iter 906, loss = 2070.9266309236386\n",
            "iter 907, loss = 2070.7114810680105\n",
            "iter 908, loss = 2070.496333306828\n",
            "iter 909, loss = 2070.281187641056\n",
            "iter 910, loss = 2070.0660440716506\n",
            "iter 911, loss = 2069.8509025995795\n",
            "iter 912, loss = 2069.6357632258027\n",
            "iter 913, loss = 2069.420625951285\n",
            "iter 914, loss = 2069.2054907769875\n",
            "iter 915, loss = 2068.9903577038754\n",
            "iter 916, loss = 2068.7752267329156\n",
            "iter 917, loss = 2068.560097865068\n",
            "iter 918, loss = 2068.3449711013013\n",
            "iter 919, loss = 2068.1298464425818\n",
            "iter 920, loss = 2067.9147238898736\n",
            "iter 921, loss = 2067.6996034441445\n",
            "iter 922, loss = 2067.484485106361\n",
            "iter 923, loss = 2067.2693688774884\n",
            "iter 924, loss = 2067.0542547584987\n",
            "iter 925, loss = 2066.8391427503575\n",
            "iter 926, loss = 2066.624032854032\n",
            "iter 927, loss = 2066.4089250704965\n",
            "iter 928, loss = 2066.1938194007166\n",
            "iter 929, loss = 2065.978715845661\n",
            "iter 930, loss = 2065.763614406304\n",
            "iter 931, loss = 2065.548515083615\n",
            "iter 932, loss = 2065.333417878563\n",
            "iter 933, loss = 2065.118322792123\n",
            "iter 934, loss = 2064.9032298252637\n",
            "iter 935, loss = 2064.688138978961\n",
            "iter 936, loss = 2064.4730502541865\n",
            "iter 937, loss = 2064.257963651913\n",
            "iter 938, loss = 2064.0428791731138\n",
            "iter 939, loss = 2063.8277968187645\n",
            "iter 940, loss = 2063.6127165898397\n",
            "iter 941, loss = 2063.3976384873154\n",
            "iter 942, loss = 2063.182562512164\n",
            "iter 943, loss = 2062.9674886653647\n",
            "iter 944, loss = 2062.752416947891\n",
            "iter 945, loss = 2062.5373473607233\n",
            "iter 946, loss = 2062.3222799048344\n",
            "iter 947, loss = 2062.1072145812063\n",
            "iter 948, loss = 2061.8921513908135\n",
            "iter 949, loss = 2061.6770903346373\n",
            "iter 950, loss = 2061.4620314136555\n",
            "iter 951, loss = 2061.24697462885\n",
            "iter 952, loss = 2061.0319199811947\n",
            "iter 953, loss = 2060.816867471674\n",
            "iter 954, loss = 2060.6018171012697\n",
            "iter 955, loss = 2060.3867688709606\n",
            "iter 956, loss = 2060.1717227817276\n",
            "iter 957, loss = 2059.9566788345555\n",
            "iter 958, loss = 2059.7416370304254\n",
            "iter 959, loss = 2059.5265973703195\n",
            "iter 960, loss = 2059.3115598552213\n",
            "iter 961, loss = 2059.096524486114\n",
            "iter 962, loss = 2058.8814912639837\n",
            "iter 963, loss = 2058.6664601898146\n",
            "iter 964, loss = 2058.4514312645897\n",
            "iter 965, loss = 2058.2364044892956\n",
            "iter 966, loss = 2058.021379864919\n",
            "iter 967, loss = 2057.8063573924446\n",
            "iter 968, loss = 2057.5913370728613\n",
            "iter 969, loss = 2057.376318907154\n",
            "iter 970, loss = 2057.161302896312\n",
            "iter 971, loss = 2056.9462890413233\n",
            "iter 972, loss = 2056.7312773431745\n",
            "iter 973, loss = 2056.5162678028573\n",
            "iter 974, loss = 2056.301260421358\n",
            "iter 975, loss = 2056.0862551996693\n",
            "iter 976, loss = 2055.8712521387793\n",
            "iter 977, loss = 2055.6562512396795\n",
            "iter 978, loss = 2055.4412525033626\n",
            "iter 979, loss = 2055.226255930817\n",
            "iter 980, loss = 2055.011261523036\n",
            "iter 981, loss = 2054.796269281014\n",
            "iter 982, loss = 2054.5812792057413\n",
            "iter 983, loss = 2054.3662912982113\n",
            "iter 984, loss = 2054.151305559419\n",
            "iter 985, loss = 2053.936321990357\n",
            "iter 986, loss = 2053.7213405920224\n",
            "iter 987, loss = 2053.506361365409\n",
            "iter 988, loss = 2053.29138431151\n",
            "iter 989, loss = 2053.0764094313236\n",
            "iter 990, loss = 2052.8614367258465\n",
            "iter 991, loss = 2052.646466196074\n",
            "iter 992, loss = 2052.4314978430043\n",
            "iter 993, loss = 2052.2165316676346\n",
            "iter 994, loss = 2052.001567670964\n",
            "iter 995, loss = 2051.7866058539903\n",
            "iter 996, loss = 2051.571646217712\n",
            "iter 997, loss = 2051.3566887631305\n",
            "iter 998, loss = 2051.1417334912408\n",
            "iter 999, loss = 2050.9267804030487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wR5vI1kaq-A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x8DBm4jofqB"
      },
      "source": [
        "### 2-Layer Neural Network Classifier in Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6xDg4tTSS1_"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX6o6B3WryE-"
      },
      "source": [
        "def softmax(z):\n",
        "  _exp = np.exp(z)\n",
        "  return _exp/np.sum(_exp)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrBYb_J9sqsn",
        "outputId": "d18c7153-7bb0-417c-c463-9836cd3ba01d"
      },
      "source": [
        "# Softmax squishes the array values between [0, 1] such that the sum of all values is 1. \n",
        "# Great for getting class probabilities in a neural network. Historically used in Logistic Regression.  \n",
        "\n",
        "softmax([0.346, 0.283, 0.965])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.26343426, 0.24734988, 0.48921587])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2NmXMYSu8DX"
      },
      "source": [
        "num_train_examples = 900\n",
        "num_features = 20 \n",
        "num_classes = 2 \n",
        "\n",
        "# let's make some data \n",
        "X_train = tf.random.normal((num_train_examples, num_features), dtype=tf.float32)\n",
        "y_train_cat = tf.random.categorical(tf.math.log([[0.5, 0.5]]), num_train_examples)\n",
        "y_train_cat = tf.reshape(y_train_cat, [num_train_examples])"
      ],
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sucMEu7pSopL"
      },
      "source": [
        "num_units_1 = 20 \n",
        "num_units_2 = 10 \n",
        "num_out = 2 \n",
        "\n",
        "def init_weights(num_features, num_units):\n",
        "  w_init = tf.random_normal_initializer()\n",
        "  b_init = tf.zeros_initializer()\n",
        "\n",
        "  W = tf.Variable(initial_value=w_init(shape=(num_features, num_units)), trainable=True)\n",
        "  b = tf.Variable(initial_value=b_init(shape=(num_units)), trainable=True)\n",
        "\n",
        "  return W, b\n",
        "\n",
        "def linear_layer(X, W, b):\n",
        "  y = tf.matmul(X, W) + b  # You could also write X @ W for dot product.  \n",
        "  return y\n",
        "\n",
        "# Initialize the weights and biases\n",
        "Wh1, bh1 = init_weights(num_features, num_units_1)\n",
        "Wh2, bh2 = init_weights(num_units_1, num_units_2)\n",
        "Wo, bo = init_weights(num_units_2, num_out)\n",
        "\n",
        "\n",
        "def feed_forward_2_layer_nn(X, trainable_weights):\n",
        "  Wh1, bh1, Wh2, bh2, Wo, bo = trainable_weights\n",
        "\n",
        "  linear_1 = linear_layer(X, Wh1, bh1)\n",
        "  # print(\"Shape linear_1\", linear_1.shape)\n",
        "\n",
        "  activation_1 = tf.nn.relu(linear_1)\n",
        "  # print(\"Shape activation_1\", activation_1.shape)\n",
        "\n",
        "  linear_2 = linear_layer(activation_1, Wh2, bh2)\n",
        "  # print(\"Shape linear_2\", linear_2.shape)\n",
        "  \n",
        "  activation_2 = tf.nn.relu(linear_2)\n",
        "  # print(\"Shape activation_2\", activation_2.shape)\n",
        "\n",
        "  out = linear_layer(activation_2, Wo, bo)\n",
        "  # print(\"Shape linear_out\", out.shape)\n",
        "\n",
        "  return out"
      ],
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTFGDluKIW44",
        "outputId": "134f31a5-ae6e-4bed-ba3c-b9eb73254044"
      },
      "source": [
        "# Let's test the feed forward with some data.\n",
        "train_weights = Wh1, bh1, Wh2, bh2, Wo, bo \n",
        "y_hat = feed_forward_2_layer_nn(X_train, train_weights)\n",
        "logits = tf.nn.softmax(y_hat)\n",
        "preds = tf.reduce_max(logits, axis=1)\n",
        "\n",
        "print(preds[:10])\n",
        "print(y_train_cat[:10])"
      ],
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[0.5000326  0.50078565 0.5015442  0.5007061  0.50062114 0.5000519\n",
            " 0.5006972  0.501736   0.5010477  0.501371  ], shape=(10,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]], shape=(10, 1), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_UFIWzBBiXv"
      },
      "source": [
        "### Primer on Tensorflow `GradientTape`\n",
        "\n",
        "Remember above we hand calculated all the derivatives of the hidden layers. It was a pretty heavy computation for a single layer! With GradientTape, we can pass all the weights and biases we want to calculate the gradients for the loss function. It does the job. Let's see a simple MLP and how the gradient is calculated.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK7gIpuOI3TQ"
      },
      "source": [
        "y_train = np.random.random((num_train_examples, num_out))\n",
        "\n",
        "Wh, bh = init_weights(num_features, num_units_1)\n",
        "Wout, bout = init_weights(num_units_1, num_out)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  zh = X_train@Wh + bh\n",
        "  h = tf.nn.relu(zh)\n",
        "  y_hat = h@Wout + bout\n",
        "  loss = tf.reduce_mean((y_hat - y_train)**2)\n",
        "\n",
        "grad = tape.gradient(loss, [Wh, bh, Wout, bout])\n",
        "grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GBSOxtpC-h_"
      },
      "source": [
        "### Training our 2-layer classifier with `GradientTape` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8zbjKcILz0I"
      },
      "source": [
        "from copy import deepcopy"
      ],
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G7wQJ0A6UP9",
        "outputId": "73e52be0-d01f-4b40-8f3b-5d309d0446c5"
      },
      "source": [
        "lr = 0.0001\n",
        "losses = []\n",
        "N = len(y_train)\n",
        "trainable_weights = Wh1, bh1, Wh2, bh2, Wo, bo\n",
        "\n",
        "\n",
        "def optimizer_sgd(g, w, learning_rate):\n",
        "  updated_weights = []\n",
        "  for _g, _w in zip(g, w):\n",
        "    _w = _w - tf.scalar_mul(learning_rate, _g)\n",
        "    updated_weights.append(_w)\n",
        "  return updated_weights \n",
        "\n",
        "for i in range(num_iterations):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(trainable_weights)\n",
        "\n",
        "    out = feed_forward_2_layer_nn(X_train, trainable_weights)\n",
        "    logits = tf.nn.softmax(out)\n",
        "    # y_hat = tf.argmax(logits, axis=1)\n",
        "    # preds = tf.reduce_max(logits, axis=1)\n",
        "    # y_hat = tf.reshape(preds, [num_train_examples, 1])\n",
        "    # loss = y_train_cat @ tf.math.log(y_hat) + (1 - y_train_cat) @ tf.math.log(1 - y_hat)\n",
        "    \n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(y_train_cat, logits)\n",
        "\n",
        "  grad = tape.gradient(loss, trainable_weights)\n",
        "  grad = deepcopy(grad)\n",
        "\n",
        "  trainable_weights = optimizer_sgd(grad, trainable_weights, lr)\n",
        "\n",
        "  mean_loss = tf.reduce_mean(loss)\n",
        "  losses.append(mean_loss)\n",
        "\n",
        "  print(\"iter %s, loss = %s\" % (i, mean_loss))\n"
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss = tf.Tensor(0.69316626, shape=(), dtype=float32)\n",
            "iter 1, loss = tf.Tensor(0.6930724, shape=(), dtype=float32)\n",
            "iter 2, loss = tf.Tensor(0.6929805, shape=(), dtype=float32)\n",
            "iter 3, loss = tf.Tensor(0.69289094, shape=(), dtype=float32)\n",
            "iter 4, loss = tf.Tensor(0.69280326, shape=(), dtype=float32)\n",
            "iter 5, loss = tf.Tensor(0.6927176, shape=(), dtype=float32)\n",
            "iter 6, loss = tf.Tensor(0.6926339, shape=(), dtype=float32)\n",
            "iter 7, loss = tf.Tensor(0.6925521, shape=(), dtype=float32)\n",
            "iter 8, loss = tf.Tensor(0.6924722, shape=(), dtype=float32)\n",
            "iter 9, loss = tf.Tensor(0.692394, shape=(), dtype=float32)\n",
            "iter 10, loss = tf.Tensor(0.6923177, shape=(), dtype=float32)\n",
            "iter 11, loss = tf.Tensor(0.6922431, shape=(), dtype=float32)\n",
            "iter 12, loss = tf.Tensor(0.6921702, shape=(), dtype=float32)\n",
            "iter 13, loss = tf.Tensor(0.6920989, shape=(), dtype=float32)\n",
            "iter 14, loss = tf.Tensor(0.6920293, shape=(), dtype=float32)\n",
            "iter 15, loss = tf.Tensor(0.6919613, shape=(), dtype=float32)\n",
            "iter 16, loss = tf.Tensor(0.6918947, shape=(), dtype=float32)\n",
            "iter 17, loss = tf.Tensor(0.69182974, shape=(), dtype=float32)\n",
            "iter 18, loss = tf.Tensor(0.69176626, shape=(), dtype=float32)\n",
            "iter 19, loss = tf.Tensor(0.69170415, shape=(), dtype=float32)\n",
            "iter 20, loss = tf.Tensor(0.6916434, shape=(), dtype=float32)\n",
            "iter 21, loss = tf.Tensor(0.6915841, shape=(), dtype=float32)\n",
            "iter 22, loss = tf.Tensor(0.6915263, shape=(), dtype=float32)\n",
            "iter 23, loss = tf.Tensor(0.69146967, shape=(), dtype=float32)\n",
            "iter 24, loss = tf.Tensor(0.69141424, shape=(), dtype=float32)\n",
            "iter 25, loss = tf.Tensor(0.69136006, shape=(), dtype=float32)\n",
            "iter 26, loss = tf.Tensor(0.6913071, shape=(), dtype=float32)\n",
            "iter 27, loss = tf.Tensor(0.69125533, shape=(), dtype=float32)\n",
            "iter 28, loss = tf.Tensor(0.6912048, shape=(), dtype=float32)\n",
            "iter 29, loss = tf.Tensor(0.6911554, shape=(), dtype=float32)\n",
            "iter 30, loss = tf.Tensor(0.6911071, shape=(), dtype=float32)\n",
            "iter 31, loss = tf.Tensor(0.6910598, shape=(), dtype=float32)\n",
            "iter 32, loss = tf.Tensor(0.69101375, shape=(), dtype=float32)\n",
            "iter 33, loss = tf.Tensor(0.6909686, shape=(), dtype=float32)\n",
            "iter 34, loss = tf.Tensor(0.69092435, shape=(), dtype=float32)\n",
            "iter 35, loss = tf.Tensor(0.6908812, shape=(), dtype=float32)\n",
            "iter 36, loss = tf.Tensor(0.6908391, shape=(), dtype=float32)\n",
            "iter 37, loss = tf.Tensor(0.6907978, shape=(), dtype=float32)\n",
            "iter 38, loss = tf.Tensor(0.6907575, shape=(), dtype=float32)\n",
            "iter 39, loss = tf.Tensor(0.6907182, shape=(), dtype=float32)\n",
            "iter 40, loss = tf.Tensor(0.6906796, shape=(), dtype=float32)\n",
            "iter 41, loss = tf.Tensor(0.690642, shape=(), dtype=float32)\n",
            "iter 42, loss = tf.Tensor(0.69060504, shape=(), dtype=float32)\n",
            "iter 43, loss = tf.Tensor(0.6905691, shape=(), dtype=float32)\n",
            "iter 44, loss = tf.Tensor(0.6905341, shape=(), dtype=float32)\n",
            "iter 45, loss = tf.Tensor(0.69049966, shape=(), dtype=float32)\n",
            "iter 46, loss = tf.Tensor(0.6904661, shape=(), dtype=float32)\n",
            "iter 47, loss = tf.Tensor(0.6904332, shape=(), dtype=float32)\n",
            "iter 48, loss = tf.Tensor(0.6904011, shape=(), dtype=float32)\n",
            "iter 49, loss = tf.Tensor(0.6903697, shape=(), dtype=float32)\n",
            "iter 50, loss = tf.Tensor(0.6903391, shape=(), dtype=float32)\n",
            "iter 51, loss = tf.Tensor(0.69030905, shape=(), dtype=float32)\n",
            "iter 52, loss = tf.Tensor(0.69027984, shape=(), dtype=float32)\n",
            "iter 53, loss = tf.Tensor(0.6902511, shape=(), dtype=float32)\n",
            "iter 54, loss = tf.Tensor(0.6902231, shape=(), dtype=float32)\n",
            "iter 55, loss = tf.Tensor(0.6901958, shape=(), dtype=float32)\n",
            "iter 56, loss = tf.Tensor(0.690169, shape=(), dtype=float32)\n",
            "iter 57, loss = tf.Tensor(0.69014287, shape=(), dtype=float32)\n",
            "iter 58, loss = tf.Tensor(0.6901174, shape=(), dtype=float32)\n",
            "iter 59, loss = tf.Tensor(0.6900923, shape=(), dtype=float32)\n",
            "iter 60, loss = tf.Tensor(0.6900679, shape=(), dtype=float32)\n",
            "iter 61, loss = tf.Tensor(0.6900439, shape=(), dtype=float32)\n",
            "iter 62, loss = tf.Tensor(0.6900206, shape=(), dtype=float32)\n",
            "iter 63, loss = tf.Tensor(0.6899977, shape=(), dtype=float32)\n",
            "iter 64, loss = tf.Tensor(0.6899754, shape=(), dtype=float32)\n",
            "iter 65, loss = tf.Tensor(0.6899535, shape=(), dtype=float32)\n",
            "iter 66, loss = tf.Tensor(0.68993217, shape=(), dtype=float32)\n",
            "iter 67, loss = tf.Tensor(0.68991125, shape=(), dtype=float32)\n",
            "iter 68, loss = tf.Tensor(0.68989086, shape=(), dtype=float32)\n",
            "iter 69, loss = tf.Tensor(0.68987095, shape=(), dtype=float32)\n",
            "iter 70, loss = tf.Tensor(0.68985146, shape=(), dtype=float32)\n",
            "iter 71, loss = tf.Tensor(0.68983245, shape=(), dtype=float32)\n",
            "iter 72, loss = tf.Tensor(0.68981385, shape=(), dtype=float32)\n",
            "iter 73, loss = tf.Tensor(0.6897956, shape=(), dtype=float32)\n",
            "iter 74, loss = tf.Tensor(0.6897777, shape=(), dtype=float32)\n",
            "iter 75, loss = tf.Tensor(0.6897604, shape=(), dtype=float32)\n",
            "iter 76, loss = tf.Tensor(0.68974334, shape=(), dtype=float32)\n",
            "iter 77, loss = tf.Tensor(0.6897267, shape=(), dtype=float32)\n",
            "iter 78, loss = tf.Tensor(0.6897104, shape=(), dtype=float32)\n",
            "iter 79, loss = tf.Tensor(0.6896944, shape=(), dtype=float32)\n",
            "iter 80, loss = tf.Tensor(0.6896787, shape=(), dtype=float32)\n",
            "iter 81, loss = tf.Tensor(0.6896636, shape=(), dtype=float32)\n",
            "iter 82, loss = tf.Tensor(0.6896487, shape=(), dtype=float32)\n",
            "iter 83, loss = tf.Tensor(0.68963414, shape=(), dtype=float32)\n",
            "iter 84, loss = tf.Tensor(0.68961984, shape=(), dtype=float32)\n",
            "iter 85, loss = tf.Tensor(0.6896059, shape=(), dtype=float32)\n",
            "iter 86, loss = tf.Tensor(0.68959236, shape=(), dtype=float32)\n",
            "iter 87, loss = tf.Tensor(0.68957907, shape=(), dtype=float32)\n",
            "iter 88, loss = tf.Tensor(0.68956596, shape=(), dtype=float32)\n",
            "iter 89, loss = tf.Tensor(0.68955326, shape=(), dtype=float32)\n",
            "iter 90, loss = tf.Tensor(0.68954074, shape=(), dtype=float32)\n",
            "iter 91, loss = tf.Tensor(0.68952864, shape=(), dtype=float32)\n",
            "iter 92, loss = tf.Tensor(0.68951666, shape=(), dtype=float32)\n",
            "iter 93, loss = tf.Tensor(0.689505, shape=(), dtype=float32)\n",
            "iter 94, loss = tf.Tensor(0.6894936, shape=(), dtype=float32)\n",
            "iter 95, loss = tf.Tensor(0.68948257, shape=(), dtype=float32)\n",
            "iter 96, loss = tf.Tensor(0.68947166, shape=(), dtype=float32)\n",
            "iter 97, loss = tf.Tensor(0.68946093, shape=(), dtype=float32)\n",
            "iter 98, loss = tf.Tensor(0.6894505, shape=(), dtype=float32)\n",
            "iter 99, loss = tf.Tensor(0.6894403, shape=(), dtype=float32)\n",
            "iter 100, loss = tf.Tensor(0.68943036, shape=(), dtype=float32)\n",
            "iter 101, loss = tf.Tensor(0.68942064, shape=(), dtype=float32)\n",
            "iter 102, loss = tf.Tensor(0.689411, shape=(), dtype=float32)\n",
            "iter 103, loss = tf.Tensor(0.68940175, shape=(), dtype=float32)\n",
            "iter 104, loss = tf.Tensor(0.68939257, shape=(), dtype=float32)\n",
            "iter 105, loss = tf.Tensor(0.6893836, shape=(), dtype=float32)\n",
            "iter 106, loss = tf.Tensor(0.68937486, shape=(), dtype=float32)\n",
            "iter 107, loss = tf.Tensor(0.68936634, shape=(), dtype=float32)\n",
            "iter 108, loss = tf.Tensor(0.68935794, shape=(), dtype=float32)\n",
            "iter 109, loss = tf.Tensor(0.6893498, shape=(), dtype=float32)\n",
            "iter 110, loss = tf.Tensor(0.6893418, shape=(), dtype=float32)\n",
            "iter 111, loss = tf.Tensor(0.6893339, shape=(), dtype=float32)\n",
            "iter 112, loss = tf.Tensor(0.6893262, shape=(), dtype=float32)\n",
            "iter 113, loss = tf.Tensor(0.6893187, shape=(), dtype=float32)\n",
            "iter 114, loss = tf.Tensor(0.6893113, shape=(), dtype=float32)\n",
            "iter 115, loss = tf.Tensor(0.6893041, shape=(), dtype=float32)\n",
            "iter 116, loss = tf.Tensor(0.6892971, shape=(), dtype=float32)\n",
            "iter 117, loss = tf.Tensor(0.68929017, shape=(), dtype=float32)\n",
            "iter 118, loss = tf.Tensor(0.68928343, shape=(), dtype=float32)\n",
            "iter 119, loss = tf.Tensor(0.6892769, shape=(), dtype=float32)\n",
            "iter 120, loss = tf.Tensor(0.68927044, shape=(), dtype=float32)\n",
            "iter 121, loss = tf.Tensor(0.68926406, shape=(), dtype=float32)\n",
            "iter 122, loss = tf.Tensor(0.6892578, shape=(), dtype=float32)\n",
            "iter 123, loss = tf.Tensor(0.6892518, shape=(), dtype=float32)\n",
            "iter 124, loss = tf.Tensor(0.6892458, shape=(), dtype=float32)\n",
            "iter 125, loss = tf.Tensor(0.68924, shape=(), dtype=float32)\n",
            "iter 126, loss = tf.Tensor(0.68923426, shape=(), dtype=float32)\n",
            "iter 127, loss = tf.Tensor(0.68922865, shape=(), dtype=float32)\n",
            "iter 128, loss = tf.Tensor(0.68922323, shape=(), dtype=float32)\n",
            "iter 129, loss = tf.Tensor(0.6892178, shape=(), dtype=float32)\n",
            "iter 130, loss = tf.Tensor(0.6892126, shape=(), dtype=float32)\n",
            "iter 131, loss = tf.Tensor(0.6892075, shape=(), dtype=float32)\n",
            "iter 132, loss = tf.Tensor(0.6892025, shape=(), dtype=float32)\n",
            "iter 133, loss = tf.Tensor(0.68919754, shape=(), dtype=float32)\n",
            "iter 134, loss = tf.Tensor(0.68919265, shape=(), dtype=float32)\n",
            "iter 135, loss = tf.Tensor(0.68918794, shape=(), dtype=float32)\n",
            "iter 136, loss = tf.Tensor(0.68918335, shape=(), dtype=float32)\n",
            "iter 137, loss = tf.Tensor(0.6891788, shape=(), dtype=float32)\n",
            "iter 138, loss = tf.Tensor(0.6891744, shape=(), dtype=float32)\n",
            "iter 139, loss = tf.Tensor(0.6891701, shape=(), dtype=float32)\n",
            "iter 140, loss = tf.Tensor(0.6891658, shape=(), dtype=float32)\n",
            "iter 141, loss = tf.Tensor(0.6891616, shape=(), dtype=float32)\n",
            "iter 142, loss = tf.Tensor(0.6891575, shape=(), dtype=float32)\n",
            "iter 143, loss = tf.Tensor(0.6891535, shape=(), dtype=float32)\n",
            "iter 144, loss = tf.Tensor(0.68914956, shape=(), dtype=float32)\n",
            "iter 145, loss = tf.Tensor(0.6891456, shape=(), dtype=float32)\n",
            "iter 146, loss = tf.Tensor(0.6891419, shape=(), dtype=float32)\n",
            "iter 147, loss = tf.Tensor(0.68913823, shape=(), dtype=float32)\n",
            "iter 148, loss = tf.Tensor(0.68913466, shape=(), dtype=float32)\n",
            "iter 149, loss = tf.Tensor(0.6891311, shape=(), dtype=float32)\n",
            "iter 150, loss = tf.Tensor(0.6891276, shape=(), dtype=float32)\n",
            "iter 151, loss = tf.Tensor(0.6891242, shape=(), dtype=float32)\n",
            "iter 152, loss = tf.Tensor(0.6891209, shape=(), dtype=float32)\n",
            "iter 153, loss = tf.Tensor(0.68911755, shape=(), dtype=float32)\n",
            "iter 154, loss = tf.Tensor(0.6891144, shape=(), dtype=float32)\n",
            "iter 155, loss = tf.Tensor(0.68911123, shape=(), dtype=float32)\n",
            "iter 156, loss = tf.Tensor(0.68910813, shape=(), dtype=float32)\n",
            "iter 157, loss = tf.Tensor(0.68910515, shape=(), dtype=float32)\n",
            "iter 158, loss = tf.Tensor(0.68910223, shape=(), dtype=float32)\n",
            "iter 159, loss = tf.Tensor(0.6890993, shape=(), dtype=float32)\n",
            "iter 160, loss = tf.Tensor(0.68909645, shape=(), dtype=float32)\n",
            "iter 161, loss = tf.Tensor(0.68909377, shape=(), dtype=float32)\n",
            "iter 162, loss = tf.Tensor(0.689091, shape=(), dtype=float32)\n",
            "iter 163, loss = tf.Tensor(0.68908834, shape=(), dtype=float32)\n",
            "iter 164, loss = tf.Tensor(0.6890857, shape=(), dtype=float32)\n",
            "iter 165, loss = tf.Tensor(0.6890833, shape=(), dtype=float32)\n",
            "iter 166, loss = tf.Tensor(0.6890806, shape=(), dtype=float32)\n",
            "iter 167, loss = tf.Tensor(0.6890782, shape=(), dtype=float32)\n",
            "iter 168, loss = tf.Tensor(0.6890759, shape=(), dtype=float32)\n",
            "iter 169, loss = tf.Tensor(0.6890735, shape=(), dtype=float32)\n",
            "iter 170, loss = tf.Tensor(0.68907106, shape=(), dtype=float32)\n",
            "iter 171, loss = tf.Tensor(0.6890688, shape=(), dtype=float32)\n",
            "iter 172, loss = tf.Tensor(0.6890666, shape=(), dtype=float32)\n",
            "iter 173, loss = tf.Tensor(0.6890644, shape=(), dtype=float32)\n",
            "iter 174, loss = tf.Tensor(0.68906224, shape=(), dtype=float32)\n",
            "iter 175, loss = tf.Tensor(0.68906003, shape=(), dtype=float32)\n",
            "iter 176, loss = tf.Tensor(0.689058, shape=(), dtype=float32)\n",
            "iter 177, loss = tf.Tensor(0.68905604, shape=(), dtype=float32)\n",
            "iter 178, loss = tf.Tensor(0.689054, shape=(), dtype=float32)\n",
            "iter 179, loss = tf.Tensor(0.68905205, shape=(), dtype=float32)\n",
            "iter 180, loss = tf.Tensor(0.68905014, shape=(), dtype=float32)\n",
            "iter 181, loss = tf.Tensor(0.68904835, shape=(), dtype=float32)\n",
            "iter 182, loss = tf.Tensor(0.6890465, shape=(), dtype=float32)\n",
            "iter 183, loss = tf.Tensor(0.6890446, shape=(), dtype=float32)\n",
            "iter 184, loss = tf.Tensor(0.6890429, shape=(), dtype=float32)\n",
            "iter 185, loss = tf.Tensor(0.68904114, shape=(), dtype=float32)\n",
            "iter 186, loss = tf.Tensor(0.68903947, shape=(), dtype=float32)\n",
            "iter 187, loss = tf.Tensor(0.6890377, shape=(), dtype=float32)\n",
            "iter 188, loss = tf.Tensor(0.68903613, shape=(), dtype=float32)\n",
            "iter 189, loss = tf.Tensor(0.6890344, shape=(), dtype=float32)\n",
            "iter 190, loss = tf.Tensor(0.6890328, shape=(), dtype=float32)\n",
            "iter 191, loss = tf.Tensor(0.6890313, shape=(), dtype=float32)\n",
            "iter 192, loss = tf.Tensor(0.6890298, shape=(), dtype=float32)\n",
            "iter 193, loss = tf.Tensor(0.68902826, shape=(), dtype=float32)\n",
            "iter 194, loss = tf.Tensor(0.6890268, shape=(), dtype=float32)\n",
            "iter 195, loss = tf.Tensor(0.68902534, shape=(), dtype=float32)\n",
            "iter 196, loss = tf.Tensor(0.6890239, shape=(), dtype=float32)\n",
            "iter 197, loss = tf.Tensor(0.6890225, shape=(), dtype=float32)\n",
            "iter 198, loss = tf.Tensor(0.6890211, shape=(), dtype=float32)\n",
            "iter 199, loss = tf.Tensor(0.6890198, shape=(), dtype=float32)\n",
            "iter 200, loss = tf.Tensor(0.6890184, shape=(), dtype=float32)\n",
            "iter 201, loss = tf.Tensor(0.68901706, shape=(), dtype=float32)\n",
            "iter 202, loss = tf.Tensor(0.68901575, shape=(), dtype=float32)\n",
            "iter 203, loss = tf.Tensor(0.68901455, shape=(), dtype=float32)\n",
            "iter 204, loss = tf.Tensor(0.68901336, shape=(), dtype=float32)\n",
            "iter 205, loss = tf.Tensor(0.68901205, shape=(), dtype=float32)\n",
            "iter 206, loss = tf.Tensor(0.6890108, shape=(), dtype=float32)\n",
            "iter 207, loss = tf.Tensor(0.68900967, shape=(), dtype=float32)\n",
            "iter 208, loss = tf.Tensor(0.6890086, shape=(), dtype=float32)\n",
            "iter 209, loss = tf.Tensor(0.68900734, shape=(), dtype=float32)\n",
            "iter 210, loss = tf.Tensor(0.68900627, shape=(), dtype=float32)\n",
            "iter 211, loss = tf.Tensor(0.6890051, shape=(), dtype=float32)\n",
            "iter 212, loss = tf.Tensor(0.68900406, shape=(), dtype=float32)\n",
            "iter 213, loss = tf.Tensor(0.68900293, shape=(), dtype=float32)\n",
            "iter 214, loss = tf.Tensor(0.68900186, shape=(), dtype=float32)\n",
            "iter 215, loss = tf.Tensor(0.68900084, shape=(), dtype=float32)\n",
            "iter 216, loss = tf.Tensor(0.68899983, shape=(), dtype=float32)\n",
            "iter 217, loss = tf.Tensor(0.68899876, shape=(), dtype=float32)\n",
            "iter 218, loss = tf.Tensor(0.68899775, shape=(), dtype=float32)\n",
            "iter 219, loss = tf.Tensor(0.68899685, shape=(), dtype=float32)\n",
            "iter 220, loss = tf.Tensor(0.6889958, shape=(), dtype=float32)\n",
            "iter 221, loss = tf.Tensor(0.6889949, shape=(), dtype=float32)\n",
            "iter 222, loss = tf.Tensor(0.688994, shape=(), dtype=float32)\n",
            "iter 223, loss = tf.Tensor(0.688993, shape=(), dtype=float32)\n",
            "iter 224, loss = tf.Tensor(0.6889921, shape=(), dtype=float32)\n",
            "iter 225, loss = tf.Tensor(0.68899125, shape=(), dtype=float32)\n",
            "iter 226, loss = tf.Tensor(0.6889903, shape=(), dtype=float32)\n",
            "iter 227, loss = tf.Tensor(0.68898946, shape=(), dtype=float32)\n",
            "iter 228, loss = tf.Tensor(0.6889885, shape=(), dtype=float32)\n",
            "iter 229, loss = tf.Tensor(0.6889876, shape=(), dtype=float32)\n",
            "iter 230, loss = tf.Tensor(0.68898684, shape=(), dtype=float32)\n",
            "iter 231, loss = tf.Tensor(0.688986, shape=(), dtype=float32)\n",
            "iter 232, loss = tf.Tensor(0.68898517, shape=(), dtype=float32)\n",
            "iter 233, loss = tf.Tensor(0.6889844, shape=(), dtype=float32)\n",
            "iter 234, loss = tf.Tensor(0.68898356, shape=(), dtype=float32)\n",
            "iter 235, loss = tf.Tensor(0.6889827, shape=(), dtype=float32)\n",
            "iter 236, loss = tf.Tensor(0.68898195, shape=(), dtype=float32)\n",
            "iter 237, loss = tf.Tensor(0.68898124, shape=(), dtype=float32)\n",
            "iter 238, loss = tf.Tensor(0.68898046, shape=(), dtype=float32)\n",
            "iter 239, loss = tf.Tensor(0.6889797, shape=(), dtype=float32)\n",
            "iter 240, loss = tf.Tensor(0.68897897, shape=(), dtype=float32)\n",
            "iter 241, loss = tf.Tensor(0.68897825, shape=(), dtype=float32)\n",
            "iter 242, loss = tf.Tensor(0.6889775, shape=(), dtype=float32)\n",
            "iter 243, loss = tf.Tensor(0.68897676, shape=(), dtype=float32)\n",
            "iter 244, loss = tf.Tensor(0.68897605, shape=(), dtype=float32)\n",
            "iter 245, loss = tf.Tensor(0.68897533, shape=(), dtype=float32)\n",
            "iter 246, loss = tf.Tensor(0.6889746, shape=(), dtype=float32)\n",
            "iter 247, loss = tf.Tensor(0.688974, shape=(), dtype=float32)\n",
            "iter 248, loss = tf.Tensor(0.6889732, shape=(), dtype=float32)\n",
            "iter 249, loss = tf.Tensor(0.6889726, shape=(), dtype=float32)\n",
            "iter 250, loss = tf.Tensor(0.6889718, shape=(), dtype=float32)\n",
            "iter 251, loss = tf.Tensor(0.68897116, shape=(), dtype=float32)\n",
            "iter 252, loss = tf.Tensor(0.68897045, shape=(), dtype=float32)\n",
            "iter 253, loss = tf.Tensor(0.6889699, shape=(), dtype=float32)\n",
            "iter 254, loss = tf.Tensor(0.68896925, shape=(), dtype=float32)\n",
            "iter 255, loss = tf.Tensor(0.6889686, shape=(), dtype=float32)\n",
            "iter 256, loss = tf.Tensor(0.68896794, shape=(), dtype=float32)\n",
            "iter 257, loss = tf.Tensor(0.6889674, shape=(), dtype=float32)\n",
            "iter 258, loss = tf.Tensor(0.6889668, shape=(), dtype=float32)\n",
            "iter 259, loss = tf.Tensor(0.68896616, shape=(), dtype=float32)\n",
            "iter 260, loss = tf.Tensor(0.6889656, shape=(), dtype=float32)\n",
            "iter 261, loss = tf.Tensor(0.68896496, shape=(), dtype=float32)\n",
            "iter 262, loss = tf.Tensor(0.68896437, shape=(), dtype=float32)\n",
            "iter 263, loss = tf.Tensor(0.68896383, shape=(), dtype=float32)\n",
            "iter 264, loss = tf.Tensor(0.6889633, shape=(), dtype=float32)\n",
            "iter 265, loss = tf.Tensor(0.6889627, shape=(), dtype=float32)\n",
            "iter 266, loss = tf.Tensor(0.68896204, shape=(), dtype=float32)\n",
            "iter 267, loss = tf.Tensor(0.68896157, shape=(), dtype=float32)\n",
            "iter 268, loss = tf.Tensor(0.6889609, shape=(), dtype=float32)\n",
            "iter 269, loss = tf.Tensor(0.68896043, shape=(), dtype=float32)\n",
            "iter 270, loss = tf.Tensor(0.68895984, shape=(), dtype=float32)\n",
            "iter 271, loss = tf.Tensor(0.6889593, shape=(), dtype=float32)\n",
            "iter 272, loss = tf.Tensor(0.6889588, shape=(), dtype=float32)\n",
            "iter 273, loss = tf.Tensor(0.6889581, shape=(), dtype=float32)\n",
            "iter 274, loss = tf.Tensor(0.68895763, shape=(), dtype=float32)\n",
            "iter 275, loss = tf.Tensor(0.6889571, shape=(), dtype=float32)\n",
            "iter 276, loss = tf.Tensor(0.68895656, shape=(), dtype=float32)\n",
            "iter 277, loss = tf.Tensor(0.688956, shape=(), dtype=float32)\n",
            "iter 278, loss = tf.Tensor(0.68895555, shape=(), dtype=float32)\n",
            "iter 279, loss = tf.Tensor(0.688955, shape=(), dtype=float32)\n",
            "iter 280, loss = tf.Tensor(0.6889545, shape=(), dtype=float32)\n",
            "iter 281, loss = tf.Tensor(0.688954, shape=(), dtype=float32)\n",
            "iter 282, loss = tf.Tensor(0.6889535, shape=(), dtype=float32)\n",
            "iter 283, loss = tf.Tensor(0.688953, shape=(), dtype=float32)\n",
            "iter 284, loss = tf.Tensor(0.6889525, shape=(), dtype=float32)\n",
            "iter 285, loss = tf.Tensor(0.6889519, shape=(), dtype=float32)\n",
            "iter 286, loss = tf.Tensor(0.68895143, shape=(), dtype=float32)\n",
            "iter 287, loss = tf.Tensor(0.688951, shape=(), dtype=float32)\n",
            "iter 288, loss = tf.Tensor(0.68895054, shape=(), dtype=float32)\n",
            "iter 289, loss = tf.Tensor(0.68895006, shape=(), dtype=float32)\n",
            "iter 290, loss = tf.Tensor(0.6889496, shape=(), dtype=float32)\n",
            "iter 291, loss = tf.Tensor(0.6889491, shape=(), dtype=float32)\n",
            "iter 292, loss = tf.Tensor(0.6889487, shape=(), dtype=float32)\n",
            "iter 293, loss = tf.Tensor(0.6889482, shape=(), dtype=float32)\n",
            "iter 294, loss = tf.Tensor(0.6889477, shape=(), dtype=float32)\n",
            "iter 295, loss = tf.Tensor(0.6889472, shape=(), dtype=float32)\n",
            "iter 296, loss = tf.Tensor(0.6889467, shape=(), dtype=float32)\n",
            "iter 297, loss = tf.Tensor(0.6889462, shape=(), dtype=float32)\n",
            "iter 298, loss = tf.Tensor(0.6889458, shape=(), dtype=float32)\n",
            "iter 299, loss = tf.Tensor(0.6889453, shape=(), dtype=float32)\n",
            "iter 300, loss = tf.Tensor(0.6889449, shape=(), dtype=float32)\n",
            "iter 301, loss = tf.Tensor(0.6889443, shape=(), dtype=float32)\n",
            "iter 302, loss = tf.Tensor(0.688944, shape=(), dtype=float32)\n",
            "iter 303, loss = tf.Tensor(0.68894356, shape=(), dtype=float32)\n",
            "iter 304, loss = tf.Tensor(0.6889431, shape=(), dtype=float32)\n",
            "iter 305, loss = tf.Tensor(0.6889426, shape=(), dtype=float32)\n",
            "iter 306, loss = tf.Tensor(0.6889422, shape=(), dtype=float32)\n",
            "iter 307, loss = tf.Tensor(0.6889418, shape=(), dtype=float32)\n",
            "iter 308, loss = tf.Tensor(0.6889413, shape=(), dtype=float32)\n",
            "iter 309, loss = tf.Tensor(0.6889408, shape=(), dtype=float32)\n",
            "iter 310, loss = tf.Tensor(0.6889405, shape=(), dtype=float32)\n",
            "iter 311, loss = tf.Tensor(0.68894005, shape=(), dtype=float32)\n",
            "iter 312, loss = tf.Tensor(0.6889396, shape=(), dtype=float32)\n",
            "iter 313, loss = tf.Tensor(0.6889391, shape=(), dtype=float32)\n",
            "iter 314, loss = tf.Tensor(0.6889388, shape=(), dtype=float32)\n",
            "iter 315, loss = tf.Tensor(0.6889384, shape=(), dtype=float32)\n",
            "iter 316, loss = tf.Tensor(0.68893796, shape=(), dtype=float32)\n",
            "iter 317, loss = tf.Tensor(0.68893766, shape=(), dtype=float32)\n",
            "iter 318, loss = tf.Tensor(0.6889372, shape=(), dtype=float32)\n",
            "iter 319, loss = tf.Tensor(0.68893665, shape=(), dtype=float32)\n",
            "iter 320, loss = tf.Tensor(0.6889364, shape=(), dtype=float32)\n",
            "iter 321, loss = tf.Tensor(0.6889359, shape=(), dtype=float32)\n",
            "iter 322, loss = tf.Tensor(0.68893546, shape=(), dtype=float32)\n",
            "iter 323, loss = tf.Tensor(0.688935, shape=(), dtype=float32)\n",
            "iter 324, loss = tf.Tensor(0.6889347, shape=(), dtype=float32)\n",
            "iter 325, loss = tf.Tensor(0.68893427, shape=(), dtype=float32)\n",
            "iter 326, loss = tf.Tensor(0.68893385, shape=(), dtype=float32)\n",
            "iter 327, loss = tf.Tensor(0.68893343, shape=(), dtype=float32)\n",
            "iter 328, loss = tf.Tensor(0.688933, shape=(), dtype=float32)\n",
            "iter 329, loss = tf.Tensor(0.6889327, shape=(), dtype=float32)\n",
            "iter 330, loss = tf.Tensor(0.68893236, shape=(), dtype=float32)\n",
            "iter 331, loss = tf.Tensor(0.6889319, shape=(), dtype=float32)\n",
            "iter 332, loss = tf.Tensor(0.68893147, shape=(), dtype=float32)\n",
            "iter 333, loss = tf.Tensor(0.6889312, shape=(), dtype=float32)\n",
            "iter 334, loss = tf.Tensor(0.6889308, shape=(), dtype=float32)\n",
            "iter 335, loss = tf.Tensor(0.6889304, shape=(), dtype=float32)\n",
            "iter 336, loss = tf.Tensor(0.68893003, shape=(), dtype=float32)\n",
            "iter 337, loss = tf.Tensor(0.68892974, shape=(), dtype=float32)\n",
            "iter 338, loss = tf.Tensor(0.6889294, shape=(), dtype=float32)\n",
            "iter 339, loss = tf.Tensor(0.68892896, shape=(), dtype=float32)\n",
            "iter 340, loss = tf.Tensor(0.6889286, shape=(), dtype=float32)\n",
            "iter 341, loss = tf.Tensor(0.6889283, shape=(), dtype=float32)\n",
            "iter 342, loss = tf.Tensor(0.68892795, shape=(), dtype=float32)\n",
            "iter 343, loss = tf.Tensor(0.68892753, shape=(), dtype=float32)\n",
            "iter 344, loss = tf.Tensor(0.68892723, shape=(), dtype=float32)\n",
            "iter 345, loss = tf.Tensor(0.68892694, shape=(), dtype=float32)\n",
            "iter 346, loss = tf.Tensor(0.6889266, shape=(), dtype=float32)\n",
            "iter 347, loss = tf.Tensor(0.68892634, shape=(), dtype=float32)\n",
            "iter 348, loss = tf.Tensor(0.6889259, shape=(), dtype=float32)\n",
            "iter 349, loss = tf.Tensor(0.68892556, shape=(), dtype=float32)\n",
            "iter 350, loss = tf.Tensor(0.6889253, shape=(), dtype=float32)\n",
            "iter 351, loss = tf.Tensor(0.688925, shape=(), dtype=float32)\n",
            "iter 352, loss = tf.Tensor(0.6889248, shape=(), dtype=float32)\n",
            "iter 353, loss = tf.Tensor(0.68892443, shape=(), dtype=float32)\n",
            "iter 354, loss = tf.Tensor(0.6889241, shape=(), dtype=float32)\n",
            "iter 355, loss = tf.Tensor(0.6889238, shape=(), dtype=float32)\n",
            "iter 356, loss = tf.Tensor(0.6889235, shape=(), dtype=float32)\n",
            "iter 357, loss = tf.Tensor(0.6889233, shape=(), dtype=float32)\n",
            "iter 358, loss = tf.Tensor(0.6889229, shape=(), dtype=float32)\n",
            "iter 359, loss = tf.Tensor(0.6889226, shape=(), dtype=float32)\n",
            "iter 360, loss = tf.Tensor(0.6889222, shape=(), dtype=float32)\n",
            "iter 361, loss = tf.Tensor(0.688922, shape=(), dtype=float32)\n",
            "iter 362, loss = tf.Tensor(0.68892163, shape=(), dtype=float32)\n",
            "iter 363, loss = tf.Tensor(0.6889214, shape=(), dtype=float32)\n",
            "iter 364, loss = tf.Tensor(0.68892115, shape=(), dtype=float32)\n",
            "iter 365, loss = tf.Tensor(0.6889209, shape=(), dtype=float32)\n",
            "iter 366, loss = tf.Tensor(0.68892056, shape=(), dtype=float32)\n",
            "iter 367, loss = tf.Tensor(0.6889202, shape=(), dtype=float32)\n",
            "iter 368, loss = tf.Tensor(0.68891996, shape=(), dtype=float32)\n",
            "iter 369, loss = tf.Tensor(0.6889196, shape=(), dtype=float32)\n",
            "iter 370, loss = tf.Tensor(0.68891937, shape=(), dtype=float32)\n",
            "iter 371, loss = tf.Tensor(0.6889192, shape=(), dtype=float32)\n",
            "iter 372, loss = tf.Tensor(0.68891877, shape=(), dtype=float32)\n",
            "iter 373, loss = tf.Tensor(0.6889186, shape=(), dtype=float32)\n",
            "iter 374, loss = tf.Tensor(0.68891823, shape=(), dtype=float32)\n",
            "iter 375, loss = tf.Tensor(0.688918, shape=(), dtype=float32)\n",
            "iter 376, loss = tf.Tensor(0.68891764, shape=(), dtype=float32)\n",
            "iter 377, loss = tf.Tensor(0.68891746, shape=(), dtype=float32)\n",
            "iter 378, loss = tf.Tensor(0.68891704, shape=(), dtype=float32)\n",
            "iter 379, loss = tf.Tensor(0.6889169, shape=(), dtype=float32)\n",
            "iter 380, loss = tf.Tensor(0.6889166, shape=(), dtype=float32)\n",
            "iter 381, loss = tf.Tensor(0.68891627, shape=(), dtype=float32)\n",
            "iter 382, loss = tf.Tensor(0.68891597, shape=(), dtype=float32)\n",
            "iter 383, loss = tf.Tensor(0.68891567, shape=(), dtype=float32)\n",
            "iter 384, loss = tf.Tensor(0.68891543, shape=(), dtype=float32)\n",
            "iter 385, loss = tf.Tensor(0.6889151, shape=(), dtype=float32)\n",
            "iter 386, loss = tf.Tensor(0.6889148, shape=(), dtype=float32)\n",
            "iter 387, loss = tf.Tensor(0.6889146, shape=(), dtype=float32)\n",
            "iter 388, loss = tf.Tensor(0.6889142, shape=(), dtype=float32)\n",
            "iter 389, loss = tf.Tensor(0.68891406, shape=(), dtype=float32)\n",
            "iter 390, loss = tf.Tensor(0.6889137, shape=(), dtype=float32)\n",
            "iter 391, loss = tf.Tensor(0.68891346, shape=(), dtype=float32)\n",
            "iter 392, loss = tf.Tensor(0.6889132, shape=(), dtype=float32)\n",
            "iter 393, loss = tf.Tensor(0.6889128, shape=(), dtype=float32)\n",
            "iter 394, loss = tf.Tensor(0.68891263, shape=(), dtype=float32)\n",
            "iter 395, loss = tf.Tensor(0.6889122, shape=(), dtype=float32)\n",
            "iter 396, loss = tf.Tensor(0.68891203, shape=(), dtype=float32)\n",
            "iter 397, loss = tf.Tensor(0.6889118, shape=(), dtype=float32)\n",
            "iter 398, loss = tf.Tensor(0.6889113, shape=(), dtype=float32)\n",
            "iter 399, loss = tf.Tensor(0.6889112, shape=(), dtype=float32)\n",
            "iter 400, loss = tf.Tensor(0.6889109, shape=(), dtype=float32)\n",
            "iter 401, loss = tf.Tensor(0.6889106, shape=(), dtype=float32)\n",
            "iter 402, loss = tf.Tensor(0.6889103, shape=(), dtype=float32)\n",
            "iter 403, loss = tf.Tensor(0.68891007, shape=(), dtype=float32)\n",
            "iter 404, loss = tf.Tensor(0.6889097, shape=(), dtype=float32)\n",
            "iter 405, loss = tf.Tensor(0.6889094, shape=(), dtype=float32)\n",
            "iter 406, loss = tf.Tensor(0.68890923, shape=(), dtype=float32)\n",
            "iter 407, loss = tf.Tensor(0.6889089, shape=(), dtype=float32)\n",
            "iter 408, loss = tf.Tensor(0.6889087, shape=(), dtype=float32)\n",
            "iter 409, loss = tf.Tensor(0.6889084, shape=(), dtype=float32)\n",
            "iter 410, loss = tf.Tensor(0.6889081, shape=(), dtype=float32)\n",
            "iter 411, loss = tf.Tensor(0.6889078, shape=(), dtype=float32)\n",
            "iter 412, loss = tf.Tensor(0.6889076, shape=(), dtype=float32)\n",
            "iter 413, loss = tf.Tensor(0.6889072, shape=(), dtype=float32)\n",
            "iter 414, loss = tf.Tensor(0.6889069, shape=(), dtype=float32)\n",
            "iter 415, loss = tf.Tensor(0.6889067, shape=(), dtype=float32)\n",
            "iter 416, loss = tf.Tensor(0.68890643, shape=(), dtype=float32)\n",
            "iter 417, loss = tf.Tensor(0.68890613, shape=(), dtype=float32)\n",
            "iter 418, loss = tf.Tensor(0.68890584, shape=(), dtype=float32)\n",
            "iter 419, loss = tf.Tensor(0.6889056, shape=(), dtype=float32)\n",
            "iter 420, loss = tf.Tensor(0.6889053, shape=(), dtype=float32)\n",
            "iter 421, loss = tf.Tensor(0.688905, shape=(), dtype=float32)\n",
            "iter 422, loss = tf.Tensor(0.6889048, shape=(), dtype=float32)\n",
            "iter 423, loss = tf.Tensor(0.6889046, shape=(), dtype=float32)\n",
            "iter 424, loss = tf.Tensor(0.6889042, shape=(), dtype=float32)\n",
            "iter 425, loss = tf.Tensor(0.688904, shape=(), dtype=float32)\n",
            "iter 426, loss = tf.Tensor(0.68890375, shape=(), dtype=float32)\n",
            "iter 427, loss = tf.Tensor(0.6889034, shape=(), dtype=float32)\n",
            "iter 428, loss = tf.Tensor(0.68890315, shape=(), dtype=float32)\n",
            "iter 429, loss = tf.Tensor(0.68890285, shape=(), dtype=float32)\n",
            "iter 430, loss = tf.Tensor(0.6889025, shape=(), dtype=float32)\n",
            "iter 431, loss = tf.Tensor(0.6889023, shape=(), dtype=float32)\n",
            "iter 432, loss = tf.Tensor(0.688902, shape=(), dtype=float32)\n",
            "iter 433, loss = tf.Tensor(0.6889018, shape=(), dtype=float32)\n",
            "iter 434, loss = tf.Tensor(0.6889015, shape=(), dtype=float32)\n",
            "iter 435, loss = tf.Tensor(0.68890125, shape=(), dtype=float32)\n",
            "iter 436, loss = tf.Tensor(0.68890095, shape=(), dtype=float32)\n",
            "iter 437, loss = tf.Tensor(0.6889007, shape=(), dtype=float32)\n",
            "iter 438, loss = tf.Tensor(0.68890035, shape=(), dtype=float32)\n",
            "iter 439, loss = tf.Tensor(0.68890005, shape=(), dtype=float32)\n",
            "iter 440, loss = tf.Tensor(0.6888998, shape=(), dtype=float32)\n",
            "iter 441, loss = tf.Tensor(0.6888995, shape=(), dtype=float32)\n",
            "iter 442, loss = tf.Tensor(0.6888993, shape=(), dtype=float32)\n",
            "iter 443, loss = tf.Tensor(0.688899, shape=(), dtype=float32)\n",
            "iter 444, loss = tf.Tensor(0.68889874, shape=(), dtype=float32)\n",
            "iter 445, loss = tf.Tensor(0.6888985, shape=(), dtype=float32)\n",
            "iter 446, loss = tf.Tensor(0.68889827, shape=(), dtype=float32)\n",
            "iter 447, loss = tf.Tensor(0.6888979, shape=(), dtype=float32)\n",
            "iter 448, loss = tf.Tensor(0.6888976, shape=(), dtype=float32)\n",
            "iter 449, loss = tf.Tensor(0.6888975, shape=(), dtype=float32)\n",
            "iter 450, loss = tf.Tensor(0.68889725, shape=(), dtype=float32)\n",
            "iter 451, loss = tf.Tensor(0.68889683, shape=(), dtype=float32)\n",
            "iter 452, loss = tf.Tensor(0.6888965, shape=(), dtype=float32)\n",
            "iter 453, loss = tf.Tensor(0.68889636, shape=(), dtype=float32)\n",
            "iter 454, loss = tf.Tensor(0.68889606, shape=(), dtype=float32)\n",
            "iter 455, loss = tf.Tensor(0.6888958, shape=(), dtype=float32)\n",
            "iter 456, loss = tf.Tensor(0.68889546, shape=(), dtype=float32)\n",
            "iter 457, loss = tf.Tensor(0.6888952, shape=(), dtype=float32)\n",
            "iter 458, loss = tf.Tensor(0.6888949, shape=(), dtype=float32)\n",
            "iter 459, loss = tf.Tensor(0.6888946, shape=(), dtype=float32)\n",
            "iter 460, loss = tf.Tensor(0.6888944, shape=(), dtype=float32)\n",
            "iter 461, loss = tf.Tensor(0.68889403, shape=(), dtype=float32)\n",
            "iter 462, loss = tf.Tensor(0.6888939, shape=(), dtype=float32)\n",
            "iter 463, loss = tf.Tensor(0.68889356, shape=(), dtype=float32)\n",
            "iter 464, loss = tf.Tensor(0.68889326, shape=(), dtype=float32)\n",
            "iter 465, loss = tf.Tensor(0.6888931, shape=(), dtype=float32)\n",
            "iter 466, loss = tf.Tensor(0.6888928, shape=(), dtype=float32)\n",
            "iter 467, loss = tf.Tensor(0.6888925, shape=(), dtype=float32)\n",
            "iter 468, loss = tf.Tensor(0.6888922, shape=(), dtype=float32)\n",
            "iter 469, loss = tf.Tensor(0.68889195, shape=(), dtype=float32)\n",
            "iter 470, loss = tf.Tensor(0.68889165, shape=(), dtype=float32)\n",
            "iter 471, loss = tf.Tensor(0.6888913, shape=(), dtype=float32)\n",
            "iter 472, loss = tf.Tensor(0.68889105, shape=(), dtype=float32)\n",
            "iter 473, loss = tf.Tensor(0.6888908, shape=(), dtype=float32)\n",
            "iter 474, loss = tf.Tensor(0.6888905, shape=(), dtype=float32)\n",
            "iter 475, loss = tf.Tensor(0.6888902, shape=(), dtype=float32)\n",
            "iter 476, loss = tf.Tensor(0.68889, shape=(), dtype=float32)\n",
            "iter 477, loss = tf.Tensor(0.6888896, shape=(), dtype=float32)\n",
            "iter 478, loss = tf.Tensor(0.68888944, shape=(), dtype=float32)\n",
            "iter 479, loss = tf.Tensor(0.6888891, shape=(), dtype=float32)\n",
            "iter 480, loss = tf.Tensor(0.68888885, shape=(), dtype=float32)\n",
            "iter 481, loss = tf.Tensor(0.6888886, shape=(), dtype=float32)\n",
            "iter 482, loss = tf.Tensor(0.6888884, shape=(), dtype=float32)\n",
            "iter 483, loss = tf.Tensor(0.688888, shape=(), dtype=float32)\n",
            "iter 484, loss = tf.Tensor(0.6888877, shape=(), dtype=float32)\n",
            "iter 485, loss = tf.Tensor(0.6888875, shape=(), dtype=float32)\n",
            "iter 486, loss = tf.Tensor(0.6888872, shape=(), dtype=float32)\n",
            "iter 487, loss = tf.Tensor(0.688887, shape=(), dtype=float32)\n",
            "iter 488, loss = tf.Tensor(0.6888866, shape=(), dtype=float32)\n",
            "iter 489, loss = tf.Tensor(0.6888864, shape=(), dtype=float32)\n",
            "iter 490, loss = tf.Tensor(0.6888861, shape=(), dtype=float32)\n",
            "iter 491, loss = tf.Tensor(0.6888857, shape=(), dtype=float32)\n",
            "iter 492, loss = tf.Tensor(0.6888855, shape=(), dtype=float32)\n",
            "iter 493, loss = tf.Tensor(0.68888515, shape=(), dtype=float32)\n",
            "iter 494, loss = tf.Tensor(0.68888503, shape=(), dtype=float32)\n",
            "iter 495, loss = tf.Tensor(0.6888846, shape=(), dtype=float32)\n",
            "iter 496, loss = tf.Tensor(0.6888843, shape=(), dtype=float32)\n",
            "iter 497, loss = tf.Tensor(0.688884, shape=(), dtype=float32)\n",
            "iter 498, loss = tf.Tensor(0.6888837, shape=(), dtype=float32)\n",
            "iter 499, loss = tf.Tensor(0.6888835, shape=(), dtype=float32)\n",
            "iter 500, loss = tf.Tensor(0.6888832, shape=(), dtype=float32)\n",
            "iter 501, loss = tf.Tensor(0.68888295, shape=(), dtype=float32)\n",
            "iter 502, loss = tf.Tensor(0.68888265, shape=(), dtype=float32)\n",
            "iter 503, loss = tf.Tensor(0.68888223, shape=(), dtype=float32)\n",
            "iter 504, loss = tf.Tensor(0.68888205, shape=(), dtype=float32)\n",
            "iter 505, loss = tf.Tensor(0.6888817, shape=(), dtype=float32)\n",
            "iter 506, loss = tf.Tensor(0.68888146, shape=(), dtype=float32)\n",
            "iter 507, loss = tf.Tensor(0.6888811, shape=(), dtype=float32)\n",
            "iter 508, loss = tf.Tensor(0.6888808, shape=(), dtype=float32)\n",
            "iter 509, loss = tf.Tensor(0.68888056, shape=(), dtype=float32)\n",
            "iter 510, loss = tf.Tensor(0.6888802, shape=(), dtype=float32)\n",
            "iter 511, loss = tf.Tensor(0.68887985, shape=(), dtype=float32)\n",
            "iter 512, loss = tf.Tensor(0.6888796, shape=(), dtype=float32)\n",
            "iter 513, loss = tf.Tensor(0.68887925, shape=(), dtype=float32)\n",
            "iter 514, loss = tf.Tensor(0.688879, shape=(), dtype=float32)\n",
            "iter 515, loss = tf.Tensor(0.68887866, shape=(), dtype=float32)\n",
            "iter 516, loss = tf.Tensor(0.68887836, shape=(), dtype=float32)\n",
            "iter 517, loss = tf.Tensor(0.68887806, shape=(), dtype=float32)\n",
            "iter 518, loss = tf.Tensor(0.6888777, shape=(), dtype=float32)\n",
            "iter 519, loss = tf.Tensor(0.6888774, shape=(), dtype=float32)\n",
            "iter 520, loss = tf.Tensor(0.68887717, shape=(), dtype=float32)\n",
            "iter 521, loss = tf.Tensor(0.68887687, shape=(), dtype=float32)\n",
            "iter 522, loss = tf.Tensor(0.6888766, shape=(), dtype=float32)\n",
            "iter 523, loss = tf.Tensor(0.6888762, shape=(), dtype=float32)\n",
            "iter 524, loss = tf.Tensor(0.6888759, shape=(), dtype=float32)\n",
            "iter 525, loss = tf.Tensor(0.6888756, shape=(), dtype=float32)\n",
            "iter 526, loss = tf.Tensor(0.68887526, shape=(), dtype=float32)\n",
            "iter 527, loss = tf.Tensor(0.6888749, shape=(), dtype=float32)\n",
            "iter 528, loss = tf.Tensor(0.68887466, shape=(), dtype=float32)\n",
            "iter 529, loss = tf.Tensor(0.68887436, shape=(), dtype=float32)\n",
            "iter 530, loss = tf.Tensor(0.6888741, shape=(), dtype=float32)\n",
            "iter 531, loss = tf.Tensor(0.68887377, shape=(), dtype=float32)\n",
            "iter 532, loss = tf.Tensor(0.6888736, shape=(), dtype=float32)\n",
            "iter 533, loss = tf.Tensor(0.6888731, shape=(), dtype=float32)\n",
            "iter 534, loss = tf.Tensor(0.6888728, shape=(), dtype=float32)\n",
            "iter 535, loss = tf.Tensor(0.6888725, shape=(), dtype=float32)\n",
            "iter 536, loss = tf.Tensor(0.68887216, shape=(), dtype=float32)\n",
            "iter 537, loss = tf.Tensor(0.6888719, shape=(), dtype=float32)\n",
            "iter 538, loss = tf.Tensor(0.6888716, shape=(), dtype=float32)\n",
            "iter 539, loss = tf.Tensor(0.6888713, shape=(), dtype=float32)\n",
            "iter 540, loss = tf.Tensor(0.6888709, shape=(), dtype=float32)\n",
            "iter 541, loss = tf.Tensor(0.68887067, shape=(), dtype=float32)\n",
            "iter 542, loss = tf.Tensor(0.6888703, shape=(), dtype=float32)\n",
            "iter 543, loss = tf.Tensor(0.68886995, shape=(), dtype=float32)\n",
            "iter 544, loss = tf.Tensor(0.6888698, shape=(), dtype=float32)\n",
            "iter 545, loss = tf.Tensor(0.6888694, shape=(), dtype=float32)\n",
            "iter 546, loss = tf.Tensor(0.68886906, shape=(), dtype=float32)\n",
            "iter 547, loss = tf.Tensor(0.6888688, shape=(), dtype=float32)\n",
            "iter 548, loss = tf.Tensor(0.68886846, shape=(), dtype=float32)\n",
            "iter 549, loss = tf.Tensor(0.6888682, shape=(), dtype=float32)\n",
            "iter 550, loss = tf.Tensor(0.6888678, shape=(), dtype=float32)\n",
            "iter 551, loss = tf.Tensor(0.6888675, shape=(), dtype=float32)\n",
            "iter 552, loss = tf.Tensor(0.6888673, shape=(), dtype=float32)\n",
            "iter 553, loss = tf.Tensor(0.6888669, shape=(), dtype=float32)\n",
            "iter 554, loss = tf.Tensor(0.68886656, shape=(), dtype=float32)\n",
            "iter 555, loss = tf.Tensor(0.6888663, shape=(), dtype=float32)\n",
            "iter 556, loss = tf.Tensor(0.6888659, shape=(), dtype=float32)\n",
            "iter 557, loss = tf.Tensor(0.6888656, shape=(), dtype=float32)\n",
            "iter 558, loss = tf.Tensor(0.68886536, shape=(), dtype=float32)\n",
            "iter 559, loss = tf.Tensor(0.688865, shape=(), dtype=float32)\n",
            "iter 560, loss = tf.Tensor(0.68886477, shape=(), dtype=float32)\n",
            "iter 561, loss = tf.Tensor(0.68886435, shape=(), dtype=float32)\n",
            "iter 562, loss = tf.Tensor(0.68886405, shape=(), dtype=float32)\n",
            "iter 563, loss = tf.Tensor(0.68886375, shape=(), dtype=float32)\n",
            "iter 564, loss = tf.Tensor(0.6888634, shape=(), dtype=float32)\n",
            "iter 565, loss = tf.Tensor(0.6888631, shape=(), dtype=float32)\n",
            "iter 566, loss = tf.Tensor(0.68886286, shape=(), dtype=float32)\n",
            "iter 567, loss = tf.Tensor(0.6888625, shape=(), dtype=float32)\n",
            "iter 568, loss = tf.Tensor(0.68886226, shape=(), dtype=float32)\n",
            "iter 569, loss = tf.Tensor(0.6888619, shape=(), dtype=float32)\n",
            "iter 570, loss = tf.Tensor(0.6888615, shape=(), dtype=float32)\n",
            "iter 571, loss = tf.Tensor(0.68886113, shape=(), dtype=float32)\n",
            "iter 572, loss = tf.Tensor(0.6888609, shape=(), dtype=float32)\n",
            "iter 573, loss = tf.Tensor(0.6888605, shape=(), dtype=float32)\n",
            "iter 574, loss = tf.Tensor(0.6888602, shape=(), dtype=float32)\n",
            "iter 575, loss = tf.Tensor(0.68885994, shape=(), dtype=float32)\n",
            "iter 576, loss = tf.Tensor(0.6888595, shape=(), dtype=float32)\n",
            "iter 577, loss = tf.Tensor(0.6888592, shape=(), dtype=float32)\n",
            "iter 578, loss = tf.Tensor(0.6888589, shape=(), dtype=float32)\n",
            "iter 579, loss = tf.Tensor(0.68885857, shape=(), dtype=float32)\n",
            "iter 580, loss = tf.Tensor(0.68885833, shape=(), dtype=float32)\n",
            "iter 581, loss = tf.Tensor(0.68885803, shape=(), dtype=float32)\n",
            "iter 582, loss = tf.Tensor(0.6888576, shape=(), dtype=float32)\n",
            "iter 583, loss = tf.Tensor(0.6888572, shape=(), dtype=float32)\n",
            "iter 584, loss = tf.Tensor(0.68885696, shape=(), dtype=float32)\n",
            "iter 585, loss = tf.Tensor(0.6888566, shape=(), dtype=float32)\n",
            "iter 586, loss = tf.Tensor(0.68885624, shape=(), dtype=float32)\n",
            "iter 587, loss = tf.Tensor(0.68885595, shape=(), dtype=float32)\n",
            "iter 588, loss = tf.Tensor(0.68885565, shape=(), dtype=float32)\n",
            "iter 589, loss = tf.Tensor(0.6888553, shape=(), dtype=float32)\n",
            "iter 590, loss = tf.Tensor(0.688855, shape=(), dtype=float32)\n",
            "iter 591, loss = tf.Tensor(0.68885463, shape=(), dtype=float32)\n",
            "iter 592, loss = tf.Tensor(0.6888543, shape=(), dtype=float32)\n",
            "iter 593, loss = tf.Tensor(0.688854, shape=(), dtype=float32)\n",
            "iter 594, loss = tf.Tensor(0.6888536, shape=(), dtype=float32)\n",
            "iter 595, loss = tf.Tensor(0.6888533, shape=(), dtype=float32)\n",
            "iter 596, loss = tf.Tensor(0.68885297, shape=(), dtype=float32)\n",
            "iter 597, loss = tf.Tensor(0.68885267, shape=(), dtype=float32)\n",
            "iter 598, loss = tf.Tensor(0.68885225, shape=(), dtype=float32)\n",
            "iter 599, loss = tf.Tensor(0.68885195, shape=(), dtype=float32)\n",
            "iter 600, loss = tf.Tensor(0.6888516, shape=(), dtype=float32)\n",
            "iter 601, loss = tf.Tensor(0.68885124, shape=(), dtype=float32)\n",
            "iter 602, loss = tf.Tensor(0.688851, shape=(), dtype=float32)\n",
            "iter 603, loss = tf.Tensor(0.6888506, shape=(), dtype=float32)\n",
            "iter 604, loss = tf.Tensor(0.6888502, shape=(), dtype=float32)\n",
            "iter 605, loss = tf.Tensor(0.68884987, shape=(), dtype=float32)\n",
            "iter 606, loss = tf.Tensor(0.68884957, shape=(), dtype=float32)\n",
            "iter 607, loss = tf.Tensor(0.6888492, shape=(), dtype=float32)\n",
            "iter 608, loss = tf.Tensor(0.6888488, shape=(), dtype=float32)\n",
            "iter 609, loss = tf.Tensor(0.68884856, shape=(), dtype=float32)\n",
            "iter 610, loss = tf.Tensor(0.6888482, shape=(), dtype=float32)\n",
            "iter 611, loss = tf.Tensor(0.6888478, shape=(), dtype=float32)\n",
            "iter 612, loss = tf.Tensor(0.6888475, shape=(), dtype=float32)\n",
            "iter 613, loss = tf.Tensor(0.6888472, shape=(), dtype=float32)\n",
            "iter 614, loss = tf.Tensor(0.68884677, shape=(), dtype=float32)\n",
            "iter 615, loss = tf.Tensor(0.68884635, shape=(), dtype=float32)\n",
            "iter 616, loss = tf.Tensor(0.6888461, shape=(), dtype=float32)\n",
            "iter 617, loss = tf.Tensor(0.68884575, shape=(), dtype=float32)\n",
            "iter 618, loss = tf.Tensor(0.68884534, shape=(), dtype=float32)\n",
            "iter 619, loss = tf.Tensor(0.6888451, shape=(), dtype=float32)\n",
            "iter 620, loss = tf.Tensor(0.6888446, shape=(), dtype=float32)\n",
            "iter 621, loss = tf.Tensor(0.6888443, shape=(), dtype=float32)\n",
            "iter 622, loss = tf.Tensor(0.6888439, shape=(), dtype=float32)\n",
            "iter 623, loss = tf.Tensor(0.68884367, shape=(), dtype=float32)\n",
            "iter 624, loss = tf.Tensor(0.6888433, shape=(), dtype=float32)\n",
            "iter 625, loss = tf.Tensor(0.6888429, shape=(), dtype=float32)\n",
            "iter 626, loss = tf.Tensor(0.6888426, shape=(), dtype=float32)\n",
            "iter 627, loss = tf.Tensor(0.6888421, shape=(), dtype=float32)\n",
            "iter 628, loss = tf.Tensor(0.6888418, shape=(), dtype=float32)\n",
            "iter 629, loss = tf.Tensor(0.6888416, shape=(), dtype=float32)\n",
            "iter 630, loss = tf.Tensor(0.68884116, shape=(), dtype=float32)\n",
            "iter 631, loss = tf.Tensor(0.68884087, shape=(), dtype=float32)\n",
            "iter 632, loss = tf.Tensor(0.68884045, shape=(), dtype=float32)\n",
            "iter 633, loss = tf.Tensor(0.68884015, shape=(), dtype=float32)\n",
            "iter 634, loss = tf.Tensor(0.6888398, shape=(), dtype=float32)\n",
            "iter 635, loss = tf.Tensor(0.68883944, shape=(), dtype=float32)\n",
            "iter 636, loss = tf.Tensor(0.68883914, shape=(), dtype=float32)\n",
            "iter 637, loss = tf.Tensor(0.6888387, shape=(), dtype=float32)\n",
            "iter 638, loss = tf.Tensor(0.6888384, shape=(), dtype=float32)\n",
            "iter 639, loss = tf.Tensor(0.688838, shape=(), dtype=float32)\n",
            "iter 640, loss = tf.Tensor(0.68883765, shape=(), dtype=float32)\n",
            "iter 641, loss = tf.Tensor(0.68883723, shape=(), dtype=float32)\n",
            "iter 642, loss = tf.Tensor(0.6888369, shape=(), dtype=float32)\n",
            "iter 643, loss = tf.Tensor(0.6888365, shape=(), dtype=float32)\n",
            "iter 644, loss = tf.Tensor(0.6888362, shape=(), dtype=float32)\n",
            "iter 645, loss = tf.Tensor(0.6888358, shape=(), dtype=float32)\n",
            "iter 646, loss = tf.Tensor(0.6888355, shape=(), dtype=float32)\n",
            "iter 647, loss = tf.Tensor(0.6888351, shape=(), dtype=float32)\n",
            "iter 648, loss = tf.Tensor(0.6888347, shape=(), dtype=float32)\n",
            "iter 649, loss = tf.Tensor(0.6888343, shape=(), dtype=float32)\n",
            "iter 650, loss = tf.Tensor(0.688834, shape=(), dtype=float32)\n",
            "iter 651, loss = tf.Tensor(0.6888336, shape=(), dtype=float32)\n",
            "iter 652, loss = tf.Tensor(0.68883324, shape=(), dtype=float32)\n",
            "iter 653, loss = tf.Tensor(0.68883294, shape=(), dtype=float32)\n",
            "iter 654, loss = tf.Tensor(0.6888325, shape=(), dtype=float32)\n",
            "iter 655, loss = tf.Tensor(0.6888322, shape=(), dtype=float32)\n",
            "iter 656, loss = tf.Tensor(0.68883187, shape=(), dtype=float32)\n",
            "iter 657, loss = tf.Tensor(0.68883145, shape=(), dtype=float32)\n",
            "iter 658, loss = tf.Tensor(0.6888311, shape=(), dtype=float32)\n",
            "iter 659, loss = tf.Tensor(0.6888306, shape=(), dtype=float32)\n",
            "iter 660, loss = tf.Tensor(0.6888304, shape=(), dtype=float32)\n",
            "iter 661, loss = tf.Tensor(0.68883, shape=(), dtype=float32)\n",
            "iter 662, loss = tf.Tensor(0.68882954, shape=(), dtype=float32)\n",
            "iter 663, loss = tf.Tensor(0.6888292, shape=(), dtype=float32)\n",
            "iter 664, loss = tf.Tensor(0.6888288, shape=(), dtype=float32)\n",
            "iter 665, loss = tf.Tensor(0.6888284, shape=(), dtype=float32)\n",
            "iter 666, loss = tf.Tensor(0.688828, shape=(), dtype=float32)\n",
            "iter 667, loss = tf.Tensor(0.6888277, shape=(), dtype=float32)\n",
            "iter 668, loss = tf.Tensor(0.68882734, shape=(), dtype=float32)\n",
            "iter 669, loss = tf.Tensor(0.6888269, shape=(), dtype=float32)\n",
            "iter 670, loss = tf.Tensor(0.6888265, shape=(), dtype=float32)\n",
            "iter 671, loss = tf.Tensor(0.68882614, shape=(), dtype=float32)\n",
            "iter 672, loss = tf.Tensor(0.68882585, shape=(), dtype=float32)\n",
            "iter 673, loss = tf.Tensor(0.68882537, shape=(), dtype=float32)\n",
            "iter 674, loss = tf.Tensor(0.688825, shape=(), dtype=float32)\n",
            "iter 675, loss = tf.Tensor(0.68882465, shape=(), dtype=float32)\n",
            "iter 676, loss = tf.Tensor(0.6888242, shape=(), dtype=float32)\n",
            "iter 677, loss = tf.Tensor(0.6888239, shape=(), dtype=float32)\n",
            "iter 678, loss = tf.Tensor(0.6888235, shape=(), dtype=float32)\n",
            "iter 679, loss = tf.Tensor(0.6888231, shape=(), dtype=float32)\n",
            "iter 680, loss = tf.Tensor(0.68882275, shape=(), dtype=float32)\n",
            "iter 681, loss = tf.Tensor(0.6888224, shape=(), dtype=float32)\n",
            "iter 682, loss = tf.Tensor(0.688822, shape=(), dtype=float32)\n",
            "iter 683, loss = tf.Tensor(0.6888216, shape=(), dtype=float32)\n",
            "iter 684, loss = tf.Tensor(0.6888212, shape=(), dtype=float32)\n",
            "iter 685, loss = tf.Tensor(0.6888208, shape=(), dtype=float32)\n",
            "iter 686, loss = tf.Tensor(0.6888204, shape=(), dtype=float32)\n",
            "iter 687, loss = tf.Tensor(0.68882006, shape=(), dtype=float32)\n",
            "iter 688, loss = tf.Tensor(0.6888197, shape=(), dtype=float32)\n",
            "iter 689, loss = tf.Tensor(0.6888192, shape=(), dtype=float32)\n",
            "iter 690, loss = tf.Tensor(0.68881893, shape=(), dtype=float32)\n",
            "iter 691, loss = tf.Tensor(0.6888185, shape=(), dtype=float32)\n",
            "iter 692, loss = tf.Tensor(0.6888181, shape=(), dtype=float32)\n",
            "iter 693, loss = tf.Tensor(0.68881774, shape=(), dtype=float32)\n",
            "iter 694, loss = tf.Tensor(0.6888173, shape=(), dtype=float32)\n",
            "iter 695, loss = tf.Tensor(0.68881696, shape=(), dtype=float32)\n",
            "iter 696, loss = tf.Tensor(0.68881655, shape=(), dtype=float32)\n",
            "iter 697, loss = tf.Tensor(0.68881613, shape=(), dtype=float32)\n",
            "iter 698, loss = tf.Tensor(0.6888157, shape=(), dtype=float32)\n",
            "iter 699, loss = tf.Tensor(0.68881536, shape=(), dtype=float32)\n",
            "iter 700, loss = tf.Tensor(0.688815, shape=(), dtype=float32)\n",
            "iter 701, loss = tf.Tensor(0.6888146, shape=(), dtype=float32)\n",
            "iter 702, loss = tf.Tensor(0.6888142, shape=(), dtype=float32)\n",
            "iter 703, loss = tf.Tensor(0.6888138, shape=(), dtype=float32)\n",
            "iter 704, loss = tf.Tensor(0.6888134, shape=(), dtype=float32)\n",
            "iter 705, loss = tf.Tensor(0.68881303, shape=(), dtype=float32)\n",
            "iter 706, loss = tf.Tensor(0.6888126, shape=(), dtype=float32)\n",
            "iter 707, loss = tf.Tensor(0.6888122, shape=(), dtype=float32)\n",
            "iter 708, loss = tf.Tensor(0.68881184, shape=(), dtype=float32)\n",
            "iter 709, loss = tf.Tensor(0.68881136, shape=(), dtype=float32)\n",
            "iter 710, loss = tf.Tensor(0.6888111, shape=(), dtype=float32)\n",
            "iter 711, loss = tf.Tensor(0.6888107, shape=(), dtype=float32)\n",
            "iter 712, loss = tf.Tensor(0.6888102, shape=(), dtype=float32)\n",
            "iter 713, loss = tf.Tensor(0.6888099, shape=(), dtype=float32)\n",
            "iter 714, loss = tf.Tensor(0.68880945, shape=(), dtype=float32)\n",
            "iter 715, loss = tf.Tensor(0.6888091, shape=(), dtype=float32)\n",
            "iter 716, loss = tf.Tensor(0.6888087, shape=(), dtype=float32)\n",
            "iter 717, loss = tf.Tensor(0.6888083, shape=(), dtype=float32)\n",
            "iter 718, loss = tf.Tensor(0.68880785, shape=(), dtype=float32)\n",
            "iter 719, loss = tf.Tensor(0.68880755, shape=(), dtype=float32)\n",
            "iter 720, loss = tf.Tensor(0.688807, shape=(), dtype=float32)\n",
            "iter 721, loss = tf.Tensor(0.6888068, shape=(), dtype=float32)\n",
            "iter 722, loss = tf.Tensor(0.6888063, shape=(), dtype=float32)\n",
            "iter 723, loss = tf.Tensor(0.6888059, shape=(), dtype=float32)\n",
            "iter 724, loss = tf.Tensor(0.68880546, shape=(), dtype=float32)\n",
            "iter 725, loss = tf.Tensor(0.68880516, shape=(), dtype=float32)\n",
            "iter 726, loss = tf.Tensor(0.6888047, shape=(), dtype=float32)\n",
            "iter 727, loss = tf.Tensor(0.68880427, shape=(), dtype=float32)\n",
            "iter 728, loss = tf.Tensor(0.6888039, shape=(), dtype=float32)\n",
            "iter 729, loss = tf.Tensor(0.68880343, shape=(), dtype=float32)\n",
            "iter 730, loss = tf.Tensor(0.688803, shape=(), dtype=float32)\n",
            "iter 731, loss = tf.Tensor(0.6888026, shape=(), dtype=float32)\n",
            "iter 732, loss = tf.Tensor(0.68880224, shape=(), dtype=float32)\n",
            "iter 733, loss = tf.Tensor(0.6888018, shape=(), dtype=float32)\n",
            "iter 734, loss = tf.Tensor(0.6888014, shape=(), dtype=float32)\n",
            "iter 735, loss = tf.Tensor(0.688801, shape=(), dtype=float32)\n",
            "iter 736, loss = tf.Tensor(0.6888006, shape=(), dtype=float32)\n",
            "iter 737, loss = tf.Tensor(0.68880016, shape=(), dtype=float32)\n",
            "iter 738, loss = tf.Tensor(0.6887998, shape=(), dtype=float32)\n",
            "iter 739, loss = tf.Tensor(0.6887994, shape=(), dtype=float32)\n",
            "iter 740, loss = tf.Tensor(0.68879896, shape=(), dtype=float32)\n",
            "iter 741, loss = tf.Tensor(0.68879855, shape=(), dtype=float32)\n",
            "iter 742, loss = tf.Tensor(0.6887981, shape=(), dtype=float32)\n",
            "iter 743, loss = tf.Tensor(0.68879765, shape=(), dtype=float32)\n",
            "iter 744, loss = tf.Tensor(0.68879735, shape=(), dtype=float32)\n",
            "iter 745, loss = tf.Tensor(0.68879694, shape=(), dtype=float32)\n",
            "iter 746, loss = tf.Tensor(0.68879646, shape=(), dtype=float32)\n",
            "iter 747, loss = tf.Tensor(0.6887961, shape=(), dtype=float32)\n",
            "iter 748, loss = tf.Tensor(0.6887956, shape=(), dtype=float32)\n",
            "iter 749, loss = tf.Tensor(0.6887952, shape=(), dtype=float32)\n",
            "iter 750, loss = tf.Tensor(0.6887949, shape=(), dtype=float32)\n",
            "iter 751, loss = tf.Tensor(0.6887944, shape=(), dtype=float32)\n",
            "iter 752, loss = tf.Tensor(0.68879396, shape=(), dtype=float32)\n",
            "iter 753, loss = tf.Tensor(0.6887936, shape=(), dtype=float32)\n",
            "iter 754, loss = tf.Tensor(0.6887932, shape=(), dtype=float32)\n",
            "iter 755, loss = tf.Tensor(0.68879277, shape=(), dtype=float32)\n",
            "iter 756, loss = tf.Tensor(0.6887923, shape=(), dtype=float32)\n",
            "iter 757, loss = tf.Tensor(0.68879193, shape=(), dtype=float32)\n",
            "iter 758, loss = tf.Tensor(0.68879145, shape=(), dtype=float32)\n",
            "iter 759, loss = tf.Tensor(0.68879104, shape=(), dtype=float32)\n",
            "iter 760, loss = tf.Tensor(0.6887907, shape=(), dtype=float32)\n",
            "iter 761, loss = tf.Tensor(0.68879014, shape=(), dtype=float32)\n",
            "iter 762, loss = tf.Tensor(0.6887897, shape=(), dtype=float32)\n",
            "iter 763, loss = tf.Tensor(0.6887893, shape=(), dtype=float32)\n",
            "iter 764, loss = tf.Tensor(0.68878895, shape=(), dtype=float32)\n",
            "iter 765, loss = tf.Tensor(0.68878853, shape=(), dtype=float32)\n",
            "iter 766, loss = tf.Tensor(0.688788, shape=(), dtype=float32)\n",
            "iter 767, loss = tf.Tensor(0.6887876, shape=(), dtype=float32)\n",
            "iter 768, loss = tf.Tensor(0.68878716, shape=(), dtype=float32)\n",
            "iter 769, loss = tf.Tensor(0.6887867, shape=(), dtype=float32)\n",
            "iter 770, loss = tf.Tensor(0.6887862, shape=(), dtype=float32)\n",
            "iter 771, loss = tf.Tensor(0.68878573, shape=(), dtype=float32)\n",
            "iter 772, loss = tf.Tensor(0.68878525, shape=(), dtype=float32)\n",
            "iter 773, loss = tf.Tensor(0.6887848, shape=(), dtype=float32)\n",
            "iter 774, loss = tf.Tensor(0.6887845, shape=(), dtype=float32)\n",
            "iter 775, loss = tf.Tensor(0.688784, shape=(), dtype=float32)\n",
            "iter 776, loss = tf.Tensor(0.6887836, shape=(), dtype=float32)\n",
            "iter 777, loss = tf.Tensor(0.6887831, shape=(), dtype=float32)\n",
            "iter 778, loss = tf.Tensor(0.6887827, shape=(), dtype=float32)\n",
            "iter 779, loss = tf.Tensor(0.6887822, shape=(), dtype=float32)\n",
            "iter 780, loss = tf.Tensor(0.6887818, shape=(), dtype=float32)\n",
            "iter 781, loss = tf.Tensor(0.68878126, shape=(), dtype=float32)\n",
            "iter 782, loss = tf.Tensor(0.68878084, shape=(), dtype=float32)\n",
            "iter 783, loss = tf.Tensor(0.6887804, shape=(), dtype=float32)\n",
            "iter 784, loss = tf.Tensor(0.68877995, shape=(), dtype=float32)\n",
            "iter 785, loss = tf.Tensor(0.6887795, shape=(), dtype=float32)\n",
            "iter 786, loss = tf.Tensor(0.6887791, shape=(), dtype=float32)\n",
            "iter 787, loss = tf.Tensor(0.68877864, shape=(), dtype=float32)\n",
            "iter 788, loss = tf.Tensor(0.68877816, shape=(), dtype=float32)\n",
            "iter 789, loss = tf.Tensor(0.68877774, shape=(), dtype=float32)\n",
            "iter 790, loss = tf.Tensor(0.6887773, shape=(), dtype=float32)\n",
            "iter 791, loss = tf.Tensor(0.6887768, shape=(), dtype=float32)\n",
            "iter 792, loss = tf.Tensor(0.6887764, shape=(), dtype=float32)\n",
            "iter 793, loss = tf.Tensor(0.6887759, shape=(), dtype=float32)\n",
            "iter 794, loss = tf.Tensor(0.6887755, shape=(), dtype=float32)\n",
            "iter 795, loss = tf.Tensor(0.688775, shape=(), dtype=float32)\n",
            "iter 796, loss = tf.Tensor(0.6887745, shape=(), dtype=float32)\n",
            "iter 797, loss = tf.Tensor(0.68877405, shape=(), dtype=float32)\n",
            "iter 798, loss = tf.Tensor(0.6887737, shape=(), dtype=float32)\n",
            "iter 799, loss = tf.Tensor(0.68877316, shape=(), dtype=float32)\n",
            "iter 800, loss = tf.Tensor(0.68877274, shape=(), dtype=float32)\n",
            "iter 801, loss = tf.Tensor(0.6887723, shape=(), dtype=float32)\n",
            "iter 802, loss = tf.Tensor(0.6887718, shape=(), dtype=float32)\n",
            "iter 803, loss = tf.Tensor(0.68877137, shape=(), dtype=float32)\n",
            "iter 804, loss = tf.Tensor(0.68877083, shape=(), dtype=float32)\n",
            "iter 805, loss = tf.Tensor(0.68877035, shape=(), dtype=float32)\n",
            "iter 806, loss = tf.Tensor(0.68876994, shape=(), dtype=float32)\n",
            "iter 807, loss = tf.Tensor(0.6887694, shape=(), dtype=float32)\n",
            "iter 808, loss = tf.Tensor(0.68876886, shape=(), dtype=float32)\n",
            "iter 809, loss = tf.Tensor(0.68876845, shape=(), dtype=float32)\n",
            "iter 810, loss = tf.Tensor(0.68876797, shape=(), dtype=float32)\n",
            "iter 811, loss = tf.Tensor(0.68876743, shape=(), dtype=float32)\n",
            "iter 812, loss = tf.Tensor(0.6887671, shape=(), dtype=float32)\n",
            "iter 813, loss = tf.Tensor(0.68876654, shape=(), dtype=float32)\n",
            "iter 814, loss = tf.Tensor(0.68876606, shape=(), dtype=float32)\n",
            "iter 815, loss = tf.Tensor(0.6887656, shape=(), dtype=float32)\n",
            "iter 816, loss = tf.Tensor(0.6887651, shape=(), dtype=float32)\n",
            "iter 817, loss = tf.Tensor(0.68876463, shape=(), dtype=float32)\n",
            "iter 818, loss = tf.Tensor(0.68876415, shape=(), dtype=float32)\n",
            "iter 819, loss = tf.Tensor(0.6887637, shape=(), dtype=float32)\n",
            "iter 820, loss = tf.Tensor(0.68876314, shape=(), dtype=float32)\n",
            "iter 821, loss = tf.Tensor(0.6887627, shape=(), dtype=float32)\n",
            "iter 822, loss = tf.Tensor(0.68876237, shape=(), dtype=float32)\n",
            "iter 823, loss = tf.Tensor(0.6887618, shape=(), dtype=float32)\n",
            "iter 824, loss = tf.Tensor(0.68876135, shape=(), dtype=float32)\n",
            "iter 825, loss = tf.Tensor(0.6887609, shape=(), dtype=float32)\n",
            "iter 826, loss = tf.Tensor(0.6887603, shape=(), dtype=float32)\n",
            "iter 827, loss = tf.Tensor(0.6887598, shape=(), dtype=float32)\n",
            "iter 828, loss = tf.Tensor(0.6887594, shape=(), dtype=float32)\n",
            "iter 829, loss = tf.Tensor(0.6887589, shape=(), dtype=float32)\n",
            "iter 830, loss = tf.Tensor(0.6887583, shape=(), dtype=float32)\n",
            "iter 831, loss = tf.Tensor(0.6887579, shape=(), dtype=float32)\n",
            "iter 832, loss = tf.Tensor(0.6887575, shape=(), dtype=float32)\n",
            "iter 833, loss = tf.Tensor(0.68875694, shape=(), dtype=float32)\n",
            "iter 834, loss = tf.Tensor(0.68875635, shape=(), dtype=float32)\n",
            "iter 835, loss = tf.Tensor(0.6887559, shape=(), dtype=float32)\n",
            "iter 836, loss = tf.Tensor(0.68875545, shape=(), dtype=float32)\n",
            "iter 837, loss = tf.Tensor(0.68875486, shape=(), dtype=float32)\n",
            "iter 838, loss = tf.Tensor(0.6887545, shape=(), dtype=float32)\n",
            "iter 839, loss = tf.Tensor(0.68875396, shape=(), dtype=float32)\n",
            "iter 840, loss = tf.Tensor(0.68875337, shape=(), dtype=float32)\n",
            "iter 841, loss = tf.Tensor(0.68875283, shape=(), dtype=float32)\n",
            "iter 842, loss = tf.Tensor(0.6887524, shape=(), dtype=float32)\n",
            "iter 843, loss = tf.Tensor(0.6887519, shape=(), dtype=float32)\n",
            "iter 844, loss = tf.Tensor(0.68875134, shape=(), dtype=float32)\n",
            "iter 845, loss = tf.Tensor(0.6887509, shape=(), dtype=float32)\n",
            "iter 846, loss = tf.Tensor(0.6887504, shape=(), dtype=float32)\n",
            "iter 847, loss = tf.Tensor(0.68874985, shape=(), dtype=float32)\n",
            "iter 848, loss = tf.Tensor(0.6887493, shape=(), dtype=float32)\n",
            "iter 849, loss = tf.Tensor(0.68874884, shape=(), dtype=float32)\n",
            "iter 850, loss = tf.Tensor(0.68874836, shape=(), dtype=float32)\n",
            "iter 851, loss = tf.Tensor(0.6887478, shape=(), dtype=float32)\n",
            "iter 852, loss = tf.Tensor(0.68874735, shape=(), dtype=float32)\n",
            "iter 853, loss = tf.Tensor(0.68874675, shape=(), dtype=float32)\n",
            "iter 854, loss = tf.Tensor(0.68874615, shape=(), dtype=float32)\n",
            "iter 855, loss = tf.Tensor(0.6887457, shape=(), dtype=float32)\n",
            "iter 856, loss = tf.Tensor(0.6887452, shape=(), dtype=float32)\n",
            "iter 857, loss = tf.Tensor(0.68874466, shape=(), dtype=float32)\n",
            "iter 858, loss = tf.Tensor(0.6887441, shape=(), dtype=float32)\n",
            "iter 859, loss = tf.Tensor(0.68874353, shape=(), dtype=float32)\n",
            "iter 860, loss = tf.Tensor(0.68874305, shape=(), dtype=float32)\n",
            "iter 861, loss = tf.Tensor(0.6887425, shape=(), dtype=float32)\n",
            "iter 862, loss = tf.Tensor(0.6887419, shape=(), dtype=float32)\n",
            "iter 863, loss = tf.Tensor(0.6887415, shape=(), dtype=float32)\n",
            "iter 864, loss = tf.Tensor(0.6887409, shape=(), dtype=float32)\n",
            "iter 865, loss = tf.Tensor(0.6887404, shape=(), dtype=float32)\n",
            "iter 866, loss = tf.Tensor(0.6887399, shape=(), dtype=float32)\n",
            "iter 867, loss = tf.Tensor(0.6887393, shape=(), dtype=float32)\n",
            "iter 868, loss = tf.Tensor(0.6887388, shape=(), dtype=float32)\n",
            "iter 869, loss = tf.Tensor(0.6887383, shape=(), dtype=float32)\n",
            "iter 870, loss = tf.Tensor(0.68873775, shape=(), dtype=float32)\n",
            "iter 871, loss = tf.Tensor(0.68873715, shape=(), dtype=float32)\n",
            "iter 872, loss = tf.Tensor(0.6887366, shape=(), dtype=float32)\n",
            "iter 873, loss = tf.Tensor(0.6887361, shape=(), dtype=float32)\n",
            "iter 874, loss = tf.Tensor(0.68873554, shape=(), dtype=float32)\n",
            "iter 875, loss = tf.Tensor(0.688735, shape=(), dtype=float32)\n",
            "iter 876, loss = tf.Tensor(0.6887345, shape=(), dtype=float32)\n",
            "iter 877, loss = tf.Tensor(0.68873394, shape=(), dtype=float32)\n",
            "iter 878, loss = tf.Tensor(0.6887334, shape=(), dtype=float32)\n",
            "iter 879, loss = tf.Tensor(0.68873286, shape=(), dtype=float32)\n",
            "iter 880, loss = tf.Tensor(0.6887323, shape=(), dtype=float32)\n",
            "iter 881, loss = tf.Tensor(0.6887317, shape=(), dtype=float32)\n",
            "iter 882, loss = tf.Tensor(0.6887312, shape=(), dtype=float32)\n",
            "iter 883, loss = tf.Tensor(0.6887306, shape=(), dtype=float32)\n",
            "iter 884, loss = tf.Tensor(0.68873006, shape=(), dtype=float32)\n",
            "iter 885, loss = tf.Tensor(0.6887296, shape=(), dtype=float32)\n",
            "iter 886, loss = tf.Tensor(0.688729, shape=(), dtype=float32)\n",
            "iter 887, loss = tf.Tensor(0.6887284, shape=(), dtype=float32)\n",
            "iter 888, loss = tf.Tensor(0.6887278, shape=(), dtype=float32)\n",
            "iter 889, loss = tf.Tensor(0.68872726, shape=(), dtype=float32)\n",
            "iter 890, loss = tf.Tensor(0.6887267, shape=(), dtype=float32)\n",
            "iter 891, loss = tf.Tensor(0.6887262, shape=(), dtype=float32)\n",
            "iter 892, loss = tf.Tensor(0.68872565, shape=(), dtype=float32)\n",
            "iter 893, loss = tf.Tensor(0.688725, shape=(), dtype=float32)\n",
            "iter 894, loss = tf.Tensor(0.6887245, shape=(), dtype=float32)\n",
            "iter 895, loss = tf.Tensor(0.688724, shape=(), dtype=float32)\n",
            "iter 896, loss = tf.Tensor(0.6887233, shape=(), dtype=float32)\n",
            "iter 897, loss = tf.Tensor(0.6887228, shape=(), dtype=float32)\n",
            "iter 898, loss = tf.Tensor(0.68872225, shape=(), dtype=float32)\n",
            "iter 899, loss = tf.Tensor(0.6887217, shape=(), dtype=float32)\n",
            "iter 900, loss = tf.Tensor(0.6887212, shape=(), dtype=float32)\n",
            "iter 901, loss = tf.Tensor(0.6887206, shape=(), dtype=float32)\n",
            "iter 902, loss = tf.Tensor(0.68871987, shape=(), dtype=float32)\n",
            "iter 903, loss = tf.Tensor(0.6887194, shape=(), dtype=float32)\n",
            "iter 904, loss = tf.Tensor(0.6887188, shape=(), dtype=float32)\n",
            "iter 905, loss = tf.Tensor(0.68871826, shape=(), dtype=float32)\n",
            "iter 906, loss = tf.Tensor(0.6887177, shape=(), dtype=float32)\n",
            "iter 907, loss = tf.Tensor(0.6887171, shape=(), dtype=float32)\n",
            "iter 908, loss = tf.Tensor(0.6887165, shape=(), dtype=float32)\n",
            "iter 909, loss = tf.Tensor(0.68871593, shape=(), dtype=float32)\n",
            "iter 910, loss = tf.Tensor(0.6887154, shape=(), dtype=float32)\n",
            "iter 911, loss = tf.Tensor(0.6887148, shape=(), dtype=float32)\n",
            "iter 912, loss = tf.Tensor(0.6887142, shape=(), dtype=float32)\n",
            "iter 913, loss = tf.Tensor(0.68871367, shape=(), dtype=float32)\n",
            "iter 914, loss = tf.Tensor(0.68871313, shape=(), dtype=float32)\n",
            "iter 915, loss = tf.Tensor(0.68871254, shape=(), dtype=float32)\n",
            "iter 916, loss = tf.Tensor(0.68871194, shape=(), dtype=float32)\n",
            "iter 917, loss = tf.Tensor(0.68871135, shape=(), dtype=float32)\n",
            "iter 918, loss = tf.Tensor(0.6887108, shape=(), dtype=float32)\n",
            "iter 919, loss = tf.Tensor(0.6887103, shape=(), dtype=float32)\n",
            "iter 920, loss = tf.Tensor(0.68870956, shape=(), dtype=float32)\n",
            "iter 921, loss = tf.Tensor(0.688709, shape=(), dtype=float32)\n",
            "iter 922, loss = tf.Tensor(0.6887085, shape=(), dtype=float32)\n",
            "iter 923, loss = tf.Tensor(0.6887079, shape=(), dtype=float32)\n",
            "iter 924, loss = tf.Tensor(0.6887073, shape=(), dtype=float32)\n",
            "iter 925, loss = tf.Tensor(0.68870664, shape=(), dtype=float32)\n",
            "iter 926, loss = tf.Tensor(0.68870604, shape=(), dtype=float32)\n",
            "iter 927, loss = tf.Tensor(0.6887055, shape=(), dtype=float32)\n",
            "iter 928, loss = tf.Tensor(0.6887049, shape=(), dtype=float32)\n",
            "iter 929, loss = tf.Tensor(0.6887043, shape=(), dtype=float32)\n",
            "iter 930, loss = tf.Tensor(0.6887038, shape=(), dtype=float32)\n",
            "iter 931, loss = tf.Tensor(0.68870306, shape=(), dtype=float32)\n",
            "iter 932, loss = tf.Tensor(0.6887025, shape=(), dtype=float32)\n",
            "iter 933, loss = tf.Tensor(0.6887019, shape=(), dtype=float32)\n",
            "iter 934, loss = tf.Tensor(0.68870133, shape=(), dtype=float32)\n",
            "iter 935, loss = tf.Tensor(0.6887007, shape=(), dtype=float32)\n",
            "iter 936, loss = tf.Tensor(0.6887001, shape=(), dtype=float32)\n",
            "iter 937, loss = tf.Tensor(0.6886995, shape=(), dtype=float32)\n",
            "iter 938, loss = tf.Tensor(0.6886989, shape=(), dtype=float32)\n",
            "iter 939, loss = tf.Tensor(0.68869823, shape=(), dtype=float32)\n",
            "iter 940, loss = tf.Tensor(0.6886976, shape=(), dtype=float32)\n",
            "iter 941, loss = tf.Tensor(0.688697, shape=(), dtype=float32)\n",
            "iter 942, loss = tf.Tensor(0.68869644, shape=(), dtype=float32)\n",
            "iter 943, loss = tf.Tensor(0.6886958, shape=(), dtype=float32)\n",
            "iter 944, loss = tf.Tensor(0.68869513, shape=(), dtype=float32)\n",
            "iter 945, loss = tf.Tensor(0.68869454, shape=(), dtype=float32)\n",
            "iter 946, loss = tf.Tensor(0.68869394, shape=(), dtype=float32)\n",
            "iter 947, loss = tf.Tensor(0.6886933, shape=(), dtype=float32)\n",
            "iter 948, loss = tf.Tensor(0.6886926, shape=(), dtype=float32)\n",
            "iter 949, loss = tf.Tensor(0.6886921, shape=(), dtype=float32)\n",
            "iter 950, loss = tf.Tensor(0.6886914, shape=(), dtype=float32)\n",
            "iter 951, loss = tf.Tensor(0.68869084, shape=(), dtype=float32)\n",
            "iter 952, loss = tf.Tensor(0.6886902, shape=(), dtype=float32)\n",
            "iter 953, loss = tf.Tensor(0.6886896, shape=(), dtype=float32)\n",
            "iter 954, loss = tf.Tensor(0.6886888, shape=(), dtype=float32)\n",
            "iter 955, loss = tf.Tensor(0.6886883, shape=(), dtype=float32)\n",
            "iter 956, loss = tf.Tensor(0.6886876, shape=(), dtype=float32)\n",
            "iter 957, loss = tf.Tensor(0.6886869, shape=(), dtype=float32)\n",
            "iter 958, loss = tf.Tensor(0.6886864, shape=(), dtype=float32)\n",
            "iter 959, loss = tf.Tensor(0.6886857, shape=(), dtype=float32)\n",
            "iter 960, loss = tf.Tensor(0.68868506, shape=(), dtype=float32)\n",
            "iter 961, loss = tf.Tensor(0.68868446, shape=(), dtype=float32)\n",
            "iter 962, loss = tf.Tensor(0.68868375, shape=(), dtype=float32)\n",
            "iter 963, loss = tf.Tensor(0.68868315, shape=(), dtype=float32)\n",
            "iter 964, loss = tf.Tensor(0.68868244, shape=(), dtype=float32)\n",
            "iter 965, loss = tf.Tensor(0.6886819, shape=(), dtype=float32)\n",
            "iter 966, loss = tf.Tensor(0.6886812, shape=(), dtype=float32)\n",
            "iter 967, loss = tf.Tensor(0.6886805, shape=(), dtype=float32)\n",
            "iter 968, loss = tf.Tensor(0.6886799, shape=(), dtype=float32)\n",
            "iter 969, loss = tf.Tensor(0.68867934, shape=(), dtype=float32)\n",
            "iter 970, loss = tf.Tensor(0.68867856, shape=(), dtype=float32)\n",
            "iter 971, loss = tf.Tensor(0.6886779, shape=(), dtype=float32)\n",
            "iter 972, loss = tf.Tensor(0.6886773, shape=(), dtype=float32)\n",
            "iter 973, loss = tf.Tensor(0.6886766, shape=(), dtype=float32)\n",
            "iter 974, loss = tf.Tensor(0.68867594, shape=(), dtype=float32)\n",
            "iter 975, loss = tf.Tensor(0.6886753, shape=(), dtype=float32)\n",
            "iter 976, loss = tf.Tensor(0.68867457, shape=(), dtype=float32)\n",
            "iter 977, loss = tf.Tensor(0.688674, shape=(), dtype=float32)\n",
            "iter 978, loss = tf.Tensor(0.6886734, shape=(), dtype=float32)\n",
            "iter 979, loss = tf.Tensor(0.6886726, shape=(), dtype=float32)\n",
            "iter 980, loss = tf.Tensor(0.688672, shape=(), dtype=float32)\n",
            "iter 981, loss = tf.Tensor(0.68867135, shape=(), dtype=float32)\n",
            "iter 982, loss = tf.Tensor(0.68867064, shape=(), dtype=float32)\n",
            "iter 983, loss = tf.Tensor(0.68867, shape=(), dtype=float32)\n",
            "iter 984, loss = tf.Tensor(0.6886693, shape=(), dtype=float32)\n",
            "iter 985, loss = tf.Tensor(0.6886686, shape=(), dtype=float32)\n",
            "iter 986, loss = tf.Tensor(0.68866795, shape=(), dtype=float32)\n",
            "iter 987, loss = tf.Tensor(0.68866724, shape=(), dtype=float32)\n",
            "iter 988, loss = tf.Tensor(0.6886666, shape=(), dtype=float32)\n",
            "iter 989, loss = tf.Tensor(0.6886659, shape=(), dtype=float32)\n",
            "iter 990, loss = tf.Tensor(0.68866515, shape=(), dtype=float32)\n",
            "iter 991, loss = tf.Tensor(0.68866456, shape=(), dtype=float32)\n",
            "iter 992, loss = tf.Tensor(0.6886639, shape=(), dtype=float32)\n",
            "iter 993, loss = tf.Tensor(0.6886631, shape=(), dtype=float32)\n",
            "iter 994, loss = tf.Tensor(0.68866247, shape=(), dtype=float32)\n",
            "iter 995, loss = tf.Tensor(0.68866175, shape=(), dtype=float32)\n",
            "iter 996, loss = tf.Tensor(0.6886611, shape=(), dtype=float32)\n",
            "iter 997, loss = tf.Tensor(0.6886603, shape=(), dtype=float32)\n",
            "iter 998, loss = tf.Tensor(0.68865967, shape=(), dtype=float32)\n",
            "iter 999, loss = tf.Tensor(0.688659, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "qiURy8u3EXxk",
        "outputId": "059a6f7d-e31a-4f42-be96-c47d6c37e227"
      },
      "source": [
        "plot_curve(range(num_iterations), losses)"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCDtV55FV9jy"
      },
      "source": [
        "### Different Optimizers\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cSlZPobWTrX"
      },
      "source": [
        "#### SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOu3xg7wEdRz"
      },
      "source": [
        ""
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6rWZ1jQWZnE"
      },
      "source": [
        "#### SGD with momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V88z1YeaGfma"
      },
      "source": [
        ""
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBcIbagLWeHs"
      },
      "source": [
        "#### RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9wZZsFqQynq"
      },
      "source": [
        ""
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYU_WHmNWgfJ"
      },
      "source": [
        "#### Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUylNxDZQ1np"
      },
      "source": [
        ""
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBtYRSioWmtE"
      },
      "source": [
        "#### Adam with Linear Warmup and linear decay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTEwYM_ZQ2fN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RFUTJPzceJi"
      },
      "source": [
        ""
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMCoqI77clo7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRKIOwFLezsk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}