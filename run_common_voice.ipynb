{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_common_voice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyONf8W7+ZCCHhVe2+sfO7yR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "91c92540067a4b96b2e54e9f381770ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_191407a9953e4ba9adae59a3f869ecfd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_879e2e97b92d427b8f94012893083abb",
              "IPY_MODEL_aeaa9eb8f1544804ae2293889a9a170a"
            ]
          }
        },
        "191407a9953e4ba9adae59a3f869ecfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "879e2e97b92d427b8f94012893083abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3944dec8acba496680223b4770238e11",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 13,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bba08af6be4e47bf8cc0be0c307d0b31"
          }
        },
        "aeaa9eb8f1544804ae2293889a9a170a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_492556fed3374b66b0da8d9e9d5a95ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/13 [16:32&lt;00:00, 76.37s/ex]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c36a4a7bef0349c8ace2b43b4a8d36aa"
          }
        },
        "3944dec8acba496680223b4770238e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bba08af6be4e47bf8cc0be0c307d0b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "492556fed3374b66b0da8d9e9d5a95ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c36a4a7bef0349c8ace2b43b4a8d36aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1277a0544d644dba89c70e999e9eafb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1d2c934fbe7e4d67b75f4dfacd4d7d36",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_60aa19d10221445da85b3f39a9991e47",
              "IPY_MODEL_43d6c229db214fc8a4325d972cadaaec"
            ]
          }
        },
        "1d2c934fbe7e4d67b75f4dfacd4d7d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60aa19d10221445da85b3f39a9991e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c6d67fed40e040bc93a55ed43ca9b412",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1764,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1764,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93f58c64502a4626b9078c48d3960c5b"
          }
        },
        "43d6c229db214fc8a4325d972cadaaec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de43c233b76f44a0a16949069d7f5280",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3.90k/? [00:01&lt;00:00, 3.54kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_734587d7751541f2a5d8c3eab92374d7"
          }
        },
        "c6d67fed40e040bc93a55ed43ca9b412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93f58c64502a4626b9078c48d3960c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de43c233b76f44a0a16949069d7f5280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "734587d7751541f2a5d8c3eab92374d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c513c0f46be941508f3de076f1e294ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1fb8c45f2e3c45e88e890571b4c82bbc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c644f82626f14964bd6550ec01cbb873",
              "IPY_MODEL_19771a051e4146389d3efc528afb76a5"
            ]
          }
        },
        "1fb8c45f2e3c45e88e890571b4c82bbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c644f82626f14964bd6550ec01cbb873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_78e7dac1aca24db588d79013523a2f92",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 127,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 127,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6fe1000eaad4b848b80c656cdd690ee"
          }
        },
        "19771a051e4146389d3efc528afb76a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_75c4b61e74684996819b4a48c022ca57",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 127/127 [00:02&lt;00:00, 50.06ex/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00dd3de99cd74b5695b6f83f360b4f82"
          }
        },
        "78e7dac1aca24db588d79013523a2f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6fe1000eaad4b848b80c656cdd690ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75c4b61e74684996819b4a48c022ca57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00dd3de99cd74b5695b6f83f360b4f82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55e112a12c1c435dbe1114cb7a762f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_36a1660d631e4b079222f7ee3c684b99",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7fb33cd2bd4d42b08d220eaf9a3b642b",
              "IPY_MODEL_a39a0dc5b9264e02b4dc79b2a2570020"
            ]
          }
        },
        "36a1660d631e4b079222f7ee3c684b99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7fb33cd2bd4d42b08d220eaf9a3b642b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9dc9a46171d44ef48f951e7208e874e6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 16,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 16,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad54301240db4619ad2e632d72254942"
          }
        },
        "a39a0dc5b9264e02b4dc79b2a2570020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3b10e6c6ac5f4a81b0275964597426ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 16/16 [01:00&lt;00:00,  3.78s/ba]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e210796981c4ac29899003c5468fed9"
          }
        },
        "9dc9a46171d44ef48f951e7208e874e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad54301240db4619ad2e632d72254942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b10e6c6ac5f4a81b0275964597426ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e210796981c4ac29899003c5468fed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiwangi27/googlecolab/blob/main/run_common_voice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hykLW_7GWFk5",
        "outputId": "6c8a4670-8f6a-40c9-867d-6ac7932de58b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKNnPCW4XArj",
        "outputId": "4bf1bdae-90b8-4008-e93b-e45bfa9dc4b4"
      },
      "source": [
        "!pip install -r /content/drive/MyDrive/Projects/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 12.1MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/43b396481a8298c6010afb93b3c1e71d4ba6f8c10797a7da8eb005e45081/datasets-1.5.0-py3-none-any.whl (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Projects/requirements.txt (line 3)) (1.8.1+cu101)\n",
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 54.5MB/s \n",
            "\u001b[?25hCollecting jiwer==2.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/cc/fb9d3132cba1f6d393b7d5a9398d9d4c8fc033bc54668cf87e9b197a6d7a/jiwer-2.2.0-py3-none-any.whl\n",
            "Collecting lang-trans==0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/75/b5/bc6d01112d8228e9acb45bfe0694c77bc8027144b4b8c297763ffdf6373e/lang-trans-0.6.0.tar.gz\n",
            "Requirement already satisfied: librosa==0.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 68.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 73.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets->-r /content/drive/MyDrive/Projects/requirements.txt (line 2)) (3.0.0)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/11/f7689b996f85e45f718745c899f6747ee5edb4878cadac0a41ab146828fa/fsspec-0.9.0-py3-none-any.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 59.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets->-r /content/drive/MyDrive/Projects/requirements.txt (line 2)) (1.1.5)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->-r /content/drive/MyDrive/Projects/requirements.txt (line 2)) (0.70.11.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->-r /content/drive/MyDrive/Projects/requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 3)) (3.7.4.3)\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (0.10.3.post1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (0.22.2.post1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (2.1.9)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (0.51.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/drive/MyDrive/Projects/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->-r /content/drive/MyDrive/Projects/requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->-r /content/drive/MyDrive/Projects/requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->jiwer==2.2.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 5)) (54.2.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (1.14.5)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (0.34.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.8.0->-r /content/drive/MyDrive/Projects/requirements.txt (line 7)) (2.20)\n",
            "Building wheels for collected packages: lang-trans, sacremoses, python-Levenshtein\n",
            "  Building wheel for lang-trans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lang-trans: filename=lang_trans-0.6.0-cp37-none-any.whl size=6349 sha256=6dcc308bc9033447748cc425f02683208b9ff1e1f196c3d72b1aef02a233304b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/b9/9a/0990a0b22fd12190658d7539bbfec841de1324b9fb247f1c1b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=f90d773fbceff8e844ad9f60080f3516fec3a87fc0ab800741c371a6d88f56dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149795 sha256=0c226ca062ae26582c47692678acd674b6a8ca0c16452d2c7e75d8449cff3b36\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/26/73/4b48503bac73f01cf18e52cd250947049a7f339e940c5df8fc\n",
            "Successfully built lang-trans sacremoses python-Levenshtein\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, fsspec, huggingface-hub, xxhash, datasets, torchaudio, python-Levenshtein, jiwer, lang-trans\n",
            "Successfully installed datasets-1.5.0 fsspec-0.9.0 huggingface-hub-0.0.8 jiwer-2.2.0 lang-trans-0.6.0 python-Levenshtein-0.12.2 sacremoses-0.0.44 tokenizers-0.10.2 torchaudio-0.8.1 transformers-4.5.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbR8PvzPl39n"
      },
      "source": [
        "! rm -rf ~/.cache/huggingface/datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr5lxZ0roHO-",
        "outputId": "a79a3cba-3b5a-4236-8f63-91d930e680f5"
      },
      "source": [
        "!python /content/drive/MyDrive/Projects/run_common_voice.py --model_name_or_path=\"facebook/wav2vec2-large-xlsr-53\" --dataset_config_name=\"hi\" --output_dir=\"/content/drive/MyDrive/Projects/models/wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5\" --datasets_save_path=/content/drive/MyDrive/Projects/datasets --external_data_path=/content/drive/MyDrive/Projects/OpenSLR_Hindi_Data.zip --max_train_samples=10000 --overwrite_output_dir --num_train_epochs=\"30\" --per_device_train_batch_size=\"16\" --learning_rate=\"2e-5\" --preprocessing_num_workers=\"1\" --warmup_steps=\"500\" --lr_scheduler_type=\"constant_with_warmup\" --evaluation_strategy=\"steps\" --save_steps=\"400\" --eval_steps=\"400\" --logging_steps=\"400\" --save_total_limit=\"1\" --freeze_feature_extractor --feat_proj_dropout=\"0.0\" --layerdrop=\"0.1\" --mask_time_prob=\"0.05\" --gradient_accumulation_steps=\"1\" --gradient_checkpointing --fp16 --group_by_length --do_train --do_eval\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-09 03:18:34.343784: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/09/2021 03:18:37 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "04/09/2021 03:18:37 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.STEPS, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=30.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT_WITH_WARMUP, warmup_ratio=0.0, warmup_steps=500, logging_dir=runs/Apr09_03-18-37_f82b899af71a, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=400, save_strategy=IntervalStrategy.STEPS, save_steps=400, save_total_limit=1, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=400, dataloader_num_workers=0, past_index=-1, run_name=./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=True, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n",
            "04/09/2021 03:18:39 - WARNING - datasets.builder -   Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/hi/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n",
            "04/09/2021 03:18:39 - WARNING - datasets.builder -   Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/hi/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n",
            "loading configuration file https://huggingface.co/facebook/wav2vec2-large-xlsr-53/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8508c73cd595eb416a1d517b90762416c0bc6cfbef529578079aeae4d8c14336.9c1655e075d9ef07cda724db675f9067777f6eb2dd14269a834fcde8a48e825a\n",
            "Model config Wav2Vec2Config {\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2Model\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"conv_bias\": true,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"mean\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"do_stable_layer_norm\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_dropout\": 0.0,\n",
            "  \"feat_extract_norm\": \"layer\",\n",
            "  \"feat_proj_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"gradient_checkpointing\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.1,\n",
            "  \"mask_channel_length\": 10,\n",
            "  \"mask_channel_min_space\": 1,\n",
            "  \"mask_channel_other\": 0.0,\n",
            "  \"mask_channel_prob\": 0.0,\n",
            "  \"mask_channel_selection\": \"static\",\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_space\": 1,\n",
            "  \"mask_time_other\": 0.0,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"mask_time_selection\": \"static\",\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 71,\n",
            "  \"transformers_version\": \"4.5.0\",\n",
            "  \"vocab_size\": 72\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/facebook/wav2vec2-large-xlsr-53/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/5d2a20b45a1689a376ec4a6282b9d9be42f931cdf8daf07c3668ba1070a059d9.db2a69eb44bf7b1efcfff155d4cc22155230bd8c0941701b064e9c17429a623d\n",
            "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
            "\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "100% 625/625 [05:24<00:00,  1.93ba/s]\n",
            "100% 8/8 [00:04<00:00,  1.67ba/s]\n",
            "Using amp fp16 backend\n",
            "***** Running training *****\n",
            "  Num examples = 10000\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 18750\n",
            "  0% 0/18750 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'loss': 13.0738, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.64}\n",
            "  2% 400/18750 [13:42<5:19:37,  1.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.65it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.11it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.96it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.52it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.61it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:03,  1.66it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.67it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.55it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 4.600863456726074, 'eval_wer': 1.0, 'eval_runtime': 11.2, 'eval_samples_per_second': 11.339, 'epoch': 0.64}\n",
            "  2% 400/18750 [13:54<5:19:37,  1.05s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-400/preprocessor_config.json\n",
            "{'loss': 3.6545, 'learning_rate': 2e-05, 'epoch': 1.28}\n",
            "  4% 800/18750 [28:13<10:04:51,  2.02s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.96it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.65it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.51it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.70it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 3.735856056213379, 'eval_wer': 1.0, 'eval_runtime': 11.1814, 'eval_samples_per_second': 11.358, 'epoch': 1.28}\n",
            "  4% 800/18750 [28:24<10:04:51,  2.02s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-800/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-800/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-800/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 3.4449, 'learning_rate': 2e-05, 'epoch': 1.92}\n",
            "  6% 1200/18750 [42:35<10:00:09,  2.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.32it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 3.6783628463745117, 'eval_wer': 1.0, 'eval_runtime': 11.1822, 'eval_samples_per_second': 11.357, 'epoch': 1.92}\n",
            "  6% 1200/18750 [42:46<10:00:09,  2.05s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1200/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1200/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1200/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 3.4076, 'learning_rate': 2e-05, 'epoch': 2.56}\n",
            "  9% 1600/18750 [56:43<4:36:50,  1.03it/s]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.45it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.29it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 3.5885298252105713, 'eval_wer': 1.0, 'eval_runtime': 11.2645, 'eval_samples_per_second': 11.274, 'epoch': 2.56}\n",
            "  9% 1600/18750 [56:55<4:36:50,  1.03it/s]\n",
            "100% 16/16 [00:10<00:00,  1.27it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1600/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1600/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1600/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 3.2076, 'learning_rate': 2e-05, 'epoch': 3.2}\n",
            " 11% 2000/18750 [1:11:30<9:39:56,  2.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.54it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.53it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.70it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 3.0720324516296387, 'eval_wer': 1.0, 'eval_runtime': 11.117, 'eval_samples_per_second': 11.424, 'epoch': 3.2}\n",
            " 11% 2000/18750 [1:11:41<9:39:56,  2.08s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2000/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2000/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2000/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-1600] due to args.save_total_limit\n",
            "{'loss': 2.3955, 'learning_rate': 2e-05, 'epoch': 3.84}\n",
            " 13% 2400/18750 [1:25:45<9:12:14,  2.03s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.70it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.31it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.9947023391723633, 'eval_wer': 0.9153132250580046, 'eval_runtime': 11.1983, 'eval_samples_per_second': 11.341, 'epoch': 3.84}\n",
            " 13% 2400/18750 [1:25:56<9:12:14,  2.03s/it]\n",
            "100% 16/16 [00:10<00:00,  1.28it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2400/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 1.6606, 'learning_rate': 2e-05, 'epoch': 4.48}\n",
            " 15% 2800/18750 [1:39:49<4:47:37,  1.08s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.62it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.06it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.91it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.64it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.51it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.60it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:03,  1.64it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.65it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.54it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.46it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.32it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.588626742362976, 'eval_wer': 0.8306264501160093, 'eval_runtime': 11.3323, 'eval_samples_per_second': 11.207, 'epoch': 4.48}\n",
            " 15% 2800/18750 [1:40:00<4:47:37,  1.08s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2800/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2800/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2800/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2400] due to args.save_total_limit\n",
            "{'loss': 1.3153, 'learning_rate': 2e-05, 'epoch': 5.12}\n",
            " 17% 3200/18750 [1:54:31<8:47:17,  2.03s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.70it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.4265543222427368, 'eval_wer': 0.7726218097447796, 'eval_runtime': 11.1353, 'eval_samples_per_second': 11.405, 'epoch': 5.12}\n",
            " 17% 3200/18750 [1:54:42<8:47:17,  2.03s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3200/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3200/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3200/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-2800] due to args.save_total_limit\n",
            "{'loss': 1.1603, 'learning_rate': 2e-05, 'epoch': 5.76}\n",
            " 19% 3600/18750 [2:08:57<8:42:01,  2.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.64it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.10it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.96it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.66it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.52it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.3472371101379395, 'eval_wer': 0.7412993039443155, 'eval_runtime': 11.2027, 'eval_samples_per_second': 11.337, 'epoch': 5.76}\n",
            " 19% 3600/18750 [2:09:08<8:42:01,  2.07s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3600/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3600/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3600/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3200] due to args.save_total_limit\n",
            "{'loss': 1.0426, 'learning_rate': 2e-05, 'epoch': 6.4}\n",
            " 21% 4000/18750 [2:23:15<4:22:48,  1.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.63it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.49it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.40it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.49it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.50it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.61it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.67it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2793620824813843, 'eval_wer': 0.728538283062645, 'eval_runtime': 11.2626, 'eval_samples_per_second': 11.276, 'epoch': 6.4}\n",
            " 21% 4000/18750 [2:23:26<4:22:48,  1.07s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4000/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4000/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4000/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-3600] due to args.save_total_limit\n",
            "{'loss': 0.9581, 'learning_rate': 2e-05, 'epoch': 7.04}\n",
            " 23% 4400/18750 [2:37:57<8:25:10,  2.11s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.66it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.11it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.94it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.64it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.49it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.41it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.50it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.50it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.61it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.67it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.55it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2667772769927979, 'eval_wer': 0.7006960556844548, 'eval_runtime': 11.2629, 'eval_samples_per_second': 11.276, 'epoch': 7.04}\n",
            " 23% 4400/18750 [2:38:09<8:25:10,  2.11s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4400/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 0.8893, 'learning_rate': 2e-05, 'epoch': 7.68}\n",
            " 26% 4800/18750 [2:52:28<7:58:08,  2.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.60it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:03,  1.66it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2468397617340088, 'eval_wer': 0.6740139211136891, 'eval_runtime': 11.1927, 'eval_samples_per_second': 11.347, 'epoch': 7.68}\n",
            " 26% 4800/18750 [2:52:39<7:58:08,  2.06s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4800/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4800/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4800/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4400] due to args.save_total_limit\n",
            "{'loss': 0.8475, 'learning_rate': 2e-05, 'epoch': 8.32}\n",
            " 28% 5200/18750 [3:06:35<3:58:14,  1.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.51it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2307798862457275, 'eval_wer': 0.6566125290023201, 'eval_runtime': 11.1606, 'eval_samples_per_second': 11.379, 'epoch': 8.32}\n",
            " 28% 5200/18750 [3:06:46<3:58:14,  1.05s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5200/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5200/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5200/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-4800] due to args.save_total_limit\n",
            "{'loss': 0.8003, 'learning_rate': 2e-05, 'epoch': 8.96}\n",
            " 30% 5600/18750 [3:21:04<3:55:00,  1.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2240986824035645, 'eval_wer': 0.642691415313225, 'eval_runtime': 11.1424, 'eval_samples_per_second': 11.398, 'epoch': 8.96}\n",
            " 30% 5600/18750 [3:21:16<3:55:00,  1.07s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5600/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5600/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5600/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5200] due to args.save_total_limit\n",
            "{'loss': 0.761, 'learning_rate': 2e-05, 'epoch': 9.6}\n",
            " 32% 6000/18750 [3:35:46<6:55:18,  1.95s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.34it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.222487449645996, 'eval_wer': 0.648491879350348, 'eval_runtime': 11.1522, 'eval_samples_per_second': 11.388, 'epoch': 9.6}\n",
            " 32% 6000/18750 [3:35:58<6:55:18,  1.95s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6000/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6000/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6000/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-5600] due to args.save_total_limit\n",
            "{'loss': 0.7321, 'learning_rate': 2e-05, 'epoch': 10.24}\n",
            " 34% 6400/18750 [3:49:56<3:40:20,  1.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.32it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2313493490219116, 'eval_wer': 0.6415313225058005, 'eval_runtime': 11.1635, 'eval_samples_per_second': 11.376, 'epoch': 10.24}\n",
            " 34% 6400/18750 [3:50:07<3:40:20,  1.07s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6400/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6000] due to args.save_total_limit\n",
            "{'loss': 0.7114, 'learning_rate': 2e-05, 'epoch': 10.88}\n",
            " 36% 6800/18750 [4:04:16<3:27:35,  1.04s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.66it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.11it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.96it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2032357454299927, 'eval_wer': 0.617169373549884, 'eval_runtime': 11.171, 'eval_samples_per_second': 11.369, 'epoch': 10.88}\n",
            " 36% 6800/18750 [4:04:28<3:27:35,  1.04s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6800/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6800/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6800/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6400] due to args.save_total_limit\n",
            "{'loss': 0.6789, 'learning_rate': 2e-05, 'epoch': 11.52}\n",
            " 38% 7200/18750 [4:18:48<6:11:44,  1.93s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.49it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.60it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.67it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2387803792953491, 'eval_wer': 0.6264501160092807, 'eval_runtime': 11.1889, 'eval_samples_per_second': 11.351, 'epoch': 11.52}\n",
            " 38% 7200/18750 [4:18:59<6:11:44,  1.93s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7200/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7200/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7200/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-6800] due to args.save_total_limit\n",
            "{'loss': 0.6522, 'learning_rate': 2e-05, 'epoch': 12.16}\n",
            " 41% 7600/18750 [4:33:03<3:13:19,  1.04s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.96it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.52it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.49it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.59it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:03,  1.65it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.67it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.55it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.45it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.30it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.235715389251709, 'eval_wer': 0.6322505800464037, 'eval_runtime': 11.3191, 'eval_samples_per_second': 11.22, 'epoch': 12.16}\n",
            " 41% 7600/18750 [4:33:14<3:13:19,  1.04s/it]\n",
            "100% 16/16 [00:10<00:00,  1.27it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7600/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7600/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7600/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7200] due to args.save_total_limit\n",
            "{'loss': 0.6292, 'learning_rate': 2e-05, 'epoch': 12.8}\n",
            " 43% 8000/18750 [4:47:29<3:04:15,  1.03s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.67it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.93it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.65it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.51it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.42it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.50it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.67it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2083860635757446, 'eval_wer': 0.622969837587007, 'eval_runtime': 11.2228, 'eval_samples_per_second': 11.316, 'epoch': 12.8}\n",
            " 43% 8000/18750 [4:47:40<3:04:15,  1.03s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8000/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8000/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8000/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-7600] due to args.save_total_limit\n",
            "{'loss': 0.6191, 'learning_rate': 2e-05, 'epoch': 13.44}\n",
            " 45% 8400/18750 [5:02:00<5:39:36,  1.97s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.71it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.69it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.54it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.53it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.53it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.70it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.34it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2343614101409912, 'eval_wer': 0.6183294663573086, 'eval_runtime': 11.1555, 'eval_samples_per_second': 11.385, 'epoch': 13.44}\n",
            " 45% 8400/18750 [5:02:12<5:39:36,  1.97s/it]\n",
            "100% 16/16 [00:10<00:00,  1.28it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8400/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8000] due to args.save_total_limit\n",
            "{'loss': 0.6059, 'learning_rate': 2e-05, 'epoch': 14.08}\n",
            " 47% 8800/18750 [5:16:14<2:56:50,  1.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.60it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.05it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.92it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.65it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.51it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.42it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.61it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.67it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.55it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.253428339958191, 'eval_wer': 0.6136890951276102, 'eval_runtime': 11.2574, 'eval_samples_per_second': 11.281, 'epoch': 14.08}\n",
            " 47% 8800/18750 [5:16:25<2:56:50,  1.07s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8800/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8800/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8800/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8400] due to args.save_total_limit\n",
            "{'loss': 0.5755, 'learning_rate': 2e-05, 'epoch': 14.72}\n",
            " 49% 9200/18750 [5:30:33<2:50:16,  1.07s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.34it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.2543489933013916, 'eval_wer': 0.617169373549884, 'eval_runtime': 11.1471, 'eval_samples_per_second': 11.393, 'epoch': 14.72}\n",
            " 49% 9200/18750 [5:30:44<2:50:16,  1.07s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9200/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9200/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9200/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-8800] due to args.save_total_limit\n",
            "{'loss': 0.5545, 'learning_rate': 2e-05, 'epoch': 15.36}\n",
            " 51% 9600/18750 [5:45:17<5:07:27,  2.02s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.07it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.91it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.64it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.51it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.23753023147583, 'eval_wer': 0.605568445475638, 'eval_runtime': 11.2288, 'eval_samples_per_second': 11.31, 'epoch': 15.36}\n",
            " 51% 9600/18750 [5:45:28<5:07:27,  2.02s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9600/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9600/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9600/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9200] due to args.save_total_limit\n",
            "{'loss': 0.539, 'learning_rate': 2e-05, 'epoch': 16.0}\n",
            " 53% 10000/18750 [5:59:25<2:54:47,  1.20s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.53it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:03,  1.67it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.32it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.2773118019104004, 'eval_wer': 0.6183294663573086, 'eval_runtime': 11.1935, 'eval_samples_per_second': 11.346, 'epoch': 16.0}\n",
            " 53% 10000/18750 [5:59:36<2:54:47,  1.20s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10000/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10000/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10000/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-9600] due to args.save_total_limit\n",
            "{'loss': 0.5216, 'learning_rate': 2e-05, 'epoch': 16.64}\n",
            " 55% 10400/18750 [6:13:47<2:19:12,  1.00s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.70it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.14it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.54it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.46it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.31it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.2595456838607788, 'eval_wer': 0.5997679814385151, 'eval_runtime': 11.1736, 'eval_samples_per_second': 11.366, 'epoch': 16.64}\n",
            " 55% 10400/18750 [6:13:58<2:19:12,  1.00s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10400/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 0.5124, 'learning_rate': 2e-05, 'epoch': 17.28}\n",
            " 58% 10800/18750 [6:28:27<4:43:16,  2.14s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.70it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.14it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.69it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.54it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.53it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.70it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.55it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.44it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.31it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.2657837867736816, 'eval_wer': 0.5962877030162413, 'eval_runtime': 11.1938, 'eval_samples_per_second': 11.346, 'epoch': 17.28}\n",
            " 58% 10800/18750 [6:28:38<4:43:16,  2.14s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10800/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10800/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10800/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10400] due to args.save_total_limit\n",
            "{'loss': 0.5109, 'learning_rate': 2e-05, 'epoch': 17.92}\n",
            " 60% 11200/18750 [6:42:42<4:00:01,  1.91s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.274712324142456, 'eval_wer': 0.6102088167053364, 'eval_runtime': 11.1474, 'eval_samples_per_second': 11.393, 'epoch': 17.92}\n",
            " 60% 11200/18750 [6:42:53<4:00:01,  1.91s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11200/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11200/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11200/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-10800] due to args.save_total_limit\n",
            "{'loss': 0.4966, 'learning_rate': 2e-05, 'epoch': 18.56}\n",
            " 62% 11600/18750 [6:56:52<2:01:36,  1.02s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.96it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.66it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.52it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.61it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.67it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.55it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.46it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.2945005893707275, 'eval_wer': 0.611368909512761, 'eval_runtime': 11.2164, 'eval_samples_per_second': 11.323, 'epoch': 18.56}\n",
            " 62% 11600/18750 [6:57:03<2:01:36,  1.02s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11600/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11600/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11600/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11200] due to args.save_total_limit\n",
            "{'loss': 0.4769, 'learning_rate': 2e-05, 'epoch': 19.2}\n",
            " 64% 12000/18750 [7:11:32<3:48:14,  2.03s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.313339352607727, 'eval_wer': 0.5870069605568445, 'eval_runtime': 11.1628, 'eval_samples_per_second': 11.377, 'epoch': 19.2}\n",
            " 64% 12000/18750 [7:11:43<3:48:14,  2.03s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12000/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12000/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12000/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-11600] due to args.save_total_limit\n",
            "{'loss': 0.4536, 'learning_rate': 2e-05, 'epoch': 19.84}\n",
            " 66% 12400/18750 [7:25:55<3:33:26,  2.02s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.70it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:03,  1.66it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.66it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.54it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.46it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.3111202716827393, 'eval_wer': 0.6009280742459396, 'eval_runtime': 11.1878, 'eval_samples_per_second': 11.352, 'epoch': 19.84}\n",
            " 66% 12400/18750 [7:26:06<3:33:26,  2.02s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12400/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12000] due to args.save_total_limit\n",
            "{'loss': 0.4539, 'learning_rate': 2e-05, 'epoch': 20.48}\n",
            " 68% 12800/18750 [7:39:58<1:38:49,  1.00it/s]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.3200886249542236, 'eval_wer': 0.5974477958236659, 'eval_runtime': 11.1642, 'eval_samples_per_second': 11.376, 'epoch': 20.48}\n",
            " 68% 12800/18750 [7:40:10<1:38:49,  1.00it/s]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12800/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12800/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12800/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12400] due to args.save_total_limit\n",
            "{'loss': 0.4424, 'learning_rate': 2e-05, 'epoch': 21.12}\n",
            " 70% 13200/18750 [7:54:35<3:15:52,  2.12s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.41it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.48it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.48it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.58it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:03,  1.65it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.67it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.55it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.46it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.3269623517990112, 'eval_wer': 0.5904872389791184, 'eval_runtime': 11.2696, 'eval_samples_per_second': 11.269, 'epoch': 21.12}\n",
            " 70% 13200/18750 [7:54:46<3:15:52,  2.12s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13200/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13200/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13200/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-12800] due to args.save_total_limit\n",
            "{'loss': 0.4268, 'learning_rate': 2e-05, 'epoch': 21.76}\n",
            " 73% 13600/18750 [8:08:58<3:01:32,  2.12s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.70it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.14it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.3292210102081299, 'eval_wer': 0.5812064965197216, 'eval_runtime': 11.1792, 'eval_samples_per_second': 11.36, 'epoch': 21.76}\n",
            " 73% 13600/18750 [8:09:09<3:01:32,  2.12s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13600/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13600/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13600/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13200] due to args.save_total_limit\n",
            "{'loss': 0.4303, 'learning_rate': 2e-05, 'epoch': 22.4}\n",
            " 75% 14000/18750 [8:23:06<1:18:06,  1.01it/s]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.3576560020446777, 'eval_wer': 0.5939675174013921, 'eval_runtime': 11.2001, 'eval_samples_per_second': 11.339, 'epoch': 22.4}\n",
            " 75% 14000/18750 [8:23:17<1:18:06,  1.01it/s]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14000/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14000/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14000/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-13600] due to args.save_total_limit\n",
            "{'loss': 0.4088, 'learning_rate': 2e-05, 'epoch': 23.04}\n",
            " 77% 14400/18750 [8:37:50<2:26:10,  2.02s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.70it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.14it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.70it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.48it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.34it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.3453797101974487, 'eval_wer': 0.5916473317865429, 'eval_runtime': 11.1216, 'eval_samples_per_second': 11.419, 'epoch': 23.04}\n",
            " 77% 14400/18750 [8:38:01<2:26:10,  2.02s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14400/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14000] due to args.save_total_limit\n",
            "{'loss': 0.4036, 'learning_rate': 2e-05, 'epoch': 23.68}\n",
            " 79% 14800/18750 [8:52:13<2:13:53,  2.03s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.70it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.61it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.67it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.55it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.3775463104248047, 'eval_wer': 0.5881670533642691, 'eval_runtime': 11.1942, 'eval_samples_per_second': 11.345, 'epoch': 23.68}\n",
            " 79% 14800/18750 [8:52:24<2:13:53,  2.03s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14800/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14800/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14800/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14400] due to args.save_total_limit\n",
            "{'loss': 0.3945, 'learning_rate': 2e-05, 'epoch': 24.32}\n",
            " 81% 15200/18750 [9:06:19<1:02:43,  1.06s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.67it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.54it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.44it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.31it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.374667763710022, 'eval_wer': 0.6032482598607889, 'eval_runtime': 11.2955, 'eval_samples_per_second': 11.243, 'epoch': 24.32}\n",
            " 81% 15200/18750 [9:06:30<1:02:43,  1.06s/it]\n",
            "100% 16/16 [00:10<00:00,  1.26it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15200/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15200/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15200/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-14800] due to args.save_total_limit\n",
            "{'loss': 0.392, 'learning_rate': 2e-05, 'epoch': 24.96}\n",
            " 83% 15600/18750 [9:20:41<54:30,  1.04s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.96it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 1.3735636472702026, 'eval_wer': 0.5765661252900232, 'eval_runtime': 11.1828, 'eval_samples_per_second': 11.357, 'epoch': 24.96}\n",
            " 83% 15600/18750 [9:20:52<54:30,  1.04s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15600/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15600/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15600/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15200] due to args.save_total_limit\n",
            "{'loss': 0.3766, 'learning_rate': 2e-05, 'epoch': 25.6}\n",
            " 85% 16000/18750 [9:35:17<1:33:22,  2.04s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.68it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.50it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.61it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.67it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 1.4049561023712158, 'eval_wer': 0.5672853828306265, 'eval_runtime': 11.1702, 'eval_samples_per_second': 11.37, 'epoch': 25.6}\n",
            " 85% 16000/18750 [9:35:28<1:33:22,  2.04s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16000/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16000/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16000/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-15600] due to args.save_total_limit\n",
            "{'loss': 0.3755, 'learning_rate': 2e-05, 'epoch': 26.24}\n",
            " 87% 16400/18750 [9:49:24<40:30,  1.03s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.65it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.09it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.94it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.66it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.51it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.42it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.51it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.68it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.53it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.44it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.31it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 1.4114658832550049, 'eval_wer': 0.5788863109048724, 'eval_runtime': 11.2877, 'eval_samples_per_second': 11.251, 'epoch': 26.24}\n",
            " 87% 16400/18750 [9:49:36<40:30,  1.03s/it]\n",
            "100% 16/16 [00:10<00:00,  1.28it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16400/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16000] due to args.save_total_limit\n",
            "{'loss': 0.3518, 'learning_rate': 2e-05, 'epoch': 26.88}\n",
            " 90% 16800/18750 [10:03:52<34:12,  1.05s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.69it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.13it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.98it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.70it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.53it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.46it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.32it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.3969961404800415, 'eval_wer': 0.5893271461716937, 'eval_runtime': 11.1873, 'eval_samples_per_second': 11.352, 'epoch': 26.88}\n",
            " 90% 16800/18750 [10:04:04<34:12,  1.05s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16800/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16800/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16800/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16400] due to args.save_total_limit\n",
            "{'loss': 0.3587, 'learning_rate': 2e-05, 'epoch': 27.52}\n",
            " 92% 17200/18750 [10:18:28<51:47,  2.00s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.67it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.52it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:03,  1.65it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.63it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:02,  1.50it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.43it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.31it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.4127262830734253, 'eval_wer': 0.5765661252900232, 'eval_runtime': 11.3025, 'eval_samples_per_second': 11.236, 'epoch': 27.52}\n",
            " 92% 17200/18750 [10:18:39<51:47,  2.00s/it]\n",
            "100% 16/16 [00:10<00:00,  1.28it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17200/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17200/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17200/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-16800] due to args.save_total_limit\n",
            "{'loss': 0.3487, 'learning_rate': 2e-05, 'epoch': 28.16}\n",
            " 94% 17600/18750 [10:32:41<19:10,  1.00s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.67it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.44it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.63it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.33it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.4493649005889893, 'eval_wer': 0.5951276102088167, 'eval_runtime': 11.1683, 'eval_samples_per_second': 11.371, 'epoch': 28.16}\n",
            " 94% 17600/18750 [10:32:52<19:10,  1.00s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17600/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17600/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17600/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17200] due to args.save_total_limit\n",
            "{'loss': 0.3472, 'learning_rate': 2e-05, 'epoch': 28.8}\n",
            " 96% 18000/18750 [10:47:03<12:57,  1.04s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.67it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.68it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:04<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.69it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.69it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.56it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.47it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.34it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.4520224332809448, 'eval_wer': 0.5707656612529002, 'eval_runtime': 11.1477, 'eval_samples_per_second': 11.392, 'epoch': 28.8}\n",
            " 96% 18000/18750 [10:47:14<12:57,  1.04s/it]\n",
            "100% 16/16 [00:10<00:00,  1.30it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-18000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-18000/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-18000/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-18000/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-17600] due to args.save_total_limit\n",
            "{'loss': 0.3302, 'learning_rate': 2e-05, 'epoch': 29.44}\n",
            " 98% 18400/18750 [11:01:39<11:44,  2.01s/it]***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:05,  2.67it/s]\u001b[A\n",
            " 19% 3/16 [00:01<00:06,  2.12it/s]\u001b[A\n",
            " 25% 4/16 [00:02<00:06,  1.97it/s]\u001b[A\n",
            " 31% 5/16 [00:02<00:06,  1.67it/s]\u001b[A\n",
            " 38% 6/16 [00:03<00:06,  1.53it/s]\u001b[A\n",
            " 44% 7/16 [00:04<00:06,  1.43it/s]\u001b[A\n",
            " 50% 8/16 [00:05<00:05,  1.52it/s]\u001b[A\n",
            " 56% 9/16 [00:05<00:04,  1.52it/s]\u001b[A\n",
            " 62% 10/16 [00:06<00:03,  1.62it/s]\u001b[A\n",
            " 69% 11/16 [00:06<00:02,  1.68it/s]\u001b[A\n",
            " 75% 12/16 [00:07<00:02,  1.70it/s]\u001b[A\n",
            " 81% 13/16 [00:08<00:01,  1.57it/s]\u001b[A\n",
            " 88% 14/16 [00:08<00:01,  1.45it/s]\u001b[A\n",
            " 94% 15/16 [00:09<00:00,  1.32it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 1.4457380771636963, 'eval_wer': 0.5649651972157773, 'eval_runtime': 11.1904, 'eval_samples_per_second': 11.349, 'epoch': 29.44}\n",
            " 98% 18400/18750 [11:01:50<11:44,  2.01s/it]\n",
            "100% 16/16 [00:10<00:00,  1.29it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-18400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-18400/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-18400/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-18400/preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/checkpoint-18000] due to args.save_total_limit\n",
            "100% 18750/18750 [11:14:09<00:00,  1.21s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 40449.2934, 'train_samples_per_second': 0.464, 'epoch': 30.0}\n",
            "100% 18750/18750 [11:14:09<00:00,  2.16s/it]\n",
            "Saving model checkpoint to ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/config.json\n",
            "Model weights saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/pytorch_model.bin\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/preprocessor_config.json\n",
            "Configuration saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/preprocessor_config.json\n",
            "tokenizer config file saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/tokenizer_config.json\n",
            "Special tokens file saved in ./wav2vec2-large-xlsr-hindi-openslr-bs16-lr2e-5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                      =        30.0\n",
            "  init_mem_cpu_alloc_delta   =       782MB\n",
            "  init_mem_cpu_peaked_delta  =         0MB\n",
            "  init_mem_gpu_alloc_delta   =      1203MB\n",
            "  init_mem_gpu_peaked_delta  =         0MB\n",
            "  train_mem_cpu_alloc_delta  =      3835MB\n",
            "  train_mem_cpu_peaked_delta =      1143MB\n",
            "  train_mem_gpu_alloc_delta  =      3602MB\n",
            "  train_mem_gpu_peaked_delta =      4468MB\n",
            "  train_runtime              = 11:14:09.29\n",
            "  train_samples              =       10000\n",
            "  train_samples_per_second   =       0.464\n",
            "04/09/2021 14:42:24 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 127\n",
            "  Batch size = 8\n",
            "100% 16/16 [00:10<00:00,  1.50it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                     =       30.0\n",
            "  eval_loss                 =     1.4557\n",
            "  eval_mem_cpu_alloc_delta  =        6MB\n",
            "  eval_mem_cpu_peaked_delta =        0MB\n",
            "  eval_mem_gpu_alloc_delta  =        0MB\n",
            "  eval_mem_gpu_peaked_delta =     1451MB\n",
            "  eval_runtime              = 0:00:11.22\n",
            "  eval_samples              =        127\n",
            "  eval_samples_per_second   =     11.318\n",
            "  eval_wer                  =     0.5661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvMhgF-V9US8",
        "outputId": "1ce39eeb-a9b3-4ff7-f610-fdfeb686a2a0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar 28 20:57:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    23W / 300W |      2MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4UY3EXspQky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022bbb50-4995-41f5-e06c-8455a57fc864"
      },
      "source": [
        "!git clone https://huggingface.co/shiwangi27/wave2vec2-large-xlsr-hindi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wave2vec2-large-xlsr-hindi'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEgEUvzuJtEY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "91c92540067a4b96b2e54e9f381770ab",
            "191407a9953e4ba9adae59a3f869ecfd",
            "879e2e97b92d427b8f94012893083abb",
            "aeaa9eb8f1544804ae2293889a9a170a",
            "3944dec8acba496680223b4770238e11",
            "bba08af6be4e47bf8cc0be0c307d0b31",
            "492556fed3374b66b0da8d9e9d5a95ed",
            "c36a4a7bef0349c8ace2b43b4a8d36aa"
          ]
        },
        "outputId": "dbc96113-6343-4631-bc50-233b18d52b29"
      },
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "test_dataset = load_dataset(\"common_voice\", \"hi\", split=\"test[:10%]\") #TODO: replace {lang_id} in your language code here. Make sure the code is one of the *ISO codes* of [this](https://huggingface.co/languages) site.\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"shiwangi27/wave2vec2-large-xlsr-hindi\") #TODO: replace {model_id} with your model id. The model id consists of {your_username}/{your_modelname}, *e.g.* `elgeish/wav2vec2-large-xlsr-53-arabic`\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"shiwangi27/wave2vec2-large-xlsr-hindi\") #TODO: replace {model_id} with your model id. The model id consists of {your_username}/{your_modelname}, *e.g.* `elgeish/wav2vec2-large-xlsr-53-arabic`\n",
        "\n",
        "resampler = torchaudio.transforms.Resample(48_000, 16_000)\n",
        "\n",
        "# Preprocessing the datasets.\n",
        "# We need to read the aduio files as arrays\n",
        "def speech_file_to_array_fn(batch):\n",
        "\tspeech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "\tbatch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n",
        "\treturn batch\n",
        "\n",
        "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
        "inputs = processor(test_dataset[:50][\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "\tlogits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
        "\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "print(\"Prediction:\", processor.batch_decode(predicted_ids))\n",
        "print(\"Reference:\", test_dataset[:50][\"sentence\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/hi/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91c92540067a4b96b2e54e9f381770ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Prediction: ['जर्मानी कयाता था थे रोजयोद चनार रह', 'मिकाधरवाजा खोलिए', 'मुझे मुर्ख बनना पसंद नहीं है', 'आप दोलोंग देख रली हैं', 'यह रगी तेड़ी चाए', 'दिल्लीए उंशीहार में सुहावने मौसम के भीक कराथेक ज्याम बना लगों क़ी ए मुशीगत', 'महीनाओं के लिए इसके पलप है', 'भहसोता जैसे खउन समझती रहीरोंनया बादनेवा निकला करंपा नू', 'आतंगवादी लखनों कोबी दे सकते हैं पठान कोटे जै स घाव', 'मुझे अपने पिता की गरी भी पर सर्ब नहीं आती हैं', 'तुम गाड़ी चला सकते हो क्या', 'ईंटिकिट कारोबार में आयार शिटिसी को मिल सकती हैं चुन्होती', 'वह होया और और होया']\n",
            "Reference: ['जर्मनी चाहता था कि रूस युद्ध से बाहर रहे।', 'दरवाज़ा खोलिए।', 'मुझे मूर्ख बनना पसंद नहीं है।', 'आप दोनों देख रही हैं।', 'यह रही तेरी चाय।', 'दिल्ली-एनसीआर में सुहावने मौसम के बीच ट्रैफिक जाम बना लोगों के लिए मुसीबत', 'महिलाओं के लिये स्ट्रिप क्लब है।', 'बरसों तक जिसे खून समझती रही दुनिया, बाद में वो निकला गर्म पानी', 'आतंकवादी लखनऊ को भी दे सकते हैं पठानकोट जैसा घाव!', 'मुझे अपने पिता की ग़रीबी पर शर्म नहीं आती है।', 'तुम गाड़ी चला सकते हो क्या?', 'ईटिकट कारोबार में आईआरसीटीसी को मिल सकती है चुनौती', 'वह रोया और और रोया।']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2fHzA2OWIzy"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KABFPmuopZMm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235,
          "referenced_widgets": [
            "1277a0544d644dba89c70e999e9eafb0",
            "1d2c934fbe7e4d67b75f4dfacd4d7d36",
            "60aa19d10221445da85b3f39a9991e47",
            "43d6c229db214fc8a4325d972cadaaec",
            "c6d67fed40e040bc93a55ed43ca9b412",
            "93f58c64502a4626b9078c48d3960c5b",
            "de43c233b76f44a0a16949069d7f5280",
            "734587d7751541f2a5d8c3eab92374d7",
            "c513c0f46be941508f3de076f1e294ec",
            "1fb8c45f2e3c45e88e890571b4c82bbc",
            "c644f82626f14964bd6550ec01cbb873",
            "19771a051e4146389d3efc528afb76a5",
            "78e7dac1aca24db588d79013523a2f92",
            "d6fe1000eaad4b848b80c656cdd690ee",
            "75c4b61e74684996819b4a48c022ca57",
            "00dd3de99cd74b5695b6f83f360b4f82",
            "55e112a12c1c435dbe1114cb7a762f40",
            "36a1660d631e4b079222f7ee3c684b99",
            "7fb33cd2bd4d42b08d220eaf9a3b642b",
            "a39a0dc5b9264e02b4dc79b2a2570020",
            "9dc9a46171d44ef48f951e7208e874e6",
            "ad54301240db4619ad2e632d72254942",
            "3b10e6c6ac5f4a81b0275964597426ac",
            "1e210796981c4ac29899003c5468fed9"
          ]
        },
        "outputId": "31cf6518-0bdf-43ff-8d68-0620aad4e6c1"
      },
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import re\n",
        "\n",
        "test_dataset = load_dataset(\"common_voice\", \"hi\", split=\"test\") #TODO: replace {lang_id} in your language code here. Make sure the code is one of the *ISO codes* of [this](https://huggingface.co/languages) site.\n",
        "wer = load_metric(\"wer\")\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"shiwangi27/wave2vec2-large-xlsr-hindi\") #TODO: replace {model_id} with your model id. The model id consists of {your_username}/{your_modelname}, *e.g.* `elgeish/wav2vec2-large-xlsr-53-arabic`\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"shiwangi27/wave2vec2-large-xlsr-hindi\") #TODO: replace {model_id} with your model id. The model id consists of {your_username}/{your_modelname}, *e.g.* `elgeish/wav2vec2-large-xlsr-53-arabic`\n",
        "model.to(\"cuda\")\n",
        "\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\�\\।\\']'  # TODO: adapt this list to include all special characters you removed from the data\n",
        "resampler = torchaudio.transforms.Resample(48_000, 16_000)\n",
        "\n",
        "# Preprocessing the datasets.\n",
        "# We need to read the aduio files as arrays\n",
        "def speech_file_to_array_fn(batch):\n",
        "\tbatch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower()\n",
        "\tspeech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "\tbatch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n",
        "\treturn batch\n",
        "\n",
        "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
        "\n",
        "# Preprocessing the datasets.\n",
        "# We need to read the aduio files as arrays\n",
        "def evaluate(batch):\n",
        "\tinputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\tlogits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n",
        "\n",
        "\tpred_ids = torch.argmax(logits, dim=-1)\n",
        "\tbatch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n",
        "\treturn batch\n",
        "\n",
        "result = test_dataset.map(evaluate, batched=True, batch_size=8)\n",
        "\n",
        "print(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/hi/6.1.0/0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1277a0544d644dba89c70e999e9eafb0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1764.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c513c0f46be941508f3de076f1e294ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=127.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55e112a12c1c435dbe1114cb7a762f40",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WER: 46.055684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V11H0NNOqj7_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aY7IkLHpb2j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWUTeautqO6T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}