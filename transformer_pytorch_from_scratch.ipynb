{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer-pytorch-from-scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqDYlyIzf77LVl2Tct7fip",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiwangi27/googlecolab/blob/main/transformer_pytorch_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AiB9mZMXRHH"
      },
      "source": [
        "### PyTorch for building and training Transformer NLP models from scratch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCPd43FHvit8"
      },
      "source": [
        "This is an implementation of Transformers from the [Attention is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper by Vaswani et al. \n",
        "\n",
        "For this implementation, I have referred Andrej Karpathy's [minGPT](https://github.com/karpathy/minGPT/blob/master/mingpt).\n",
        "\n",
        "I have tried explaining each step in comments for better understanding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAQsXp-3XXrV"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tojmHl9NLhJp"
      },
      "source": [
        "X_train_embeddings = torch.rand(size=(32, 128, 512), dtype=torch.float32, device=\"cpu\") "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZeBDA1AXa9H"
      },
      "source": [
        "class Config:\n",
        "  pass"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW2V_514X5DX"
      },
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Multi-head Self-attention is the learnt subspace representation of a sequence\n",
        "  through linear projections of scaled dot product of attentions.\n",
        "  multi_head_self_attention = softmax (k . q / âˆšd) * v \n",
        "  \"\"\"\n",
        "  def __init__(self, embed_dim: int, \n",
        "               num_heads: int,\n",
        "               activation: nn.functional = F.relu, \n",
        "               bias: bool = True, \n",
        "               attention_dropout=0.1, \n",
        "               residual_dropout=0.1, \n",
        "               mask: bool = False) -> Tensor:\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.embed_dim = embed_dim\n",
        "    self.activation = activation\n",
        "    self.bias = bias\n",
        "    self.mask = mask\n",
        "\n",
        "    # Define Query, Key and Value as learnable linear layers\n",
        "    self.linear_q = nn.Linear(embed_dim, embed_dim)\n",
        "    self.linear_k = nn.Linear(embed_dim, embed_dim)\n",
        "    self.linear_v = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    # Define Dropout to add regularization to attention or to residual\n",
        "    self.att_dropout = nn.Dropout(p=attention_dropout)\n",
        "    self.res_dropout = nn.Dropout(p=residual_dropout)\n",
        "\n",
        "    # Define a Linear Projection layer.   \n",
        "    self.out_projection = nn.Linear(embed_dim, embed_dim)\n",
        "  \n",
        "  def _reshape_to_subspace(self, x):\n",
        "    # If the shape of the input was (batch_size, seq_len, embed_dim) = (32, 128, 512). \n",
        "    # For 8 heads, we first reshape it to (32, 128, 8, 64) and then (32*8, 128, 64). \n",
        "\n",
        "    batch_size, seq_len, embed_dim = x.size()\n",
        "    subspace_dim = embed_dim // self.num_heads\n",
        "    new_batch_size = batch_size * self.num_heads\n",
        "\n",
        "    return x.reshape(batch_size, seq_len, self.num_heads, subspace_dim)\\\n",
        "            .permute((0, 2, 1, 3))\\\n",
        "            .reshape(new_batch_size, seq_len, subspace_dim)\n",
        "\n",
        "  def _reshape_from_subspace(self, x):\n",
        "    # Here we do the reverse of the above to get back the full embed dim after subspace learning.  \n",
        "\n",
        "    new_batch_size, seq_len, subspace_dim = x.size()\n",
        "    embed_dim = subspace_dim * self.num_heads\n",
        "    batch_size = new_batch_size // self.num_heads\n",
        "\n",
        "    return x.reshape(batch_size, self.num_heads, seq_len, subspace_dim)\\\n",
        "            .permute(0, 2, 1, 3)\\\n",
        "            .reshape(batch_size, seq_len, embed_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    # Query, Key, Value vectors \n",
        "    q = self.linear_q(X)\n",
        "    k = self.linear_k(X)\n",
        "    v = self.linear_v(X)\n",
        "  \n",
        "    # Reshape the embed dim to subspace representation, splitting the sequence into multiple heads. \n",
        "    q = self._reshape_to_subspace(q)\n",
        "    k = self._reshape_to_subspace(k)\n",
        "    v = self._reshape_to_subspace(v)\n",
        "\n",
        "    # Scaling by inverse square root of the embed dimension is empirically found really effective.  \n",
        "    d = q.size(-1)\n",
        "    \n",
        "    # Scaled dot product attentions (optionally masked self attentions for architectures like GPT-2.)\n",
        "    scaled_dot_product = q.matmul(k.transpose(-2, -1)) / math.sqrt(d)\n",
        "    if self.mask:\n",
        "      scaled_dot_product = scaled_dot_product.masked_fill(mask[:,:,:seq_len,:seq_len] == 0, float('-inf'))\n",
        "\n",
        "    attention = F.softmax(scaled_dot_product, dim=-1)\n",
        "    \n",
        "    # Attention dropout is applied to the output of the softmax.\n",
        "    attention = self.att_dropout(attention)\n",
        "    \n",
        "    y = attention.matmul(v)\n",
        "\n",
        "    # Reshape the subspace learned representation back to the full embed dimension.\n",
        "    y = self._reshape_from_subspace(y)\n",
        "\n",
        "    # Linear projection of the output from each of the multi heads. \n",
        "    y = self.out_projection(y)\n",
        "    \n",
        "    # residual dropout - it's the layer dropout before passing the output to the next layer. \n",
        "    y = self.res_dropout(y)\n",
        "\n",
        "    return y\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLy8wY7QcFBc"
      },
      "source": [
        "multi_head_self_attention = MultiHeadSelfAttention(embed_dim=512, num_heads=8)\n",
        "attentions = multi_head_self_attention(X_train_embeddings)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKfBL3qogVza",
        "outputId": "9e310c37-cce4-4801-e1bb-86553944fd27"
      },
      "source": [
        "attentions"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0205,  0.1748,  0.0574,  ...,  0.0205, -0.3576,  0.0832],\n",
              "         [ 0.0145,  0.1700,  0.0703,  ...,  0.0160, -0.3584,  0.0752],\n",
              "         [ 0.0190,  0.1771,  0.0671,  ...,  0.0135, -0.3471,  0.0743],\n",
              "         ...,\n",
              "         [ 0.0248,  0.1696,  0.0598,  ...,  0.0155, -0.3576,  0.0000],\n",
              "         [ 0.0222,  0.1853,  0.0575,  ...,  0.0000, -0.3604,  0.0775],\n",
              "         [ 0.0231,  0.1779,  0.0668,  ...,  0.0000, -0.3434,  0.0757]],\n",
              "\n",
              "        [[ 0.0390,  0.0000,  0.0657,  ...,  0.0471, -0.3786,  0.0786],\n",
              "         [ 0.0316,  0.2001,  0.0631,  ...,  0.0403, -0.3645,  0.0845],\n",
              "         [ 0.0000,  0.2026,  0.0628,  ...,  0.0330, -0.0000,  0.0879],\n",
              "         ...,\n",
              "         [ 0.0273,  0.1791,  0.0660,  ...,  0.0372, -0.3662,  0.0860],\n",
              "         [ 0.0232,  0.1913,  0.0711,  ...,  0.0373, -0.3744,  0.0686],\n",
              "         [ 0.0224,  0.1874,  0.0664,  ...,  0.0442, -0.3709,  0.0843]],\n",
              "\n",
              "        [[ 0.0351,  0.2084,  0.0697,  ...,  0.0041, -0.3639,  0.0711],\n",
              "         [ 0.0210,  0.1926,  0.0841,  ...,  0.0021, -0.3538,  0.0000],\n",
              "         [ 0.0303,  0.0000,  0.0709,  ...,  0.0027, -0.3716,  0.0767],\n",
              "         ...,\n",
              "         [ 0.0000,  0.1927,  0.0693,  ...,  0.0104, -0.3635,  0.0820],\n",
              "         [ 0.0000,  0.2015,  0.0672,  ...,  0.0043, -0.3642,  0.0707],\n",
              "         [ 0.0316,  0.2033,  0.0739,  ...,  0.0117, -0.3628,  0.0849]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.0196,  0.2020,  0.0741,  ...,  0.0420, -0.3613,  0.0000],\n",
              "         [ 0.0128,  0.2037,  0.0700,  ...,  0.0314, -0.3589,  0.0792],\n",
              "         [ 0.0000,  0.1898,  0.0000,  ...,  0.0446, -0.3628,  0.0000],\n",
              "         ...,\n",
              "         [ 0.0000,  0.1984,  0.0704,  ...,  0.0407, -0.3639,  0.0792],\n",
              "         [ 0.0150,  0.1981,  0.0741,  ...,  0.0379, -0.3652,  0.0841],\n",
              "         [ 0.0118,  0.1942,  0.0743,  ...,  0.0369, -0.0000,  0.0788]],\n",
              "\n",
              "        [[ 0.0214,  0.1964,  0.0777,  ...,  0.0321, -0.3477,  0.0785],\n",
              "         [ 0.0207,  0.2124,  0.0865,  ...,  0.0278, -0.3587,  0.0826],\n",
              "         [ 0.0186,  0.2024,  0.0832,  ...,  0.0279, -0.3500,  0.0790],\n",
              "         ...,\n",
              "         [ 0.0178,  0.2047,  0.0884,  ...,  0.0309, -0.3550,  0.0830],\n",
              "         [ 0.0218,  0.2098,  0.0000,  ...,  0.0399, -0.3610,  0.0917],\n",
              "         [ 0.0178,  0.2071,  0.0885,  ...,  0.0332, -0.3540,  0.0793]],\n",
              "\n",
              "        [[ 0.0432,  0.2001,  0.0749,  ...,  0.0259, -0.0000,  0.0519],\n",
              "         [ 0.0347,  0.1920,  0.0757,  ...,  0.0253, -0.0000,  0.0572],\n",
              "         [ 0.0217,  0.1819,  0.0867,  ...,  0.0280, -0.3642,  0.0561],\n",
              "         ...,\n",
              "         [ 0.0315,  0.1881,  0.0834,  ...,  0.0230, -0.3633,  0.0629],\n",
              "         [ 0.0387,  0.2001,  0.0721,  ...,  0.0161, -0.3544,  0.0459],\n",
              "         [ 0.0260,  0.1748,  0.0829,  ...,  0.0238, -0.3665,  0.0000]]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvzrjSnSYOxX"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "  def forward(self):\n",
        "    pass"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i0i4EmRX73X"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BertModel, self).__init__()\n",
        "  def forward(self):\n",
        "    pass"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt0xaVOsYBHI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}